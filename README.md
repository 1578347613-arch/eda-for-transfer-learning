# eda-for-transfer-learning
---

## ğŸš€ TL;DR å¿«é€Ÿä¸Šæ‰‹

```bash

python -m data_loader.cli --opamp 5t_opamp --val-split 0.2 --seed 42

# 3) è®­ç»ƒåŸºçº¿ MLPï¼ˆä¿å­˜ baseline æƒé‡ï¼‰
python -m training.train

# 4) è®­ç»ƒ AlignHeteroMLP + CORALï¼ˆä¿å­˜ align_hetero æƒé‡ï¼‰
python -m training.train_align_coral

# 5) å¾®è°ƒ DualHeadMLPï¼ˆåˆ†é˜¶æ®µè®­ç»ƒï¼Œä¿å­˜ dualhead å¾®è°ƒæƒé‡ï¼‰
python -m fine_tune.fine_tune

# 6) é›†æˆæ¨ç† & æŒ‡æ ‡è¯„ä¼°ï¼ˆåæ ‡å‡†åŒ–åˆ°ç‰©ç†å•ä½ï¼‰
python -m inference.infer_ensemble
```

---

## ğŸ“¦ é¡¹ç›®æ¦‚è§ˆ

æœ¬å·¥ç¨‹é¢å‘æ¨¡æ‹Ÿç”µè·¯ï¼ˆå¦‚è¿æ”¾ï¼‰å¤šç›®æ ‡å›å½’ï¼Œæä¾›ï¼š

- **æ•°æ®ç®¡é“**ï¼šåŠ è½½ã€é¢„å¤„ç†ï¼ˆå« `log1p`ã€æ ‡å‡†åŒ–ï¼‰ä»¥åŠä¿å­˜/å¤ç”¨ scalerã€‚  
- **æ¨¡å‹åº“**ï¼š`MLP`ã€`AlignHeteroMLP`ï¼ˆå¼‚æ–¹å·®ï¼‰ã€`DualHeadMLP`ï¼ˆåŒå¤´å¾®è°ƒï¼‰ã€‚  
- **è®­ç»ƒè„šæœ¬**ï¼šåŸºçº¿è®­ç»ƒã€å¸¦ CORAL çš„è·¨åŸŸå¯¹é½è®­ç»ƒã€åˆ†é˜¶æ®µå¾®è°ƒï¼ˆL2-SP æ­£åˆ™ï¼‰ã€‚  
- **æ¨ç†è„šæœ¬**ï¼šä¸¤æ¨¡å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ æƒ + MSE æƒé‡èåˆï¼Œè‡ªåŠ¨åæ ‡å‡†åŒ–ä¸è¯„ä¼°ã€‚

---

## ğŸ—‚ ç›®å½•ç»“æ„

```
src_new/
â”œâ”€â”€ config.py                 # ç»Ÿä¸€ç®¡ç†è¶…å‚æ•°&è®¾å¤‡
â”‚---inverse_mdn.py
----inverse_opt.py
â”œâ”€â”€ data_loader/
â”‚   â”œâ”€â”€ __init__.py           # æš´éœ² load/preprocess/split/scale çš„ç»Ÿä¸€API
â”‚   â”œâ”€â”€ cli.py                # å‘½ä»¤è¡Œå¿«é€ŸéªŒè¯æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ data_loader.py        # åŠ è½½+é¢„å¤„ç†+åˆ’åˆ†
â”‚   â””â”€â”€ scaler_utils.py       # ä¿å­˜/åŠ è½½ x,y çš„ StandardScaler
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mlp.py                # åŸºç¡€ MLP
â”‚   â”œâ”€â”€ align_hetero.py       # AlignHeteroMLPï¼ˆbackbone + hetero_headï¼‰
â”‚   â””â”€â”€ model_utils.py        # ä¿å­˜/åŠ è½½æƒé‡ï¼Œä¸»å¹²è¿ç§»å·¥å…·
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒåŸºçº¿ MLP
â”‚   â””â”€â”€ train_align_coral.py  # AlignHeteroMLP + CORAL è®­ç»ƒ
â”‚
â”œâ”€â”€ fine_tune/
â”‚   â””â”€â”€ fine_tune.py          # DualHeadMLP åˆ†é˜¶æ®µå¾®è°ƒï¼ˆbiasâ†’headâ†’è§£å†»æœ€åå±‚ï¼‰
â”‚
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ infer_ensemble.py     # é›†æˆæ¨ç†ï¼ˆæ¸©åº¦æ ‡å®š+ç²¾åº¦/MSEåŠ æƒï¼‰ï¼Œè¯„ä¼°MSE/MAE/RÂ²
â”‚
â””â”€â”€ losses/
    â””â”€â”€ loss_function.py      # heteroscedastic_nll / batch_r2 / coral_loss
# æ³¨ï¼šè‹¥ä½ ä½¿ç”¨ `from losses import x`ï¼Œè¯·ç¡®ä¿ losses/__init__.py å·²å¯¼å‡ºä¸Šè¿°å‡½æ•°ã€‚
```

---

## ğŸ”§ å®‰è£…ä¸ä¾èµ–

**Python**ï¼šå»ºè®® 3.10  
**PyTorch**ï¼šæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬å®‰è£…ï¼ˆhttps://pytorch.org/ï¼‰  

è‹¥æ—  `requirements.txt`ï¼Œå¯ä½¿ç”¨ä¸‹åˆ—åŸºç¡€ä¾èµ–ï¼ˆæŒ‰éœ€å¢å‡ï¼‰ï¼š

```txt
numpy>=1.24
scikit-learn>=1.3
joblib>=1.3
torch>=2.1
tqdm>=4.66
```

---

## âš™ï¸ é…ç½®ï¼ˆ`config.py`ï¼‰

ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è¶…å‚æ•°ï¼Œéšç”¨éšæ”¹ï¼Œè®­ç»ƒè„šæœ¬è‡ªåŠ¨è¯»å–ï¼š

```python
# å…³é”®å‚æ•°ç¤ºä¾‹ï¼ˆå®é™…ä»¥ä½ çš„ config.py ä¸ºå‡†ï¼‰
OPAMP_TYPE   = '5t_opamp'
DEVICE       = 'cuda'  # è‡ªåŠ¨æ£€æµ‹åŒæ ·å¯ï¼š'cuda' if torch.cuda.is_available() else 'cpu'

# è®­ç»ƒ
EPOCHS       = 50
PATIENCE     = 10
LEARNING_RATE= 1e-4
BATCH_SIZE   = 256

# æ¨¡å‹
HIDDEN_DIM   = 512
NUM_LAYERS   = 6
DROPOUT_RATE = 0.1

# å¯¹é½/å¾®è°ƒ
LAMBDA_CORAL = 0.05
ALPHA_R2     = 1.0
L2SP_LAMBDA  = 1e-4
LR_BIAS      = 3e-4
LR_HEAD      = 1e-4
LR_UNFREEZE  = 5e-5
WEIGHT_DECAY = 1e-4
```

> **å»ºè®®**ï¼šä»…é€šè¿‡ `config.py` æ”¹åŠ¨è¶…å‚æ•°ï¼Œé¿å…åœ¨è„šæœ¬å†…â€œç¡¬ç¼–ç â€ï¼Œä¿è¯å…¨å·¥ç¨‹ä¸€è‡´ã€‚

---

## ğŸ§ª æ•°æ®ä¸é¢„å¤„ç†

- **æ•°æ®å…¥å£**ï¼š`data_loader.get_data_and_scalers(opamp_type=OPAMP_TYPE)`  
  è¿”å›ï¼š
  ```python
  {
    "source": (X_source_scaled, y_source_scaled),
    "target_train": (X_target_train, y_target_train),
    "target_val": (X_target_val, y_target_val),
    "x_scaler": x_scaler,
    "y_scaler": y_scaler,
  }
  ```
- é¢„å¤„ç†åŒ…å« `log1p`ï¼ˆå¯¹ç‰¹å®šç›®æ ‡ï¼Œå¦‚ `ugf`, `cmrr`ï¼‰å’Œæ ‡å‡†åŒ–ã€‚åæ ‡å‡†åŒ–ä¸ `expm1` åœ¨æ¨ç†é˜¶æ®µè‡ªåŠ¨å®Œæˆã€‚

---

## ğŸ— æ¨¡å‹ä¸æŸå¤±ï¼ˆAPI é€Ÿè§ˆï¼‰

### æ¨¡å‹

```python
from models import MLP, AlignHeteroMLP, DualHeadMLP

# MLP
m = MLP(input_dim, output_dim)                 # forward(x) -> y_hat

# AlignHeteroMLPï¼ˆå¼‚æ–¹å·®ï¼‰
m = AlignHeteroMLP(input_dim, output_dim)      # forward(x) -> (mu, logvar, features)

# DualHeadMLPï¼ˆå¾®è°ƒåŒå¤´ï¼‰
yB = model(x, domain='B')                      # æŒ‡å®šä½¿ç”¨ B å¤´
```

### æ¨¡å‹å·¥å…·

```python
from models.model_utils import (
  load_backbone_from_trained_mlp, save_model, load_model
)

load_backbone_from_trained_mlp(pretrained_mlp, align_model)
save_model(model, 'results/xxx.pth')
load_model(model, 'results/xxx.pth')
```

### æŸå¤±å‡½æ•°

```python
# è‹¥æ—  __init__.py å¯¼å‡ºï¼Œè¯·æ”¹ä¸ºï¼šfrom losses.loss_function import ...
from losses import heteroscedastic_nll, batch_r2, coral_loss
```

- `heteroscedastic_nll(mu, logvar, y, reduction='mean')`
- `batch_r2(y_true, y_pred, eps=1e-8)`
- `coral_loss(feat_a, feat_b, unbiased=True, eps=1e-6)`

---
ä¸€ã€‚æ­£å‘è®¾è®¡
## ğŸƒâ€â™‚ï¸ è®­ç»ƒ/å¾®è°ƒ/æ¨ç†æµç¨‹

### 1) è®­ç»ƒåŸºçº¿ MLP

```bash
python -m training.train
```

- **è¾“å…¥**ï¼š`source` ä½œä¸ºè®­ç»ƒé›†ï¼Œ`target_val` ä½œä¸ºéªŒè¯é›†  
- **è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_baseline_model.pth`  
- **æ—¥å¿—**ï¼šæ‰“å°æ¯è½® Train/Val MSE

### 2) è®­ç»ƒ AlignHeteroMLP + CORAL

```bash
python -m training.train_align_coral
```

- è½½å…¥åŸºçº¿ MLP ä½œä¸º **backbone** åˆå§‹æƒé‡  
- ç›®æ ‡åŸŸ `B` ä¸Šè®­ç»ƒå¼‚æ–¹å·® NLL + `RÂ²`ï¼ˆè½¬åŒ–ä¸ºæŸå¤±ï¼‰+ **CORAL**ï¼ˆè·¨åŸŸç‰¹å¾å¯¹é½ï¼‰  
- **è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_align_hetero_lambda{LAMBDA_CORAL:.3f}.pth`  
- **éªŒè¯æŒ‡æ ‡**ï¼šval NLLï¼ˆè¶Šå°è¶Šå¥½ï¼‰

### 3) åˆ†é˜¶æ®µå¾®è°ƒ DualHeadMLP

```bash
python -m fine_tune.fine_tune
```

åˆ†ä¸‰é˜¶æ®µï¼ˆå‡åœ¨ç›®æ ‡åŸŸ `B`ï¼‰ï¼š
1. **Bias æ ¡å‡†**ï¼šä»…è®­ç»ƒ `head_B.bias`
2. **è®­ç»ƒ B å¤´æƒé‡**ï¼šå¯ç”¨ L2-SP æ­£åˆ™ï¼Œçº¦æŸåç¦»é¢„è®­ç»ƒä¸»å¹²
3. **éƒ¨åˆ†è§£å†»**ï¼šè§£å†»ä¸»å¹²æœ€åä¸€å±‚ + B å¤´

**è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_dualhead_finetuned.pth`

> ä½¿ç”¨åˆ°çš„å…³é”®æ¥å£ï¼š
> - `run_epoch(model, loader, optimizer, loss_fn, phase, pretrained_state=None)`
> - `main()`ï¼šç»„ç»‡ä¸Šè¿°ä¸‰é˜¶æ®µè®­ç»ƒä¸æ—©åœ

### 4) é›†æˆæ¨ç†ä¸è¯„ä¼°

```bash
python -m inference.infer_ensemble
```

åšäº†ä»¥ä¸‹å·¥ä½œï¼š
- è½½å…¥ä¸¤ä¸ªå¼‚æ–¹å·®æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼š`align_hetero` ä¸ `target_only_hetero`ï¼‰  
- **æ¸©åº¦æ ‡å®š**ï¼ˆé—­å¼è§£ï¼‰æ ¡å‡†æ–¹å·®  
- **æ ·æœ¬çº§ç²¾åº¦æƒé‡** + **æŒ‡æ ‡çº§ MSE æƒé‡** èåˆ  
- åæ ‡å‡†åŒ– & `expm1`ï¼ˆå¯¹ `ugf`, `cmrr`ï¼‰  
- è¾“å‡ºæ¯ä¸ªæŒ‡æ ‡çš„ **MSE/MAE/RÂ²**

**æ‰“å°ç¤ºä¾‹**ï¼š
```
=== Ensemble on B-VAL (ç‰©ç†å•ä½) ===
slewrate_pos    MSE=...  MAE=...  R2=...
dc_gain         MSE=...  MAE=...  R2=...
ugf             MSE=...  MAE=...  R2=...
phase_margin    MSE=...  MAE=...  R2=...
cmrr            MSE=...  MAE=...  R2=...
```
äºŒ. åå‘è®¾è®¡ï¼ˆinverse_mdn.pyï¼‰
åŠŸèƒ½ï¼šé€šè¿‡è®­ç»ƒæ··åˆå¯†åº¦ç½‘ç»œï¼ˆMDNï¼‰æ¥å­¦ä¹ ä»ç›®æ ‡å€¼ y åˆ°è¾“å…¥å€¼ x çš„æ˜ å°„å…³ç³»ã€‚æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šè®­ç»ƒæ¨¡å¼å’Œé‡‡æ ·æ¨¡å¼ã€‚

1.1 è®­ç»ƒæ¨¡å¼
åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ä¸€ç»„æ ‡å‡†åŒ–çš„ç›®æ ‡æ•°æ® y_scaled å’Œè¾“å…¥æ•°æ® x_scaled æ¥è®­ç»ƒä¸€ä¸ª MDN æ¨¡å‹ã€‚æ¨¡å‹å°†å­¦ä¹ ä»ç›®æ ‡è¾“å‡ºåˆ°è¾“å…¥çš„æ˜ å°„ã€‚è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹æƒé‡å’Œæ ‡å‡†åŒ–å™¨ï¼ˆscalerï¼‰å°†è¢«ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚

ç”¨æ³•ï¼š

python -m inverse_mdn   --opamp 5t_opamp   --components 10 --hidden 256 --layers 4   --batch-size 128 --epochs 60 --lr 1e-3


1.2 é‡‡æ ·æ¨¡å¼
åœ¨é‡‡æ ·æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·æä¾›ä¸€ä¸ªç›®æ ‡ y_targetï¼Œå·¥å…·å°†åŸºäºå·²è®­ç»ƒçš„ MDN æ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰è¾“å…¥ x_scaledï¼Œè¿™äº›è¾“å…¥èƒ½å¤Ÿä½¿å¾—æ¨¡å‹çš„è¾“å‡ºæ¥è¿‘ç›®æ ‡ y_targetã€‚

ç”¨æ³•ï¼š

python -m inverse_mdn --sample \
  --model ../results/mdn_5t_opamp.pth \
  --y-target "2.5e8,200,1.5e6,65,20000" \
  --n 64 \
  --out ../results/inverse/init_64.npy



2. åå‘ä¼˜åŒ–ï¼ˆinverse_opt.pyï¼‰
åŠŸèƒ½ï¼šä½¿ç”¨åå‘ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–è¾“å…¥ x ä½¿å¾—æ¨¡å‹çš„è¾“å‡º y æ»¡è¶³ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡æˆ–çº¦æŸæ¡ä»¶ã€‚æ”¯æŒå¤šç§ç›®æ ‡ç±»å‹ï¼ˆæœ€å°åŒ–ã€æœ€å¤§åŒ–ã€ç›®æ ‡å€¼ã€èŒƒå›´ç­‰ï¼‰å’Œçº¦æŸæ¡ä»¶ï¼ˆå¦‚ ugf_band å’Œ pm_bandï¼‰ã€‚

2.1 åå‘ä¼˜åŒ–
åœ¨åå‘ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå·¥å…·ä¼šä½¿ç”¨å¤šä¸ªåˆå§‹ç‚¹å¯¹è¾“å…¥ x è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæœ€ä¼˜çš„è¾“å…¥ x_scaledï¼Œä½¿å¾—å…¶é¢„æµ‹è¾“å‡º y_scaled è¾¾åˆ°ç»™å®šç›®æ ‡ã€‚ä¼˜åŒ–ç»“æœå°†ä¿å­˜åœ¨æŒ‡å®šçš„ç›®å½•ä¸­ã€‚

ç”¨æ³•ï¼š
align heteroç›´æ¥ä¼˜åŒ–ï¼š
python -m inverse_opt \
  --opamp 5t_opamp \
  --ckpt ../results/5t_opamp_align_hetero_lambda0.050.pth \
  --model-type align_hetero \
  --y-target "2.5e8,200,1.5e6,65,20000" \
  --goal "min,min,range,range,min" \
  --ugf-band "8.0e5:2.0e6" \
  --pm-band "60:75" \
  --weights "0.05,0.40,0.90,0.10,0.65" \
  --prior 1e-3 \
  --n-init 512 --steps 800 --lr 0.002 \
  --finish-lbfgs 80 \
  --save-dir ../results/inverse/run_align


  
æ­é…mdnåˆå€¼ï¼ˆhybridï¼‰
python -m inverse_opt \
  --opamp 5t_opamp \
  --ckpt ../results/5t_opamp_align_hetero_lambda0.050.pth \
  --model-type align_hetero \
  --y-target "2.5e8,200,1.5e6,65,20000" \
  --goal "min,min,range,range,min" \
  --ugf-band "8.0e5:2.0e6" \
  --pm-band "60:75" \
  --weights "0.05,0.40,0.90,0.10,0.65" \
  --prior 1e-3 \
  --n-init 512 --steps 800 --lr 0.002 \
  --finish-lbfgs 80 \
  --save-dir ../results/inverse/run_align

ä½¿ç”¨dualhead

python -m inverse_opt \
  --opamp 5t_opamp \
  --ckpt ../results/5t_opamp_dualhead_finetuned.pth \
  --model-type dualhead_b \
  --y-target "2.5e8,200,1.5e6,65,20000" \
  --goal "min,min,range,range,min"


## ğŸ§­ å¸¸è§é—®é¢˜ï¼ˆFAQ / Troubleshootingï¼‰

- **æ‰¾ä¸åˆ° baseline æƒé‡**  
  å…ˆè¿è¡Œï¼š`python -m training.train`ï¼Œä¼šåœ¨ `results/` ä¸‹ç”Ÿæˆ `{OPAMP_TYPE}_baseline_model.pth`ã€‚

- **CUDA ä¸å¯ç”¨/æ˜¾å­˜ä¸è¶³**  
  åœ¨ `config.py` å°† `DEVICE='cpu'` æˆ–å‡å° `BATCH_SIZE`/`HIDDEN_DIM`ã€‚

- **å½¢çŠ¶ä¸åŒ¹é…**  
  æ£€æŸ¥ `input_dim = X.shape[1]` ä¸æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´ï¼›`output_dim = y.shape[1]` ä¸ä»»åŠ¡æŒ‡æ ‡æ•°é‡ä¸€è‡´ã€‚

- **CORAL æƒé‡è®¾ç½®**  
  `LAMBDA_CORAL` è¿‡å¤§å¯èƒ½æ‹–æ…¢æ”¶æ•›ï¼›å¯å…ˆä» `0.01~0.05` ç½‘æ ¼æœç´¢ã€‚

- **RÂ² æŸå¤±æƒé‡**  
  `ALPHA_R2` æ§åˆ¶ RÂ² ç›®æ ‡ï¼›è‹¥ NLL ä¸»å¯¼ä¸è¶³ï¼Œå¯é€‚å½“ä¸Šè°ƒã€‚

- **losses å¯¼å…¥å¤±è´¥**  
  è‹¥ä½¿ç”¨ `from losses import ...` æŠ¥é”™ï¼Œè¯·æ”¹ç”¨  
  `from losses.loss_function import heteroscedastic_nll, batch_r2, coral_loss`  
  æˆ–åœ¨ `losses/__init__.py` ä¸­æ˜¾å¼å¯¼å‡ºã€‚

---

## ğŸ“Œ é‡è¦è¾“å‡ºä¸çº¦å®š

- **æ¨¡å‹æƒé‡**ï¼ˆé»˜è®¤ä¿å­˜åœ¨ `results/`ï¼‰
  - åŸºçº¿ï¼š`{OPAMP_TYPE}_baseline_model.pth`
  - å¯¹é½ï¼š`{OPAMP_TYPE}_align_hetero_lambda{LAMBDA:.3f}.pth`
  - å¾®è°ƒï¼š`{OPAMP_TYPE}_dualhead_finetuned.pth`
- **Scaler**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šä¿å­˜ `x/y` çš„ scalerï¼ˆè·¯å¾„è§ä½ çš„å®ç°ï¼Œä¸€èˆ¬åœ¨ `results/` ä¸‹ï¼‰
- **åˆ—åçº¦å®š**ï¼š`COLS = ['slewrate_pos', 'dc_gain', 'ugf', 'phase_margin', 'cmrr']`  
  å…¶ä¸­ `ugf`, `cmrr` åœ¨æ¨ç†è¯„ä¼°æ—¶ä¼š `expm1` åå˜æ¢ã€‚

---

---
## ä¸€äº›å‚è€ƒå…¬å¼
### 1. MSE (Mean Squared Error) - å‡æ–¹è¯¯å·®

#### **å…¬å¼ (Formula)**

$$
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

* $n$: æ ·æœ¬æ•°é‡
* $y_i$: ç¬¬ $i$ ä¸ªæ ·æœ¬çš„çœŸå®å€¼
* $\hat{y}_i$: ç¬¬ $i$ ä¸ªæ ·æœ¬çš„æ¨¡å‹é¢„æµ‹å€¼

#### **æ„ä¹‰ (Meaning)**

MSE è®¡ç®—çš„æ˜¯é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®çš„å¹³æ–¹çš„å¹³å‡å€¼ã€‚å®ƒè¡¡é‡äº†æ¨¡å‹çš„é¢„æµ‹è¯¯å·®ï¼Œå€¼è¶Šå°è¡¨ç¤ºæ¨¡å‹é¢„æµ‹è¶Šç²¾å‡†ã€‚

#### **ç‰¹ç‚¹ä¸åº”ç”¨ (Characteristics & Application)**

---

### 2. MAE (Mean Absolute Error) - å¹³å‡ç»å¯¹è¯¯å·®

#### **å…¬å¼ (Formula)**

$$
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

#### **æ„ä¹‰ (Meaning)**

MAE è®¡ç®—çš„æ˜¯é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹å·®çš„ç»å¯¹å€¼çš„å¹³å‡å€¼ã€‚å®ƒç›´æ¥è¡¡é‡äº†é¢„æµ‹å€¼çš„å¹³å‡è¯¯å·®å¤§å°ã€‚

---

### 3. RÂ² (R-squared) - å†³å®šç³»æ•°

#### **å…¬å¼ (Formula)**

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} = 1 - \frac{\text{MSE}}{\text{Var}(y)}
$$

* $\bar{y}$: æ‰€æœ‰çœŸå®å€¼çš„å¹³å‡å€¼

#### **æ„ä¹‰ (Meaning)**

RÂ² è¡¡é‡çš„æ˜¯æ¨¡å‹èƒ½å¤Ÿè§£é‡Šçš„ç›®æ ‡å˜é‡æ–¹å·®çš„æ¯”ä¾‹ã€‚å®ƒçš„å–å€¼èŒƒå›´é€šå¸¸åœ¨ 0 åˆ° 1 ä¹‹é—´ï¼ˆä½†åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¸ºè´Ÿï¼‰ã€‚

* **$R^2 \approx 1$**: æ¨¡å‹å‡ ä¹è§£é‡Šäº†æ‰€æœ‰çš„å˜å¼‚ï¼Œæ‹Ÿåˆæ•ˆæœéå¸¸å¥½ã€‚
* **$R^2 \approx 0$**: æ¨¡å‹çš„è¡¨ç°å’Œç›´æ¥ç”¨å¹³å‡å€¼è¿›è¡Œé¢„æµ‹å·®ä¸å¤šï¼Œæ‹Ÿåˆæ•ˆæœå¾ˆå·®ã€‚
* **$R^2 < 0$**: æ¨¡å‹æ¯”ç›´æ¥ç”¨å¹³å‡å€¼é¢„æµ‹è¿˜è¦ç³Ÿç³•ã€‚


---

### 4. CORAL (Correlation Alignment Loss) - ç›¸å…³æ€§å¯¹é½æŸå¤±

#### **å…¬å¼ (Formula)**

$$
L_{\text{CORAL}} = \frac{1}{4d^2} \| C_S - C_T \|_F^2
$$

* $d$: ç‰¹å¾çš„ç»´åº¦
* $C_S$: **æºåŸŸ**æ•°æ®ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µ
* $C_T$: **ç›®æ ‡åŸŸ**æ•°æ®ç‰¹å¾çš„åæ–¹å·®çŸ©é˜µ
* $\| \cdot \|_F^2$: çŸ©é˜µçš„å¼—ç½—è´å°¼ä¹Œæ–¯èŒƒæ•°çš„å¹³æ–¹ï¼ˆå³çŸ©é˜µæ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œï¼‰

#### **æ„ä¹‰ (Meaning)**

CORAL æ˜¯ä¸€ç§ç”¨äº**é¢†åŸŸè‡ªé€‚åº” (Domain Adaptation)** çš„æŸå¤±å‡½æ•°ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šé€šè¿‡æœ€å°åŒ–æºåŸŸå’Œç›®æ ‡åŸŸç‰¹å¾åˆ†å¸ƒçš„**äºŒé˜¶ç»Ÿè®¡é‡ï¼ˆåæ–¹å·®ï¼‰**çš„å·®å¼‚ï¼Œæ¥å¯¹é½ä¸¤ä¸ªåŸŸçš„ç‰¹å¾åˆ†å¸ƒã€‚

ç®€å•æ¥è¯´ï¼Œå®ƒè¿«ä½¿æ¨¡å‹å­¦ä¹ ä¸€ç§ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨è¿™ç§è¡¨ç¤ºä¸‹ï¼ŒæºåŸŸæ•°æ®ç‰¹å¾å†…éƒ¨çš„ç›¸å…³æ€§å’Œç›®æ ‡åŸŸæ•°æ®ç‰¹å¾å†…éƒ¨çš„ç›¸å…³æ€§å˜å¾—å°½å¯èƒ½ä¸€è‡´ã€‚

---

### 5. Heteroscedastic NLL - å¼‚æ–¹å·®è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±

#### **å…¬å¼ (Formula)**

å‡è®¾é¢„æµ‹çš„è¯¯å·®æœä»é«˜æ–¯åˆ†å¸ƒ $N(\mu, \sigma^2)$ï¼Œåˆ™æŸå¤±å‡½æ•°ä¸ºï¼š

$$
L_{\text{NLL}} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{(y_i - \mu_i)^2}{2\sigma_i^2} + \frac{1}{2}\log(\sigma_i^2) \right)
$$

åœ¨æ‚¨çš„ä»£ç ä¸­ï¼Œæ¨¡å‹ç›´æ¥é¢„æµ‹ $\mu_i$ å’Œå¯¹æ•°æ–¹å·® $\text{logvar}_i = \log(\sigma_i^2)$ï¼Œæ‰€ä»¥å…¬å¼å˜ä¸ºï¼š

$$
L_{\text{NLL}} = \frac{1}{n} \sum_{i=1}^{n} \left( \frac{(y_i - \mu_i)^2}{2\exp(\text{logvar}_i)} + \frac{1}{2}\text{logvar}_i \right)
$$

#### **æ„ä¹‰ (Meaning)**

è¿™ä¸ªæŸå¤±å‡½æ•°è®©æ¨¡å‹åŒæ—¶å­¦ä¹ ä¸¤ä»¶äº‹ï¼š
1.  **é¢„æµ‹å‡å€¼ ($\mu_i$)**ï¼šå³æ¨¡å‹çš„ç›´æ¥é¢„æµ‹å€¼ã€‚
2.  **é¢„æµ‹ä¸ç¡®å®šæ€§ ($\sigma_i^2$)**ï¼šæ¨¡å‹ä¸º**æ¯ä¸€ä¸ª**é¢„æµ‹ç»™å‡ºä¸€ä¸ªæ–¹å·®ï¼Œä»£è¡¨å…¶ç½®ä¿¡åº¦ã€‚

* **ç¬¬ä¸€é¡¹ $\frac{(y_i - \mu_i)^2}{2\sigma_i^2}$**: æƒ©ç½šé¢„æµ‹è¯¯å·®ã€‚è¿™ä¸ªæƒ©ç½šä¼šè¢«æ¨¡å‹é¢„æµ‹çš„æ–¹å·® $\sigma_i^2$ æ‰€è°ƒèŠ‚ã€‚å¦‚æœæ¨¡å‹å¯¹æŸä¸ªé¢„æµ‹å¾ˆä¸è‡ªä¿¡ï¼ˆç»™å‡ºäº†ä¸€ä¸ªå¤§çš„ $\sigma_i^2$ï¼‰ï¼Œé‚£ä¹ˆå³ä½¿è¯¯å·®å¾ˆå¤§ï¼Œè¿™ä¸€é¡¹çš„æƒ©ç½šä¹Ÿä¼šå˜å°ã€‚
* **ç¬¬äºŒé¡¹ $\frac{1}{2}\log(\sigma_i^2)$**: æ­£åˆ™åŒ–é¡¹ã€‚å®ƒæƒ©ç½šæ¨¡å‹æ— ç†ç”±åœ°ç»™å‡ºè¿‡å¤§çš„æ–¹å·®ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸€é¡¹ï¼Œæ¨¡å‹ä¼šå·æ‡’ï¼Œå¯¹æ‰€æœ‰ç‚¹éƒ½é¢„æµ‹æå¤§çš„æ–¹å·®æ¥æœ€å°åŒ–ç¬¬ä¸€é¡¹æŸå¤±ã€‚
