# eda-for-transfer-learning
for dataloaderï¼š
  å¥½çš„ï¼Œè¿™äº›å¯è§†åŒ–å›¾è¡¨éå¸¸ç›´è§‚åœ°æ­ç¤ºäº†æ•°æ®çš„æ ¸å¿ƒç‰¹å¾ã€‚æˆ‘ä»¬æ¥é€ä¸€åˆ†æï¼Œå¹¶åŸºäºæ­¤è®¾è®¡ä¸€ä¸ªå¥å£®ã€åˆç†çš„æ•°æ®é¢„å¤„ç†æ–¹æ³•ã€‚

            ### **æ•°æ®ç‰¹å¾åˆ†æ**
            
            ä»ä½ æä¾›çš„å›¾ç‰‡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥æ€»ç»“å‡ºä¸‰ä¸ªå…³é”®çš„ã€å¿…é¡»åœ¨é¢„å¤„ç†ä¸­è§£å†³çš„ç‰¹æ€§ï¼š
            
            1.  **è¾“å…¥ç‰¹å¾åˆ†å¸ƒå¯¹é½ (Input Feature Distributions are Aligned)**:
                *   **è§‚å¯Ÿ**: æŸ¥çœ‹ `w1`, `w2`, `w3`, `l1`, `l2`, `l3`, å’Œ `ibias` çš„å›¾ã€‚æ— è®ºæ˜¯å‡åŒ€åˆ†å¸ƒ (`w1`, `w2`, `w3`) è¿˜æ˜¯ç¦»æ•£åˆ†å¸ƒ (`l1`, `l2`, `l3`, `ibias`)ï¼Œå·¥è‰ºA (è“è‰²) å’Œå·¥è‰ºB (çº¢è‰²) çš„åˆ†å¸ƒå½¢çŠ¶å’ŒèŒƒå›´**å‡ ä¹å®Œå…¨ä¸€è‡´**ã€‚
                *   **ç»“è®º**: è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„æ¶ˆæ¯ã€‚å®ƒæ„å‘³ç€ä¸¤ä¸ªå·¥è‰ºçš„æ•°æ®æ˜¯åœ¨ç›¸åŒçš„è®¾è®¡ç©ºé—´å†…è¿›è¡Œé‡‡æ ·çš„ã€‚æˆ‘ä»¬çš„æ¨¡å‹ä¸éœ€è¦å¤„ç†è¾“å…¥åŸŸçš„åç§»ï¼Œå¯ä»¥ä¸“æ³¨äºå­¦ä¹ ä»è¿™ä¸ª**å…±åŒçš„è¾“å…¥ç©ºé—´**åˆ°**ä¸åŒçš„è¾“å‡ºç©ºé—´**çš„æ˜ å°„å…³ç³»ã€‚
            
            2.  **è¾“å‡ºæ€§èƒ½å­˜åœ¨æ˜¾è‘—çš„å·¥è‰ºæ¼‚ç§» (Significant Process Shift in Outputs)**:
                *   **è§‚å¯Ÿ**: `dc_gain` çš„å›¾æ˜¯è¿™ä¸ªç°è±¡æœ€å…¸å‹çš„ä¾‹å­ã€‚å·¥è‰ºAçš„åˆ†å¸ƒé›†ä¸­åœ¨100ä»¥ä¸‹ï¼Œè€Œå·¥è‰ºBçš„åˆ†å¸ƒåˆ™ä»150å¼€å§‹ã€‚**ä¸¤ä¸ªåˆ†å¸ƒå‡ ä¹æ²¡æœ‰ä»»ä½•é‡å **ã€‚`slewrate_pos` çš„åˆ†å¸ƒä¹Ÿæ˜¾ç¤ºå‡ºå½¢æ€ä¸Šçš„å·®å¼‚ã€‚
                *   **ç»“è®º**: è¿™æ­£æ˜¯æœ¬æ¬¡ç«èµ›çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚å¯¹äºç›¸åŒçš„è¾“å…¥å‚æ•°ï¼Œä¸¤ä¸ªå·¥è‰ºäº§ç”Ÿçš„æ€§èƒ½æŒ‡æ ‡æ˜¯ç³»ç»Ÿæ€§ä¸åŒçš„ã€‚æˆ‘ä»¬çš„æ¨¡å‹**å¿…é¡»å­¦ä¹ åˆ°è¿™ç§â€œæ¼‚ç§»â€æˆ–â€œåç§»â€**ã€‚ä¸€ä¸ªåªåœ¨å·¥è‰ºAä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œåœ¨é¢„æµ‹å·¥è‰ºBçš„ `dc_gain` æ—¶ä¼šäº§ç”Ÿå·¨å¤§çš„ã€ç³»ç»Ÿæ€§çš„åå·®ã€‚
            
            3.  **éƒ¨åˆ†è¾“å‡ºæ€§èƒ½æ•°æ®æåº¦åæ–œ/å­˜åœ¨æç«¯å€¼ (Highly Skewed Outputs / Outliers)**:
                *   **è§‚å¯Ÿ**: `ugf` (å•ä½å¢ç›Šå¸¦å®½) çš„å›¾æœ€ä¸ºçªå‡ºã€‚ç»å¤§å¤šæ•°æ•°æ®ç‚¹éƒ½æŒ¤åœ¨æ¥è¿‘0çš„ä¸€ç«¯ï¼Œå½¢æˆä¸€ä¸ªéå¸¸é«˜çš„å³°ï¼ŒåŒæ—¶æœ‰ä¸€æ¡å»¶ä¼¸åˆ° `1e11` çš„æé•¿çš„å°¾å·´ã€‚
                *   **ç»“è®º**: è¿™ç§æåº¦åæ–œçš„åˆ†å¸ƒå¯¹ç¥ç»ç½‘ç»œè®­ç»ƒæ˜¯**ç¾éš¾æ€§çš„**ã€‚å¦‚æœä½¿ç”¨å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œé‚£å‡ ä¸ª `1e11` çš„æç«¯å€¼æ‰€äº§ç”Ÿçš„å·¨å¤§è¯¯å·®å°†å®Œå…¨ä¸»å¯¼æ¨¡å‹çš„æ¢¯åº¦æ›´æ–°ï¼Œå¯¼è‡´æ¨¡å‹åªå…³æ³¨å¦‚ä½•æ‹Ÿåˆé‚£å‡ ä¸ªç‚¹ï¼Œè€Œå¿½ç•¥äº†ç»å¤§å¤šæ•°æ­£å¸¸æ•°æ®ï¼Œæœ€ç»ˆæ¨¡å‹æ€§èƒ½ä¼šéå¸¸å·®ã€‚
            
            ### **åˆç†çš„DataLoaderæ•°æ®é¢„å¤„ç†æ–¹æ³•**
            
            åŸºäºä»¥ä¸Šåˆ†æï¼Œä¸€ä¸ªåˆç†çš„æ•°æ®é¢„å¤„ç†æµç¨‹åº”è¯¥åŒ…å«ä»¥ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼Œå¹¶å°è£…åœ¨ä¸€ä¸ª `DataLoader` æˆ–æ•°æ®å‡†å¤‡å‡½æ•°ä¸­ã€‚
            
            ---
            
            #### **æ­¥éª¤ 1: å¯¹æ•°å˜æ¢ (Log Transform) å¤„ç†åæ–œæ•°æ®**
            
            **ç›®çš„**: è§£å†³ç¬¬3ä¸ªé—®é¢˜â€”â€”æ•°æ®æåº¦åæ–œã€‚
            
            å¯¹æ•°å˜æ¢æ˜¯å¤„ç†è¿™ç§é•¿å°¾åˆ†å¸ƒã€ä½¿æ•°æ®æ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒçš„ç»ä½³æ–¹æ³•ã€‚å®ƒå¯ä»¥æå¤§åœ°å‹ç¼©æ•°æ®çš„èŒƒå›´ï¼Œé™ä½æç«¯å€¼çš„å½±å“ã€‚
            
            *   **æ“ä½œ**: å¯¹ `ugf` å’Œ `cmrr` (æ ¹æ®æˆ‘ä»¬ä¹‹å‰çš„ç»Ÿè®¡åˆ†æï¼Œ`cmrr` ä¹Ÿæœ‰ç±»ä¼¼é—®é¢˜) è¿™ä¸¤åˆ—åº”ç”¨ `np.log1p()` å‡½æ•°ã€‚`log1p(x)` è®¡ç®— `log(1+x)`ï¼Œå¥½å¤„æ˜¯å½“ `x=0` æ—¶ä¸ä¼šå‡ºé”™ã€‚
            
            #### **æ­¥éª¤ 2: æ•°æ®æ ‡å‡†åŒ– (Standardization)**
            
            **ç›®çš„**: è§£å†³ä¸åŒç‰¹å¾å’Œç›®æ ‡ä¹‹é—´å·¨å¤§çš„æ•°å€¼èŒƒå›´å·®å¼‚ï¼Œå¹¶ä¸ºæ¨¡å‹å­¦ä¹ â€œå·¥è‰ºæ¼‚ç§»â€æä¾›ä¿¡å·ã€‚
            
            ç¥ç»ç½‘ç»œå¯¹è¾“å…¥æ•°æ®çš„å°ºåº¦éå¸¸æ•æ„Ÿã€‚æ ‡å‡†åŒ–ï¼ˆå°†æ•°æ®è½¬æ¢ä¸ºå‡å€¼ä¸º0ï¼Œæ–¹å·®ä¸º1ï¼‰æ˜¯å¿…ä¸å¯å°‘çš„æ­¥éª¤ã€‚
            
            *   **æ“ä½œ**: ä½¿ç”¨ `sklearn.preprocessing.StandardScaler`ã€‚
            *   **å…³é”®ç­–ç•¥**:
                1.  **åˆ›å»ºä¸¤ä¸ªScaler**: ä¸€ä¸ªç”¨äºè¾“å…¥ç‰¹å¾ `X` (`x_scaler`)ï¼Œä¸€ä¸ªç”¨äºï¼ˆç»è¿‡å¯¹æ•°å˜æ¢åçš„ï¼‰è¾“å‡ºç›®æ ‡ `Y` (`y_scaler`)ã€‚
                2.  **åœ¨å·¥è‰ºA (Source) æ•°æ®ä¸Š `fit` Scaler**: æˆ‘ä»¬å°†å·¥è‰ºAä½œä¸ºçŸ¥è¯†çš„æ¥æºã€‚å› æ­¤ï¼Œæˆ‘ä»¬**åªåœ¨å·¥è‰ºAçš„æ•°æ®ä¸Šè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®** (`x_scaler.fit(X_source)`, `y_scaler.fit(y_source)`)ã€‚
                3.  **ç”¨åŒä¸€ä¸ªScalerå» `transform` æ‰€æœ‰æ•°æ®**: ä½¿ç”¨åœ¨å·¥è‰ºAä¸Š `fit` å¥½çš„ `scaler` å»è½¬æ¢å·¥è‰ºAå’Œå·¥è‰ºBçš„æ•°æ®ã€‚
            
            **ğŸ’¡ ä¸ºä»€ä¹ˆè¿™æ ·åšï¼Ÿ**
            è¿™è‡³å…³é‡è¦ã€‚å¦‚æœæˆ‘ä»¬åˆ†åˆ«åœ¨Aå’ŒBä¸Š `fit`ï¼Œé‚£ä¹ˆä¸¤ç»„æ•°æ®çš„ `dc_gain` éƒ½ä¼šè¢«è½¬æ¢æˆå‡å€¼ä¸º0çš„åˆ†å¸ƒï¼Œæ¨¡å‹å°±**çœ‹ä¸åˆ°**é‚£ä¸ªå·¨å¤§çš„å·¥è‰ºæ¼‚ç§»äº†ã€‚è€Œé€šè¿‡åœ¨Aä¸Š `fit` å¹¶åŒæ—¶ `transform` Aå’ŒBï¼Œå·¥è‰ºAçš„ `dc_gain` è½¬æ¢åå‡å€¼ä¸º0ï¼Œè€Œå·¥è‰ºBçš„ `dc_gain` è½¬æ¢åä¼šæ˜¯ä¸€ä¸ª**æ˜¾è‘—å¤§äº0çš„åˆ†å¸ƒ**ã€‚è¿™ä¸ªéé›¶çš„å‡å€¼ï¼Œå°±æ˜¯æ¨¡å‹éœ€è¦å­¦ä¹ çš„â€œå·¥è‰ºæ¼‚ç§»â€ä¿¡å·ï¼
            
            #### **æ­¥éª¤ 3: æ•°æ®é›†åˆ’åˆ† (Splitting)**
            
            **ç›®çš„**: ä¸ºæ¨¡å‹çš„é¢„è®­ç»ƒã€å¾®è°ƒå’ŒéªŒè¯å‡†å¤‡å¥½æ•°æ®é›†ã€‚
            
            *   **æ“ä½œ**:
                1.  **é¢„è®­ç»ƒé›†**: å¤„ç†åçš„å·¥è‰ºAæ•°æ® `(X_source_scaled, y_source_scaled)`ã€‚
                2.  **å¾®è°ƒ/éªŒè¯é›†**: å°†å¤„ç†åçš„å·¥è‰ºBæ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆä¾‹å¦‚ï¼Œ80%ç”¨äºå¾®è°ƒè®­ç»ƒï¼Œ20%ç”¨äºéªŒè¯ï¼‰ã€‚





---

## ğŸš€ TL;DR å¿«é€Ÿä¸Šæ‰‹

```bash

python -m data_loader.cli --opamp 5t_opamp --val-split 0.2 --seed 42

# 3) è®­ç»ƒåŸºçº¿ MLPï¼ˆä¿å­˜ baseline æƒé‡ï¼‰
python -m training.train

# 4) è®­ç»ƒ AlignHeteroMLP + CORALï¼ˆä¿å­˜ align_hetero æƒé‡ï¼‰
python -m training.train_align_coral

# 5) å¾®è°ƒ DualHeadMLPï¼ˆåˆ†é˜¶æ®µè®­ç»ƒï¼Œä¿å­˜ dualhead å¾®è°ƒæƒé‡ï¼‰
python -m fine_tune.fine_tune

# 6) é›†æˆæ¨ç† & æŒ‡æ ‡è¯„ä¼°ï¼ˆåæ ‡å‡†åŒ–åˆ°ç‰©ç†å•ä½ï¼‰
python -m inference.infer_ensemble
```

---

## ğŸ“¦ é¡¹ç›®æ¦‚è§ˆ

æœ¬å·¥ç¨‹é¢å‘æ¨¡æ‹Ÿç”µè·¯ï¼ˆå¦‚è¿æ”¾ï¼‰å¤šç›®æ ‡å›å½’ï¼Œæä¾›ï¼š

- **æ•°æ®ç®¡é“**ï¼šåŠ è½½ã€é¢„å¤„ç†ï¼ˆå« `log1p`ã€æ ‡å‡†åŒ–ï¼‰ä»¥åŠä¿å­˜/å¤ç”¨ scalerã€‚  
- **æ¨¡å‹åº“**ï¼š`MLP`ã€`AlignHeteroMLP`ï¼ˆå¼‚æ–¹å·®ï¼‰ã€`DualHeadMLP`ï¼ˆåŒå¤´å¾®è°ƒï¼‰ã€‚  
- **è®­ç»ƒè„šæœ¬**ï¼šåŸºçº¿è®­ç»ƒã€å¸¦ CORAL çš„è·¨åŸŸå¯¹é½è®­ç»ƒã€åˆ†é˜¶æ®µå¾®è°ƒï¼ˆL2-SP æ­£åˆ™ï¼‰ã€‚  
- **æ¨ç†è„šæœ¬**ï¼šä¸¤æ¨¡å‹ä¸ç¡®å®šæ€§æ„ŸçŸ¥åŠ æƒ + MSE æƒé‡èåˆï¼Œè‡ªåŠ¨åæ ‡å‡†åŒ–ä¸è¯„ä¼°ã€‚

---

## ğŸ—‚ ç›®å½•ç»“æ„

```
src_new/
â”œâ”€â”€ config.py                 # ç»Ÿä¸€ç®¡ç†è¶…å‚æ•°&è®¾å¤‡
â”‚---inverse_mdn.py
----inverse_opt.py
â”œâ”€â”€ data_loader/
â”‚   â”œâ”€â”€ __init__.py           # æš´éœ² load/preprocess/split/scale çš„ç»Ÿä¸€API
â”‚   â”œâ”€â”€ cli.py                # å‘½ä»¤è¡Œå¿«é€ŸéªŒè¯æ•°æ®åŠ è½½
â”‚   â”œâ”€â”€ data_loader.py        # åŠ è½½+é¢„å¤„ç†+åˆ’åˆ†
â”‚   â””â”€â”€ scaler_utils.py       # ä¿å­˜/åŠ è½½ x,y çš„ StandardScaler
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mlp.py                # åŸºç¡€ MLP
â”‚   â”œâ”€â”€ align_hetero.py       # AlignHeteroMLPï¼ˆbackbone + hetero_headï¼‰
â”‚   â””â”€â”€ model_utils.py        # ä¿å­˜/åŠ è½½æƒé‡ï¼Œä¸»å¹²è¿ç§»å·¥å…·
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # è®­ç»ƒåŸºçº¿ MLP
â”‚   â””â”€â”€ train_align_coral.py  # AlignHeteroMLP + CORAL è®­ç»ƒ
â”‚
â”œâ”€â”€ fine_tune/
â”‚   â””â”€â”€ fine_tune.py          # DualHeadMLP åˆ†é˜¶æ®µå¾®è°ƒï¼ˆbiasâ†’headâ†’è§£å†»æœ€åå±‚ï¼‰
â”‚
â”œâ”€â”€ inference/
â”‚   â””â”€â”€ infer_ensemble.py     # é›†æˆæ¨ç†ï¼ˆæ¸©åº¦æ ‡å®š+ç²¾åº¦/MSEåŠ æƒï¼‰ï¼Œè¯„ä¼°MSE/MAE/RÂ²
â”‚
â””â”€â”€ losses/
    â””â”€â”€ loss_function.py      # heteroscedastic_nll / batch_r2 / coral_loss
# æ³¨ï¼šè‹¥ä½ ä½¿ç”¨ `from losses import x`ï¼Œè¯·ç¡®ä¿ losses/__init__.py å·²å¯¼å‡ºä¸Šè¿°å‡½æ•°ã€‚
```

---

## ğŸ”§ å®‰è£…ä¸ä¾èµ–

**Python**ï¼šå»ºè®® 3.10  
**PyTorch**ï¼šæ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬å®‰è£…ï¼ˆhttps://pytorch.org/ï¼‰  

è‹¥æ—  `requirements.txt`ï¼Œå¯ä½¿ç”¨ä¸‹åˆ—åŸºç¡€ä¾èµ–ï¼ˆæŒ‰éœ€å¢å‡ï¼‰ï¼š

```txt
numpy>=1.24
scikit-learn>=1.3
joblib>=1.3
torch>=2.1
tqdm>=4.66
```

---

## âš™ï¸ é…ç½®ï¼ˆ`config.py`ï¼‰

ç»Ÿä¸€ç®¡ç†æ‰€æœ‰è¶…å‚æ•°ï¼Œéšç”¨éšæ”¹ï¼Œè®­ç»ƒè„šæœ¬è‡ªåŠ¨è¯»å–ï¼š

```python
# å…³é”®å‚æ•°ç¤ºä¾‹ï¼ˆå®é™…ä»¥ä½ çš„ config.py ä¸ºå‡†ï¼‰
OPAMP_TYPE   = '5t_opamp'
DEVICE       = 'cuda'  # è‡ªåŠ¨æ£€æµ‹åŒæ ·å¯ï¼š'cuda' if torch.cuda.is_available() else 'cpu'

# è®­ç»ƒ
EPOCHS       = 50
PATIENCE     = 10
LEARNING_RATE= 1e-4
BATCH_SIZE   = 256

# æ¨¡å‹
HIDDEN_DIM   = 512
NUM_LAYERS   = 6
DROPOUT_RATE = 0.1

# å¯¹é½/å¾®è°ƒ
LAMBDA_CORAL = 0.05
ALPHA_R2     = 1.0
L2SP_LAMBDA  = 1e-4
LR_BIAS      = 3e-4
LR_HEAD      = 1e-4
LR_UNFREEZE  = 5e-5
WEIGHT_DECAY = 1e-4
```

> **å»ºè®®**ï¼šä»…é€šè¿‡ `config.py` æ”¹åŠ¨è¶…å‚æ•°ï¼Œé¿å…åœ¨è„šæœ¬å†…â€œç¡¬ç¼–ç â€ï¼Œä¿è¯å…¨å·¥ç¨‹ä¸€è‡´ã€‚

---

## ğŸ§ª æ•°æ®ä¸é¢„å¤„ç†

- **æ•°æ®å…¥å£**ï¼š`data_loader.get_data_and_scalers(opamp_type=OPAMP_TYPE)`  
  è¿”å›ï¼š
  ```python
  {
    "source": (X_source_scaled, y_source_scaled),
    "target_train": (X_target_train, y_target_train),
    "target_val": (X_target_val, y_target_val),
    "x_scaler": x_scaler,
    "y_scaler": y_scaler,
  }
  ```
- é¢„å¤„ç†åŒ…å« `log1p`ï¼ˆå¯¹ç‰¹å®šç›®æ ‡ï¼Œå¦‚ `ugf`, `cmrr`ï¼‰å’Œæ ‡å‡†åŒ–ã€‚åæ ‡å‡†åŒ–ä¸ `expm1` åœ¨æ¨ç†é˜¶æ®µè‡ªåŠ¨å®Œæˆã€‚

---

## ğŸ— æ¨¡å‹ä¸æŸå¤±ï¼ˆAPI é€Ÿè§ˆï¼‰

### æ¨¡å‹

```python
from models import MLP, AlignHeteroMLP, DualHeadMLP

# MLP
m = MLP(input_dim, output_dim)                 # forward(x) -> y_hat

# AlignHeteroMLPï¼ˆå¼‚æ–¹å·®ï¼‰
m = AlignHeteroMLP(input_dim, output_dim)      # forward(x) -> (mu, logvar, features)

# DualHeadMLPï¼ˆå¾®è°ƒåŒå¤´ï¼‰
yB = model(x, domain='B')                      # æŒ‡å®šä½¿ç”¨ B å¤´
```

### æ¨¡å‹å·¥å…·

```python
from models.model_utils import (
  load_backbone_from_trained_mlp, save_model, load_model
)

load_backbone_from_trained_mlp(pretrained_mlp, align_model)
save_model(model, 'results/xxx.pth')
load_model(model, 'results/xxx.pth')
```

### æŸå¤±å‡½æ•°

```python
# è‹¥æ—  __init__.py å¯¼å‡ºï¼Œè¯·æ”¹ä¸ºï¼šfrom losses.loss_function import ...
from losses import heteroscedastic_nll, batch_r2, coral_loss
```

- `heteroscedastic_nll(mu, logvar, y, reduction='mean')`
- `batch_r2(y_true, y_pred, eps=1e-8)`
- `coral_loss(feat_a, feat_b, unbiased=True, eps=1e-6)`

---
ä¸€ã€‚æ­£å‘è®¾è®¡
## ğŸƒâ€â™‚ï¸ è®­ç»ƒ/å¾®è°ƒ/æ¨ç†æµç¨‹

### 1) è®­ç»ƒåŸºçº¿ MLP

```bash
python -m training.train
```

- **è¾“å…¥**ï¼š`source` ä½œä¸ºè®­ç»ƒé›†ï¼Œ`target_val` ä½œä¸ºéªŒè¯é›†  
- **è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_baseline_model.pth`  
- **æ—¥å¿—**ï¼šæ‰“å°æ¯è½® Train/Val MSE

### 2) è®­ç»ƒ AlignHeteroMLP + CORAL

```bash
python -m training.train_align_coral
```

- è½½å…¥åŸºçº¿ MLP ä½œä¸º **backbone** åˆå§‹æƒé‡  
- ç›®æ ‡åŸŸ `B` ä¸Šè®­ç»ƒå¼‚æ–¹å·® NLL + `RÂ²`ï¼ˆè½¬åŒ–ä¸ºæŸå¤±ï¼‰+ **CORAL**ï¼ˆè·¨åŸŸç‰¹å¾å¯¹é½ï¼‰  
- **è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_align_hetero_lambda{LAMBDA_CORAL:.3f}.pth`  
- **éªŒè¯æŒ‡æ ‡**ï¼šval NLLï¼ˆè¶Šå°è¶Šå¥½ï¼‰

### 3) åˆ†é˜¶æ®µå¾®è°ƒ DualHeadMLP

```bash
python -m fine_tune.fine_tune
```

åˆ†ä¸‰é˜¶æ®µï¼ˆå‡åœ¨ç›®æ ‡åŸŸ `B`ï¼‰ï¼š
1. **Bias æ ¡å‡†**ï¼šä»…è®­ç»ƒ `head_B.bias`
2. **è®­ç»ƒ B å¤´æƒé‡**ï¼šå¯ç”¨ L2-SP æ­£åˆ™ï¼Œçº¦æŸåç¦»é¢„è®­ç»ƒä¸»å¹²
3. **éƒ¨åˆ†è§£å†»**ï¼šè§£å†»ä¸»å¹²æœ€åä¸€å±‚ + B å¤´

**è¾“å‡º**ï¼š`results/{OPAMP_TYPE}_dualhead_finetuned.pth`

> ä½¿ç”¨åˆ°çš„å…³é”®æ¥å£ï¼š
> - `run_epoch(model, loader, optimizer, loss_fn, phase, pretrained_state=None)`
> - `main()`ï¼šç»„ç»‡ä¸Šè¿°ä¸‰é˜¶æ®µè®­ç»ƒä¸æ—©åœ

### 4) é›†æˆæ¨ç†ä¸è¯„ä¼°

```bash
python -m inference.infer_ensemble
```

åšäº†ä»¥ä¸‹å·¥ä½œï¼š
- è½½å…¥ä¸¤ä¸ªå¼‚æ–¹å·®æ¨¡å‹ï¼ˆç¤ºä¾‹ï¼š`align_hetero` ä¸ `target_only_hetero`ï¼‰  
- **æ¸©åº¦æ ‡å®š**ï¼ˆé—­å¼è§£ï¼‰æ ¡å‡†æ–¹å·®  
- **æ ·æœ¬çº§ç²¾åº¦æƒé‡** + **æŒ‡æ ‡çº§ MSE æƒé‡** èåˆ  
- åæ ‡å‡†åŒ– & `expm1`ï¼ˆå¯¹ `ugf`, `cmrr`ï¼‰  
- è¾“å‡ºæ¯ä¸ªæŒ‡æ ‡çš„ **MSE/MAE/RÂ²**

**æ‰“å°ç¤ºä¾‹**ï¼š
```
=== Ensemble on B-VAL (ç‰©ç†å•ä½) ===
slewrate_pos    MSE=...  MAE=...  R2=...
dc_gain         MSE=...  MAE=...  R2=...
ugf             MSE=...  MAE=...  R2=...
phase_margin    MSE=...  MAE=...  R2=...
cmrr            MSE=...  MAE=...  R2=...
```
äºŒ. åå‘è®¾è®¡ï¼ˆinverse_mdn.pyï¼‰
åŠŸèƒ½ï¼šé€šè¿‡è®­ç»ƒæ··åˆå¯†åº¦ç½‘ç»œï¼ˆMDNï¼‰æ¥å­¦ä¹ ä»ç›®æ ‡å€¼ y åˆ°è¾“å…¥å€¼ x çš„æ˜ å°„å…³ç³»ã€‚æ”¯æŒä¸¤ç§æ¨¡å¼ï¼šè®­ç»ƒæ¨¡å¼å’Œé‡‡æ ·æ¨¡å¼ã€‚

1.1 è®­ç»ƒæ¨¡å¼
åœ¨è®­ç»ƒæ¨¡å¼ä¸‹ï¼Œä½¿ç”¨ä¸€ç»„æ ‡å‡†åŒ–çš„ç›®æ ‡æ•°æ® y_scaled å’Œè¾“å…¥æ•°æ® x_scaled æ¥è®­ç»ƒä¸€ä¸ª MDN æ¨¡å‹ã€‚æ¨¡å‹å°†å­¦ä¹ ä»ç›®æ ‡è¾“å‡ºåˆ°è¾“å…¥çš„æ˜ å°„ã€‚è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹æƒé‡å’Œæ ‡å‡†åŒ–å™¨ï¼ˆscalerï¼‰å°†è¢«ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„ã€‚

ç”¨æ³•ï¼š

python src/inverse_mdn.py --opamp 5t_opamp \
                          --save results/mdn_5t.pth \
                          --components 10 \
                          --hidden 256 \
                          --layers 4 \
                          --batch-size 128 \
                          --epochs 60 \
                          --lr 1e-3


1.2 é‡‡æ ·æ¨¡å¼
åœ¨é‡‡æ ·æ¨¡å¼ä¸‹ï¼Œç”¨æˆ·æä¾›ä¸€ä¸ªç›®æ ‡ y_targetï¼Œå·¥å…·å°†åŸºäºå·²è®­ç»ƒçš„ MDN æ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰è¾“å…¥ x_scaledï¼Œè¿™äº›è¾“å…¥èƒ½å¤Ÿä½¿å¾—æ¨¡å‹çš„è¾“å‡ºæ¥è¿‘ç›®æ ‡ y_targetã€‚

ç”¨æ³•ï¼š

python src/inverse_mdn.py --sample \
                          --model results/mdn_5t.pth \
                          --y-target "2.5e8,200,1.5e6,65,20000" \
                          --n 64 \
                          --out results/inverse/init_64.npy


2. åå‘ä¼˜åŒ–ï¼ˆinverse_opt.pyï¼‰
åŠŸèƒ½ï¼šä½¿ç”¨åå‘ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡ä¼˜åŒ–è¾“å…¥ x ä½¿å¾—æ¨¡å‹çš„è¾“å‡º y æ»¡è¶³ç”¨æˆ·æŒ‡å®šçš„ç›®æ ‡æˆ–çº¦æŸæ¡ä»¶ã€‚æ”¯æŒå¤šç§ç›®æ ‡ç±»å‹ï¼ˆæœ€å°åŒ–ã€æœ€å¤§åŒ–ã€ç›®æ ‡å€¼ã€èŒƒå›´ç­‰ï¼‰å’Œçº¦æŸæ¡ä»¶ï¼ˆå¦‚ ugf_band å’Œ pm_bandï¼‰ã€‚

2.1 åå‘ä¼˜åŒ–
åœ¨åå‘ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œå·¥å…·ä¼šä½¿ç”¨å¤šä¸ªåˆå§‹ç‚¹å¯¹è¾“å…¥ x è¿›è¡Œä¼˜åŒ–ï¼Œæœ€ç»ˆå¾—åˆ°ä¸€ä¸ªæœ€ä¼˜çš„è¾“å…¥ x_scaledï¼Œä½¿å¾—å…¶é¢„æµ‹è¾“å‡º y_scaled è¾¾åˆ°ç»™å®šç›®æ ‡ã€‚ä¼˜åŒ–ç»“æœå°†ä¿å­˜åœ¨æŒ‡å®šçš„ç›®å½•ä¸­ã€‚

ç”¨æ³•ï¼š

python src/inverse_opt.py --opamp 5t_opamp \
                          --ckpt results/5t_opamp_align_hetero_lambda0.050.pth \
                          --model-type align_hetero \
                          --y-target "2.5e8,200,1.5e6,65,20000" \
                          --goal "min,min,range,range,min" \
                          --ugf-band "8.0e5:2.0e6" \
                          --pm-band "60:75" \
                          --weights "0.05,0.40,0.90,0.10,0.65" \
                          --prior 1e-3 \
                          --init-npy results/inverse/init_1024.npy \
                          --n-init 1024 --steps 900 --lr 0.002 \
                          --finish-lbfgs 80 \
                          --save-dir results/inverse/try_hybrid_constrained_scaled_v2
---

## ğŸ§­ å¸¸è§é—®é¢˜ï¼ˆFAQ / Troubleshootingï¼‰

- **æ‰¾ä¸åˆ° baseline æƒé‡**  
  å…ˆè¿è¡Œï¼š`python -m training.train`ï¼Œä¼šåœ¨ `results/` ä¸‹ç”Ÿæˆ `{OPAMP_TYPE}_baseline_model.pth`ã€‚

- **CUDA ä¸å¯ç”¨/æ˜¾å­˜ä¸è¶³**  
  åœ¨ `config.py` å°† `DEVICE='cpu'` æˆ–å‡å° `BATCH_SIZE`/`HIDDEN_DIM`ã€‚

- **å½¢çŠ¶ä¸åŒ¹é…**  
  æ£€æŸ¥ `input_dim = X.shape[1]` ä¸æ¨¡å‹åˆå§‹åŒ–ä¸€è‡´ï¼›`output_dim = y.shape[1]` ä¸ä»»åŠ¡æŒ‡æ ‡æ•°é‡ä¸€è‡´ã€‚

- **CORAL æƒé‡è®¾ç½®**  
  `LAMBDA_CORAL` è¿‡å¤§å¯èƒ½æ‹–æ…¢æ”¶æ•›ï¼›å¯å…ˆä» `0.01~0.05` ç½‘æ ¼æœç´¢ã€‚

- **RÂ² æŸå¤±æƒé‡**  
  `ALPHA_R2` æ§åˆ¶ RÂ² ç›®æ ‡ï¼›è‹¥ NLL ä¸»å¯¼ä¸è¶³ï¼Œå¯é€‚å½“ä¸Šè°ƒã€‚

- **losses å¯¼å…¥å¤±è´¥**  
  è‹¥ä½¿ç”¨ `from losses import ...` æŠ¥é”™ï¼Œè¯·æ”¹ç”¨  
  `from losses.loss_function import heteroscedastic_nll, batch_r2, coral_loss`  
  æˆ–åœ¨ `losses/__init__.py` ä¸­æ˜¾å¼å¯¼å‡ºã€‚

---

## ğŸ“Œ é‡è¦è¾“å‡ºä¸çº¦å®š

- **æ¨¡å‹æƒé‡**ï¼ˆé»˜è®¤ä¿å­˜åœ¨ `results/`ï¼‰
  - åŸºçº¿ï¼š`{OPAMP_TYPE}_baseline_model.pth`
  - å¯¹é½ï¼š`{OPAMP_TYPE}_align_hetero_lambda{LAMBDA:.3f}.pth`
  - å¾®è°ƒï¼š`{OPAMP_TYPE}_dualhead_finetuned.pth`
- **Scaler**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­ä¼šä¿å­˜ `x/y` çš„ scalerï¼ˆè·¯å¾„è§ä½ çš„å®ç°ï¼Œä¸€èˆ¬åœ¨ `results/` ä¸‹ï¼‰
- **åˆ—åçº¦å®š**ï¼š`COLS = ['slewrate_pos', 'dc_gain', 'ugf', 'phase_margin', 'cmrr']`  
  å…¶ä¸­ `ugf`, `cmrr` åœ¨æ¨ç†è¯„ä¼°æ—¶ä¼š `expm1` åå˜æ¢ã€‚

---

## ğŸ§© å¼€å‘å°è´´å£«

- å·²å¯¹æ¢¯åº¦åš `clip_grad_norm_=1.0`ï¼Œå¯æé«˜è®­ç»ƒç¨³å®šæ€§ã€‚  
- å¤šå¡/æ··åˆç²¾åº¦ï¼šå½“å‰ä»£ç æœªå†…ç½®ï¼Œå¯æŒ‰éœ€æ¥å…¥ `torch.nn.parallel` / AMPã€‚  
- æƒ³è°ƒå‚ï¼Ÿåªæ”¹ `config.py`ï¼Œå…¶ä½™è„šæœ¬æ— éœ€æ”¹åŠ¨ã€‚  
- æƒ³è‡ªå®šä¹‰æŒ‡æ ‡åˆ—ï¼šåŒæ­¥ä¿®æ”¹æ•°æ®é¢„å¤„ç†åŠ `infer_ensemble.py` ä¸­çš„ `COLS` åˆ—è¡¨ä¸åå˜æ¢é€»è¾‘ã€‚



ã€
