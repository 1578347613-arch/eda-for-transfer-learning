
==================================================
=== AUTOMATED TRAINING (MODE: ALIGN_HETERO) ===
==================================================
Tasks: two_stage_opamp
Scripts: train_align_hetero

==================================================
=== STARTING TASK: TWO_STAGE_OPAMP ===
==================================================

--- Running: train_align_hetero for two_stage_opamp ---

[CMD] In directory '/home/mario1578347613/eda-for-transfer-learning-1/src', running: python -m train_align_hetero --opamp two_stage_opamp --restart
--- 开始为 two_stage_opamp 加载数据 ---
已对列 ['ugf', 'cmrr', 'dc_gain', 'slewrate_pos'] 进行 log1p 变换。
StandardScaler 已在工艺 A(Source) 上完成 fit。
A/B 全部数据已完成标准化（基于 A 域的 scaler）。
工艺 A 数据已划分为 80% 训练集 / 20% 验证集。
工艺 B 数据已划分为 80% 训练集 / 20% 验证集。
--- 动态检测到 two_stage_opamp 的维度: Input=13, Output=5 ---
--- [阶段一] two_stage_opamp 启用标准预训练 ---

--- [阶段一] 开始 Backbone 预训练 (配置: {'epochs_pretrain': 4000, 'T_0': 200, 'T_mult': 1}) ---
优化器将在以下 epoch 结束后重置: [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000, 2200, 2400, 2600, 2800, 3000, 3200, 3400, 3600, 3800, 4000]
Pre-train Epoch [1/4000], Train Loss: 0.268032, Val Loss: 0.219726, LR: 1.00e-03
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.219726
Pre-train Epoch [2/4000], Train Loss: 0.199793, Val Loss: 0.171371, LR: 1.00e-03
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.171371
Pre-train Epoch [3/4000], Train Loss: 0.174252, Val Loss: 0.146209, LR: 9.99e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.146209
Pre-train Epoch [4/4000], Train Loss: 0.155493, Val Loss: 0.134474, LR: 9.99e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.134474
Pre-train Epoch [5/4000], Train Loss: 0.146919, Val Loss: 0.129505, LR: 9.98e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.129505
Pre-train Epoch [6/4000], Train Loss: 0.142676, Val Loss: 0.120818, LR: 9.98e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.120818
Pre-train Epoch [7/4000], Train Loss: 0.137849, Val Loss: 0.116927, LR: 9.97e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.116927
Pre-train Epoch [8/4000], Train Loss: 0.130263, Val Loss: 0.111312, LR: 9.96e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.111312
Pre-train Epoch [9/4000], Train Loss: 0.127176, Val Loss: 0.117187, LR: 9.95e-04
Pre-train Epoch [10/4000], Train Loss: 0.123314, Val Loss: 0.111879, LR: 9.94e-04
Pre-train Epoch [11/4000], Train Loss: 0.119969, Val Loss: 0.105742, LR: 9.93e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.105742
Pre-train Epoch [12/4000], Train Loss: 0.116331, Val Loss: 0.106763, LR: 9.91e-04
Pre-train Epoch [13/4000], Train Loss: 0.113336, Val Loss: 0.108423, LR: 9.90e-04
Pre-train Epoch [14/4000], Train Loss: 0.115732, Val Loss: 0.099283, LR: 9.88e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.099283
Pre-train Epoch [15/4000], Train Loss: 0.108503, Val Loss: 0.098019, LR: 9.86e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.098019
Pre-train Epoch [16/4000], Train Loss: 0.107608, Val Loss: 0.097354, LR: 9.84e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.097354
Pre-train Epoch [17/4000], Train Loss: 0.107670, Val Loss: 0.102361, LR: 9.82e-04
Pre-train Epoch [18/4000], Train Loss: 0.105834, Val Loss: 0.096162, LR: 9.80e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.096162
Pre-train Epoch [19/4000], Train Loss: 0.102489, Val Loss: 0.095892, LR: 9.78e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.095892
Pre-train Epoch [20/4000], Train Loss: 0.102974, Val Loss: 0.091900, LR: 9.76e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.091900
Pre-train Epoch [21/4000], Train Loss: 0.099664, Val Loss: 0.093644, LR: 9.73e-04
Pre-train Epoch [22/4000], Train Loss: 0.098997, Val Loss: 0.091953, LR: 9.70e-04
Pre-train Epoch [23/4000], Train Loss: 0.098833, Val Loss: 0.095617, LR: 9.68e-04
Pre-train Epoch [24/4000], Train Loss: 0.097551, Val Loss: 0.094548, LR: 9.65e-04
Pre-train Epoch [25/4000], Train Loss: 0.093664, Val Loss: 0.094653, LR: 9.62e-04
Pre-train Epoch [26/4000], Train Loss: 0.093977, Val Loss: 0.091016, LR: 9.59e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.091016
Pre-train Epoch [27/4000], Train Loss: 0.094199, Val Loss: 0.091551, LR: 9.56e-04
Pre-train Epoch [28/4000], Train Loss: 0.092909, Val Loss: 0.087559, LR: 9.52e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.087559
Pre-train Epoch [29/4000], Train Loss: 0.091488, Val Loss: 0.091483, LR: 9.49e-04
Pre-train Epoch [30/4000], Train Loss: 0.092010, Val Loss: 0.092214, LR: 9.46e-04
Pre-train Epoch [31/4000], Train Loss: 0.090527, Val Loss: 0.092874, LR: 9.42e-04
Pre-train Epoch [32/4000], Train Loss: 0.090177, Val Loss: 0.093981, LR: 9.38e-04
Pre-train Epoch [33/4000], Train Loss: 0.087775, Val Loss: 0.084863, LR: 9.34e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.084863
Pre-train Epoch [34/4000], Train Loss: 0.086792, Val Loss: 0.088545, LR: 9.30e-04
Pre-train Epoch [35/4000], Train Loss: 0.086014, Val Loss: 0.086392, LR: 9.26e-04
Pre-train Epoch [36/4000], Train Loss: 0.083726, Val Loss: 0.085792, LR: 9.22e-04
Pre-train Epoch [37/4000], Train Loss: 0.085083, Val Loss: 0.085422, LR: 9.18e-04
Pre-train Epoch [38/4000], Train Loss: 0.082863, Val Loss: 0.086513, LR: 9.14e-04
Pre-train Epoch [39/4000], Train Loss: 0.081919, Val Loss: 0.087806, LR: 9.09e-04
Pre-train Epoch [40/4000], Train Loss: 0.080693, Val Loss: 0.084788, LR: 9.05e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.084788
Pre-train Epoch [41/4000], Train Loss: 0.080472, Val Loss: 0.083683, LR: 9.00e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.083683
Pre-train Epoch [42/4000], Train Loss: 0.078747, Val Loss: 0.084048, LR: 8.95e-04
Pre-train Epoch [43/4000], Train Loss: 0.081183, Val Loss: 0.090874, LR: 8.90e-04
Pre-train Epoch [44/4000], Train Loss: 0.078803, Val Loss: 0.084115, LR: 8.85e-04
Pre-train Epoch [45/4000], Train Loss: 0.077726, Val Loss: 0.081620, LR: 8.80e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.081620
Pre-train Epoch [46/4000], Train Loss: 0.078953, Val Loss: 0.086596, LR: 8.75e-04
Pre-train Epoch [47/4000], Train Loss: 0.077336, Val Loss: 0.082754, LR: 8.70e-04
Pre-train Epoch [48/4000], Train Loss: 0.076009, Val Loss: 0.085080, LR: 8.65e-04
Pre-train Epoch [49/4000], Train Loss: 0.076763, Val Loss: 0.085421, LR: 8.59e-04
Pre-train Epoch [50/4000], Train Loss: 0.074047, Val Loss: 0.084428, LR: 8.54e-04
Pre-train Epoch [51/4000], Train Loss: 0.074454, Val Loss: 0.085945, LR: 8.48e-04
Pre-train Epoch [52/4000], Train Loss: 0.073510, Val Loss: 0.083651, LR: 8.42e-04
Pre-train Epoch [53/4000], Train Loss: 0.074917, Val Loss: 0.082586, LR: 8.37e-04
Pre-train Epoch [54/4000], Train Loss: 0.071254, Val Loss: 0.083776, LR: 8.31e-04
Pre-train Epoch [55/4000], Train Loss: 0.072550, Val Loss: 0.083586, LR: 8.25e-04
Pre-train Epoch [56/4000], Train Loss: 0.071915, Val Loss: 0.082292, LR: 8.19e-04
Pre-train Epoch [57/4000], Train Loss: 0.070554, Val Loss: 0.079503, LR: 8.13e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.079503
Pre-train Epoch [58/4000], Train Loss: 0.067996, Val Loss: 0.084741, LR: 8.07e-04
Pre-train Epoch [59/4000], Train Loss: 0.069211, Val Loss: 0.083818, LR: 8.00e-04
Pre-train Epoch [60/4000], Train Loss: 0.068703, Val Loss: 0.082147, LR: 7.94e-04
Pre-train Epoch [61/4000], Train Loss: 0.069465, Val Loss: 0.079281, LR: 7.88e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.079281
Pre-train Epoch [62/4000], Train Loss: 0.068798, Val Loss: 0.085991, LR: 7.81e-04
Pre-train Epoch [63/4000], Train Loss: 0.067085, Val Loss: 0.081517, LR: 7.75e-04
Pre-train Epoch [64/4000], Train Loss: 0.067744, Val Loss: 0.081152, LR: 7.68e-04
Pre-train Epoch [65/4000], Train Loss: 0.066347, Val Loss: 0.078956, LR: 7.61e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.078956
Pre-train Epoch [66/4000], Train Loss: 0.067036, Val Loss: 0.079916, LR: 7.55e-04
Pre-train Epoch [67/4000], Train Loss: 0.064986, Val Loss: 0.082440, LR: 7.48e-04
Pre-train Epoch [68/4000], Train Loss: 0.065749, Val Loss: 0.077654, LR: 7.41e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.077654
Pre-train Epoch [69/4000], Train Loss: 0.063947, Val Loss: 0.084239, LR: 7.34e-04
Pre-train Epoch [70/4000], Train Loss: 0.064378, Val Loss: 0.083375, LR: 7.27e-04
Pre-train Epoch [71/4000], Train Loss: 0.061326, Val Loss: 0.078267, LR: 7.20e-04
Pre-train Epoch [72/4000], Train Loss: 0.063857, Val Loss: 0.081170, LR: 7.13e-04
Pre-train Epoch [73/4000], Train Loss: 0.061019, Val Loss: 0.077395, LR: 7.06e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.077395
Pre-train Epoch [74/4000], Train Loss: 0.062884, Val Loss: 0.082029, LR: 6.99e-04
Pre-train Epoch [75/4000], Train Loss: 0.062043, Val Loss: 0.082219, LR: 6.92e-04
Pre-train Epoch [76/4000], Train Loss: 0.061627, Val Loss: 0.080590, LR: 6.84e-04
Pre-train Epoch [77/4000], Train Loss: 0.058528, Val Loss: 0.080376, LR: 6.77e-04
Pre-train Epoch [78/4000], Train Loss: 0.060250, Val Loss: 0.076391, LR: 6.70e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.076391
Pre-train Epoch [79/4000], Train Loss: 0.060376, Val Loss: 0.078318, LR: 6.62e-04
Pre-train Epoch [80/4000], Train Loss: 0.058988, Val Loss: 0.076793, LR: 6.55e-04
Pre-train Epoch [81/4000], Train Loss: 0.059186, Val Loss: 0.081188, LR: 6.47e-04
Pre-train Epoch [82/4000], Train Loss: 0.059104, Val Loss: 0.078059, LR: 6.40e-04
Pre-train Epoch [83/4000], Train Loss: 0.057808, Val Loss: 0.082020, LR: 6.32e-04
Pre-train Epoch [84/4000], Train Loss: 0.059727, Val Loss: 0.079541, LR: 6.25e-04
Pre-train Epoch [85/4000], Train Loss: 0.056639, Val Loss: 0.079770, LR: 6.17e-04
Pre-train Epoch [86/4000], Train Loss: 0.057590, Val Loss: 0.080750, LR: 6.09e-04
Pre-train Epoch [87/4000], Train Loss: 0.055887, Val Loss: 0.078251, LR: 6.02e-04
Pre-train Epoch [88/4000], Train Loss: 0.055726, Val Loss: 0.077180, LR: 5.94e-04
Pre-train Epoch [89/4000], Train Loss: 0.055883, Val Loss: 0.076747, LR: 5.86e-04
Pre-train Epoch [90/4000], Train Loss: 0.055596, Val Loss: 0.077459, LR: 5.79e-04
Pre-train Epoch [91/4000], Train Loss: 0.056231, Val Loss: 0.079352, LR: 5.71e-04
Pre-train Epoch [92/4000], Train Loss: 0.055305, Val Loss: 0.079225, LR: 5.63e-04
Pre-train Epoch [93/4000], Train Loss: 0.051842, Val Loss: 0.079409, LR: 5.55e-04
Pre-train Epoch [94/4000], Train Loss: 0.054711, Val Loss: 0.077143, LR: 5.48e-04
Pre-train Epoch [95/4000], Train Loss: 0.054268, Val Loss: 0.075798, LR: 5.40e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.075798
Pre-train Epoch [96/4000], Train Loss: 0.054262, Val Loss: 0.079723, LR: 5.32e-04
Pre-train Epoch [97/4000], Train Loss: 0.053418, Val Loss: 0.077701, LR: 5.24e-04
Pre-train Epoch [98/4000], Train Loss: 0.054024, Val Loss: 0.077569, LR: 5.16e-04
Pre-train Epoch [99/4000], Train Loss: 0.051460, Val Loss: 0.076178, LR: 5.08e-04
Pre-train Epoch [100/4000], Train Loss: 0.053183, Val Loss: 0.075075, LR: 5.01e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.075075
Pre-train Epoch [101/4000], Train Loss: 0.053847, Val Loss: 0.078422, LR: 4.93e-04
Pre-train Epoch [102/4000], Train Loss: 0.052206, Val Loss: 0.077327, LR: 4.85e-04
Pre-train Epoch [103/4000], Train Loss: 0.050920, Val Loss: 0.076656, LR: 4.77e-04
Pre-train Epoch [104/4000], Train Loss: 0.052035, Val Loss: 0.078758, LR: 4.69e-04
Pre-train Epoch [105/4000], Train Loss: 0.053499, Val Loss: 0.075464, LR: 4.61e-04
Pre-train Epoch [106/4000], Train Loss: 0.053348, Val Loss: 0.077352, LR: 4.53e-04
Pre-train Epoch [107/4000], Train Loss: 0.051461, Val Loss: 0.076516, LR: 4.46e-04
Pre-train Epoch [108/4000], Train Loss: 0.050894, Val Loss: 0.077709, LR: 4.38e-04
Pre-train Epoch [109/4000], Train Loss: 0.050059, Val Loss: 0.076386, LR: 4.30e-04
Pre-train Epoch [110/4000], Train Loss: 0.049896, Val Loss: 0.077066, LR: 4.22e-04
Pre-train Epoch [111/4000], Train Loss: 0.050339, Val Loss: 0.077439, LR: 4.15e-04
Pre-train Epoch [112/4000], Train Loss: 0.050054, Val Loss: 0.076520, LR: 4.07e-04
Pre-train Epoch [113/4000], Train Loss: 0.050358, Val Loss: 0.078274, LR: 3.99e-04
Pre-train Epoch [114/4000], Train Loss: 0.049129, Val Loss: 0.077444, LR: 3.92e-04
Pre-train Epoch [115/4000], Train Loss: 0.048979, Val Loss: 0.075419, LR: 3.84e-04
Pre-train Epoch [116/4000], Train Loss: 0.050037, Val Loss: 0.076917, LR: 3.76e-04
Pre-train Epoch [117/4000], Train Loss: 0.049103, Val Loss: 0.079016, LR: 3.69e-04
Pre-train Epoch [118/4000], Train Loss: 0.049759, Val Loss: 0.076141, LR: 3.61e-04
Pre-train Epoch [119/4000], Train Loss: 0.048929, Val Loss: 0.077495, LR: 3.54e-04
Pre-train Epoch [120/4000], Train Loss: 0.047586, Val Loss: 0.075568, LR: 3.46e-04
Pre-train Epoch [121/4000], Train Loss: 0.047420, Val Loss: 0.076973, LR: 3.39e-04
Pre-train Epoch [122/4000], Train Loss: 0.048385, Val Loss: 0.077628, LR: 3.31e-04
Pre-train Epoch [123/4000], Train Loss: 0.046667, Val Loss: 0.078421, LR: 3.24e-04
Pre-train Epoch [124/4000], Train Loss: 0.047668, Val Loss: 0.075911, LR: 3.17e-04
Pre-train Epoch [125/4000], Train Loss: 0.046714, Val Loss: 0.074952, LR: 3.09e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.074952
Pre-train Epoch [126/4000], Train Loss: 0.046826, Val Loss: 0.075304, LR: 3.02e-04
Pre-train Epoch [127/4000], Train Loss: 0.046607, Val Loss: 0.077028, LR: 2.95e-04
Pre-train Epoch [128/4000], Train Loss: 0.046868, Val Loss: 0.080435, LR: 2.88e-04
Pre-train Epoch [129/4000], Train Loss: 0.047385, Val Loss: 0.076927, LR: 2.81e-04
Pre-train Epoch [130/4000], Train Loss: 0.047002, Val Loss: 0.078657, LR: 2.74e-04
Pre-train Epoch [131/4000], Train Loss: 0.046794, Val Loss: 0.076642, LR: 2.67e-04
Pre-train Epoch [132/4000], Train Loss: 0.046335, Val Loss: 0.077194, LR: 2.60e-04
Pre-train Epoch [133/4000], Train Loss: 0.045169, Val Loss: 0.077630, LR: 2.53e-04
Pre-train Epoch [134/4000], Train Loss: 0.044792, Val Loss: 0.076330, LR: 2.46e-04
Pre-train Epoch [135/4000], Train Loss: 0.044889, Val Loss: 0.076202, LR: 2.40e-04
Pre-train Epoch [136/4000], Train Loss: 0.044864, Val Loss: 0.076268, LR: 2.33e-04
Pre-train Epoch [137/4000], Train Loss: 0.044629, Val Loss: 0.077017, LR: 2.26e-04
Pre-train Epoch [138/4000], Train Loss: 0.044068, Val Loss: 0.077492, LR: 2.20e-04
Pre-train Epoch [139/4000], Train Loss: 0.045236, Val Loss: 0.076122, LR: 2.13e-04
Pre-train Epoch [140/4000], Train Loss: 0.042324, Val Loss: 0.077146, LR: 2.07e-04
Pre-train Epoch [141/4000], Train Loss: 0.044516, Val Loss: 0.077524, LR: 2.01e-04
Pre-train Epoch [142/4000], Train Loss: 0.042830, Val Loss: 0.076978, LR: 1.94e-04
Pre-train Epoch [143/4000], Train Loss: 0.044102, Val Loss: 0.075642, LR: 1.88e-04
Pre-train Epoch [144/4000], Train Loss: 0.044098, Val Loss: 0.075726, LR: 1.82e-04
Pre-train Epoch [145/4000], Train Loss: 0.042863, Val Loss: 0.075816, LR: 1.76e-04
Pre-train Epoch [146/4000], Train Loss: 0.043847, Val Loss: 0.075746, LR: 1.70e-04
Pre-train Epoch [147/4000], Train Loss: 0.042687, Val Loss: 0.074840, LR: 1.64e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.074840
Pre-train Epoch [148/4000], Train Loss: 0.042200, Val Loss: 0.073771, LR: 1.59e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.073771
Pre-train Epoch [149/4000], Train Loss: 0.042513, Val Loss: 0.074076, LR: 1.53e-04
Pre-train Epoch [150/4000], Train Loss: 0.042066, Val Loss: 0.074693, LR: 1.47e-04
Pre-train Epoch [151/4000], Train Loss: 0.041822, Val Loss: 0.074210, LR: 1.42e-04
Pre-train Epoch [152/4000], Train Loss: 0.043894, Val Loss: 0.074500, LR: 1.36e-04
Pre-train Epoch [153/4000], Train Loss: 0.040953, Val Loss: 0.074649, LR: 1.31e-04
Pre-train Epoch [154/4000], Train Loss: 0.041930, Val Loss: 0.074078, LR: 1.26e-04
Pre-train Epoch [155/4000], Train Loss: 0.041797, Val Loss: 0.074480, LR: 1.21e-04
Pre-train Epoch [156/4000], Train Loss: 0.042104, Val Loss: 0.074988, LR: 1.16e-04
Pre-train Epoch [157/4000], Train Loss: 0.041708, Val Loss: 0.073677, LR: 1.11e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.073677
Pre-train Epoch [158/4000], Train Loss: 0.042301, Val Loss: 0.074296, LR: 1.06e-04
Pre-train Epoch [159/4000], Train Loss: 0.041251, Val Loss: 0.074602, LR: 1.01e-04
Pre-train Epoch [160/4000], Train Loss: 0.040688, Val Loss: 0.074056, LR: 9.64e-05
Pre-train Epoch [161/4000], Train Loss: 0.041902, Val Loss: 0.074238, LR: 9.18e-05
Pre-train Epoch [162/4000], Train Loss: 0.042513, Val Loss: 0.075010, LR: 8.74e-05
Pre-train Epoch [163/4000], Train Loss: 0.041343, Val Loss: 0.075570, LR: 8.30e-05
Pre-train Epoch [164/4000], Train Loss: 0.040051, Val Loss: 0.075215, LR: 7.88e-05
Pre-train Epoch [165/4000], Train Loss: 0.042252, Val Loss: 0.075335, LR: 7.46e-05
Pre-train Epoch [166/4000], Train Loss: 0.040475, Val Loss: 0.074980, LR: 7.06e-05
Pre-train Epoch [167/4000], Train Loss: 0.040835, Val Loss: 0.074703, LR: 6.66e-05
Pre-train Epoch [168/4000], Train Loss: 0.040853, Val Loss: 0.074719, LR: 6.28e-05
Pre-train Epoch [169/4000], Train Loss: 0.041731, Val Loss: 0.075057, LR: 5.91e-05
Pre-train Epoch [170/4000], Train Loss: 0.040748, Val Loss: 0.074397, LR: 5.54e-05
Pre-train Epoch [171/4000], Train Loss: 0.040097, Val Loss: 0.075103, LR: 5.19e-05
Pre-train Epoch [172/4000], Train Loss: 0.040995, Val Loss: 0.074424, LR: 4.85e-05
Pre-train Epoch [173/4000], Train Loss: 0.040272, Val Loss: 0.074227, LR: 4.53e-05
Pre-train Epoch [174/4000], Train Loss: 0.039337, Val Loss: 0.073926, LR: 4.21e-05
Pre-train Epoch [175/4000], Train Loss: 0.041036, Val Loss: 0.074399, LR: 3.90e-05
Pre-train Epoch [176/4000], Train Loss: 0.039769, Val Loss: 0.073967, LR: 3.61e-05
Pre-train Epoch [177/4000], Train Loss: 0.039739, Val Loss: 0.074309, LR: 3.32e-05
Pre-train Epoch [178/4000], Train Loss: 0.039487, Val Loss: 0.073932, LR: 3.05e-05
Pre-train Epoch [179/4000], Train Loss: 0.039967, Val Loss: 0.074125, LR: 2.79e-05
Pre-train Epoch [180/4000], Train Loss: 0.041341, Val Loss: 0.074260, LR: 2.54e-05
Pre-train Epoch [181/4000], Train Loss: 0.040363, Val Loss: 0.073884, LR: 2.31e-05
Pre-train Epoch [182/4000], Train Loss: 0.041034, Val Loss: 0.074169, LR: 2.08e-05
Pre-train Epoch [183/4000], Train Loss: 0.040085, Val Loss: 0.074219, LR: 1.87e-05
Pre-train Epoch [184/4000], Train Loss: 0.038928, Val Loss: 0.073936, LR: 1.67e-05
Pre-train Epoch [185/4000], Train Loss: 0.039127, Val Loss: 0.073956, LR: 1.48e-05
Pre-train Epoch [186/4000], Train Loss: 0.040162, Val Loss: 0.073920, LR: 1.30e-05
Pre-train Epoch [187/4000], Train Loss: 0.040549, Val Loss: 0.074153, LR: 1.14e-05
Pre-train Epoch [188/4000], Train Loss: 0.041069, Val Loss: 0.074006, LR: 9.85e-06
Pre-train Epoch [189/4000], Train Loss: 0.039656, Val Loss: 0.074307, LR: 8.44e-06
Pre-train Epoch [190/4000], Train Loss: 0.039013, Val Loss: 0.074276, LR: 7.15e-06
Pre-train Epoch [191/4000], Train Loss: 0.040184, Val Loss: 0.074243, LR: 5.98e-06
Pre-train Epoch [192/4000], Train Loss: 0.040698, Val Loss: 0.074142, LR: 4.94e-06
Pre-train Epoch [193/4000], Train Loss: 0.040816, Val Loss: 0.074113, LR: 4.02e-06
Pre-train Epoch [194/4000], Train Loss: 0.039701, Val Loss: 0.074069, LR: 3.22e-06
Pre-train Epoch [195/4000], Train Loss: 0.040717, Val Loss: 0.074082, LR: 2.54e-06
Pre-train Epoch [196/4000], Train Loss: 0.039403, Val Loss: 0.074128, LR: 1.99e-06
Pre-train Epoch [197/4000], Train Loss: 0.039462, Val Loss: 0.074128, LR: 1.55e-06
Pre-train Epoch [198/4000], Train Loss: 0.040491, Val Loss: 0.074104, LR: 1.25e-06
Pre-train Epoch [199/4000], Train Loss: 0.041084, Val Loss: 0.074105, LR: 1.06e-06
Pre-train Epoch [200/4000], Train Loss: 0.039782, Val Loss: 0.074106, LR: 1.00e-03
--- Epoch 200 是一个重启点。重置 AdamW 优化器状态！ ---
Pre-train Epoch [201/4000], Train Loss: 0.050303, Val Loss: 0.081939, LR: 1.00e-03
Pre-train Epoch [202/4000], Train Loss: 0.051883, Val Loss: 0.076741, LR: 1.00e-03
Pre-train Epoch [203/4000], Train Loss: 0.051298, Val Loss: 0.078019, LR: 9.99e-04
Pre-train Epoch [204/4000], Train Loss: 0.054410, Val Loss: 0.087418, LR: 9.99e-04
Pre-train Epoch [205/4000], Train Loss: 0.050675, Val Loss: 0.079604, LR: 9.98e-04
Pre-train Epoch [206/4000], Train Loss: 0.051247, Val Loss: 0.081346, LR: 9.98e-04
Pre-train Epoch [207/4000], Train Loss: 0.051923, Val Loss: 0.080628, LR: 9.97e-04
Pre-train Epoch [208/4000], Train Loss: 0.052564, Val Loss: 0.077511, LR: 9.96e-04
Pre-train Epoch [209/4000], Train Loss: 0.051971, Val Loss: 0.079489, LR: 9.95e-04
Pre-train Epoch [210/4000], Train Loss: 0.050708, Val Loss: 0.081860, LR: 9.94e-04
Pre-train Epoch [211/4000], Train Loss: 0.052002, Val Loss: 0.077991, LR: 9.93e-04
Pre-train Epoch [212/4000], Train Loss: 0.052583, Val Loss: 0.077594, LR: 9.91e-04
Pre-train Epoch [213/4000], Train Loss: 0.054797, Val Loss: 0.079460, LR: 9.90e-04
Pre-train Epoch [214/4000], Train Loss: 0.047463, Val Loss: 0.077387, LR: 9.88e-04
Pre-train Epoch [215/4000], Train Loss: 0.050277, Val Loss: 0.077029, LR: 9.86e-04
Pre-train Epoch [216/4000], Train Loss: 0.050746, Val Loss: 0.079842, LR: 9.84e-04
Pre-train Epoch [217/4000], Train Loss: 0.049051, Val Loss: 0.078956, LR: 9.82e-04
Pre-train Epoch [218/4000], Train Loss: 0.049269, Val Loss: 0.076738, LR: 9.80e-04
Pre-train Epoch [219/4000], Train Loss: 0.050973, Val Loss: 0.080186, LR: 9.78e-04
Pre-train Epoch [220/4000], Train Loss: 0.048797, Val Loss: 0.077758, LR: 9.76e-04
Pre-train Epoch [221/4000], Train Loss: 0.049045, Val Loss: 0.078671, LR: 9.73e-04
Pre-train Epoch [222/4000], Train Loss: 0.048822, Val Loss: 0.074685, LR: 9.70e-04
Pre-train Epoch [223/4000], Train Loss: 0.049608, Val Loss: 0.082695, LR: 9.68e-04
Pre-train Epoch [224/4000], Train Loss: 0.048826, Val Loss: 0.078805, LR: 9.65e-04
Pre-train Epoch [225/4000], Train Loss: 0.047531, Val Loss: 0.077659, LR: 9.62e-04
Pre-train Epoch [226/4000], Train Loss: 0.048786, Val Loss: 0.074890, LR: 9.59e-04
Pre-train Epoch [227/4000], Train Loss: 0.048170, Val Loss: 0.076850, LR: 9.56e-04
Pre-train Epoch [228/4000], Train Loss: 0.046558, Val Loss: 0.075586, LR: 9.52e-04
Pre-train Epoch [229/4000], Train Loss: 0.046725, Val Loss: 0.073915, LR: 9.49e-04
Pre-train Epoch [230/4000], Train Loss: 0.046672, Val Loss: 0.072555, LR: 9.46e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.072555
Pre-train Epoch [231/4000], Train Loss: 0.046064, Val Loss: 0.075501, LR: 9.42e-04
Pre-train Epoch [232/4000], Train Loss: 0.046562, Val Loss: 0.075492, LR: 9.38e-04
Pre-train Epoch [233/4000], Train Loss: 0.046276, Val Loss: 0.073818, LR: 9.34e-04
Pre-train Epoch [234/4000], Train Loss: 0.043800, Val Loss: 0.075071, LR: 9.30e-04
Pre-train Epoch [235/4000], Train Loss: 0.045165, Val Loss: 0.074419, LR: 9.26e-04
Pre-train Epoch [236/4000], Train Loss: 0.044542, Val Loss: 0.077234, LR: 9.22e-04
Pre-train Epoch [237/4000], Train Loss: 0.046699, Val Loss: 0.077075, LR: 9.18e-04
Pre-train Epoch [238/4000], Train Loss: 0.044571, Val Loss: 0.077083, LR: 9.14e-04
Pre-train Epoch [239/4000], Train Loss: 0.045376, Val Loss: 0.073408, LR: 9.09e-04
Pre-train Epoch [240/4000], Train Loss: 0.047320, Val Loss: 0.078756, LR: 9.05e-04
Pre-train Epoch [241/4000], Train Loss: 0.044320, Val Loss: 0.075808, LR: 9.00e-04
Pre-train Epoch [242/4000], Train Loss: 0.044438, Val Loss: 0.074800, LR: 8.95e-04
Pre-train Epoch [243/4000], Train Loss: 0.042914, Val Loss: 0.076455, LR: 8.90e-04
Pre-train Epoch [244/4000], Train Loss: 0.044175, Val Loss: 0.072948, LR: 8.85e-04
Pre-train Epoch [245/4000], Train Loss: 0.045952, Val Loss: 0.074364, LR: 8.80e-04
Pre-train Epoch [246/4000], Train Loss: 0.044024, Val Loss: 0.076418, LR: 8.75e-04
Pre-train Epoch [247/4000], Train Loss: 0.043405, Val Loss: 0.067672, LR: 8.70e-04
- (内存)预训练模型已更新，验证HuberLoss提升至: 0.067672
Pre-train Epoch [248/4000], Train Loss: 0.044349, Val Loss: 0.073693, LR: 8.65e-04
Pre-train Epoch [249/4000], Train Loss: 0.042578, Val Loss: 0.072834, LR: 8.59e-04
Pre-train Epoch [250/4000], Train Loss: 0.042604, Val Loss: 0.079848, LR: 8.54e-04
Pre-train Epoch [251/4000], Train Loss: 0.041292, Val Loss: 0.074198, LR: 8.48e-04
Pre-train Epoch [252/4000], Train Loss: 0.042404, Val Loss: 0.075169, LR: 8.42e-04
Pre-train Epoch [253/4000], Train Loss: 0.043183, Val Loss: 0.076539, LR: 8.37e-04
Pre-train Epoch [254/4000], Train Loss: 0.043840, Val Loss: 0.076450, LR: 8.31e-04
Pre-train Epoch [255/4000], Train Loss: 0.045096, Val Loss: 0.076451, LR: 8.25e-04
Pre-train Epoch [256/4000], Train Loss: 0.042164, Val Loss: 0.078109, LR: 8.19e-04
Pre-train Epoch [257/4000], Train Loss: 0.042158, Val Loss: 0.075154, LR: 8.13e-04
Pre-train Epoch [258/4000], Train Loss: 0.043240, Val Loss: 0.073030, LR: 8.07e-04
Pre-train Epoch [259/4000], Train Loss: 0.041143, Val Loss: 0.074285, LR: 8.00e-04
Pre-train Epoch [260/4000], Train Loss: 0.043230, Val Loss: 0.076085, LR: 7.94e-04
Pre-train Epoch [261/4000], Train Loss: 0.041575, Val Loss: 0.076736, LR: 7.88e-04
Pre-train Epoch [262/4000], Train Loss: 0.040331, Val Loss: 0.074275, LR: 7.81e-04
Pre-train Epoch [263/4000], Train Loss: 0.040101, Val Loss: 0.077754, LR: 7.75e-04
Pre-train Epoch [264/4000], Train Loss: 0.040029, Val Loss: 0.074626, LR: 7.68e-04
Pre-train Epoch [265/4000], Train Loss: 0.039975, Val Loss: 0.080634, LR: 7.61e-04
Pre-train Epoch [266/4000], Train Loss: 0.040975, Val Loss: 0.073007, LR: 7.55e-04
Pre-train Epoch [267/4000], Train Loss: 0.040131, Val Loss: 0.074832, LR: 7.48e-04
Pre-train Epoch [268/4000], Train Loss: 0.040403, Val Loss: 0.075071, LR: 7.41e-04
Pre-train Epoch [269/4000], Train Loss: 0.039590, Val Loss: 0.073478, LR: 7.34e-04
Pre-train Epoch [270/4000], Train Loss: 0.039191, Val Loss: 0.073120, LR: 7.27e-04
Pre-train Epoch [271/4000], Train Loss: 0.039368, Val Loss: 0.072347, LR: 7.20e-04
Pre-train Epoch [272/4000], Train Loss: 0.038480, Val Loss: 0.073792, LR: 7.13e-04
Pre-train Epoch [273/4000], Train Loss: 0.040037, Val Loss: 0.075343, LR: 7.06e-04
Pre-train Epoch [274/4000], Train Loss: 0.041646, Val Loss: 0.077577, LR: 6.99e-04
Pre-train Epoch [275/4000], Train Loss: 0.039367, Val Loss: 0.074577, LR: 6.92e-04
Pre-train Epoch [276/4000], Train Loss: 0.039104, Val Loss: 0.075518, LR: 6.84e-04
Pre-train Epoch [277/4000], Train Loss: 0.037507, Val Loss: 0.074835, LR: 6.77e-04
Pre-train Epoch [278/4000], Train Loss: 0.038087, Val Loss: 0.072110, LR: 6.70e-04
Pre-train Epoch [279/4000], Train Loss: 0.038383, Val Loss: 0.073034, LR: 6.62e-04
Pre-train Epoch [280/4000], Train Loss: 0.038474, Val Loss: 0.077653, LR: 6.55e-04
Pre-train Epoch [281/4000], Train Loss: 0.037799, Val Loss: 0.074260, LR: 6.47e-04
Pre-train Epoch [282/4000], Train Loss: 0.036179, Val Loss: 0.075493, LR: 6.40e-04
Pre-train Epoch [283/4000], Train Loss: 0.037978, Val Loss: 0.074349, LR: 6.32e-04
Pre-train Epoch [284/4000], Train Loss: 0.035813, Val Loss: 0.072019, LR: 6.25e-04
Pre-train Epoch [285/4000], Train Loss: 0.037271, Val Loss: 0.075790, LR: 6.17e-04
Pre-train Epoch [286/4000], Train Loss: 0.038143, Val Loss: 0.074569, LR: 6.09e-04
Pre-train Epoch [287/4000], Train Loss: 0.036331, Val Loss: 0.076001, LR: 6.02e-04
Pre-train Epoch [288/4000], Train Loss: 0.036650, Val Loss: 0.075722, LR: 5.94e-04
Pre-train Epoch [289/4000], Train Loss: 0.037800, Val Loss: 0.073749, LR: 5.86e-04
Pre-train Epoch [290/4000], Train Loss: 0.036334, Val Loss: 0.073426, LR: 5.79e-04
Pre-train Epoch [291/4000], Train Loss: 0.035195, Val Loss: 0.072793, LR: 5.71e-04
Pre-train Epoch [292/4000], Train Loss: 0.035059, Val Loss: 0.073555, LR: 5.63e-04
Pre-train Epoch [293/4000], Train Loss: 0.036151, Val Loss: 0.071097, LR: 5.55e-04
Pre-train Epoch [294/4000], Train Loss: 0.035284, Val Loss: 0.071477, LR: 5.48e-04
Pre-train Epoch [295/4000], Train Loss: 0.034826, Val Loss: 0.072639, LR: 5.40e-04
Pre-train Epoch [296/4000], Train Loss: 0.035812, Val Loss: 0.072683, LR: 5.32e-04
Pre-train Epoch [297/4000], Train Loss: 0.036674, Val Loss: 0.071752, LR: 5.24e-04
Pre-train Epoch [298/4000], Train Loss: 0.036335, Val Loss: 0.074522, LR: 5.16e-04
Pre-train Epoch [299/4000], Train Loss: 0.033859, Val Loss: 0.072755, LR: 5.08e-04
Pre-train Epoch [300/4000], Train Loss: 0.035686, Val Loss: 0.072263, LR: 5.01e-04
Pre-train Epoch [301/4000], Train Loss: 0.034610, Val Loss: 0.072103, LR: 4.93e-04
Pre-train Epoch [302/4000], Train Loss: 0.035333, Val Loss: 0.075962, LR: 4.85e-04
Pre-train Epoch [303/4000], Train Loss: 0.034893, Val Loss: 0.073455, LR: 4.77e-04
Pre-train Epoch [304/4000], Train Loss: 0.033617, Val Loss: 0.075273, LR: 4.69e-04
Pre-train Epoch [305/4000], Train Loss: 0.034223, Val Loss: 0.074045, LR: 4.61e-04
Pre-train Epoch [306/4000], Train Loss: 0.034775, Val Loss: 0.072899, LR: 4.53e-04
Pre-train Epoch [307/4000], Train Loss: 0.033009, Val Loss: 0.072919, LR: 4.46e-04
Pre-train Epoch [308/4000], Train Loss: 0.033104, Val Loss: 0.073325, LR: 4.38e-04
Pre-train Epoch [309/4000], Train Loss: 0.033535, Val Loss: 0.074759, LR: 4.30e-04
Pre-train Epoch [310/4000], Train Loss: 0.032188, Val Loss: 0.073779, LR: 4.22e-04
Pre-train Epoch [311/4000], Train Loss: 0.032091, Val Loss: 0.074914, LR: 4.15e-04
Pre-train Epoch [312/4000], Train Loss: 0.032231, Val Loss: 0.073334, LR: 4.07e-04
Pre-train Epoch [313/4000], Train Loss: 0.033667, Val Loss: 0.073209, LR: 3.99e-04
Pre-train Epoch [314/4000], Train Loss: 0.032254, Val Loss: 0.072136, LR: 3.92e-04
Pre-train Epoch [315/4000], Train Loss: 0.031739, Val Loss: 0.073069, LR: 3.84e-04
Pre-train Epoch [316/4000], Train Loss: 0.031145, Val Loss: 0.070721, LR: 3.76e-04
Pre-train Epoch [317/4000], Train Loss: 0.032266, Val Loss: 0.070496, LR: 3.69e-04
Pre-train Epoch [318/4000], Train Loss: 0.031676, Val Loss: 0.070681, LR: 3.61e-04
Pre-train Epoch [319/4000], Train Loss: 0.032897, Val Loss: 0.072175, LR: 3.54e-04
Pre-train Epoch [320/4000], Train Loss: 0.031966, Val Loss: 0.070837, LR: 3.46e-04
Pre-train Epoch [321/4000], Train Loss: 0.031450, Val Loss: 0.071013, LR: 3.39e-04
Pre-train Epoch [322/4000], Train Loss: 0.032077, Val Loss: 0.070238, LR: 3.31e-04
Pre-train Epoch [323/4000], Train Loss: 0.031814, Val Loss: 0.070895, LR: 3.24e-04
Pre-train Epoch [324/4000], Train Loss: 0.030098, Val Loss: 0.068882, LR: 3.17e-04
Pre-train Epoch [325/4000], Train Loss: 0.030843, Val Loss: 0.071300, LR: 3.09e-04
Pre-train Epoch [326/4000], Train Loss: 0.031145, Val Loss: 0.072059, LR: 3.02e-04
Pre-train Epoch [327/4000], Train Loss: 0.031027, Val Loss: 0.070583, LR: 2.95e-04
Pre-train Epoch [328/4000], Train Loss: 0.031044, Val Loss: 0.070125, LR: 2.88e-04
Pre-train Epoch [329/4000], Train Loss: 0.031665, Val Loss: 0.070917, LR: 2.81e-04
Pre-train Epoch [330/4000], Train Loss: 0.030001, Val Loss: 0.070992, LR: 2.74e-04
Pre-train Epoch [331/4000], Train Loss: 0.030510, Val Loss: 0.072138, LR: 2.67e-04
Pre-train Epoch [332/4000], Train Loss: 0.030656, Val Loss: 0.070578, LR: 2.60e-04
Pre-train Epoch [333/4000], Train Loss: 0.030517, Val Loss: 0.070808, LR: 2.53e-04
Pre-train Epoch [334/4000], Train Loss: 0.030663, Val Loss: 0.070553, LR: 2.46e-04
Pre-train Epoch [335/4000], Train Loss: 0.030395, Val Loss: 0.071459, LR: 2.40e-04
Pre-train Epoch [336/4000], Train Loss: 0.031486, Val Loss: 0.070656, LR: 2.33e-04
Pre-train Epoch [337/4000], Train Loss: 0.031268, Val Loss: 0.071170, LR: 2.26e-04
Pre-train Epoch [338/4000], Train Loss: 0.029906, Val Loss: 0.071294, LR: 2.20e-04
Pre-train Epoch [339/4000], Train Loss: 0.030143, Val Loss: 0.072508, LR: 2.13e-04
Pre-train Epoch [340/4000], Train Loss: 0.030011, Val Loss: 0.069793, LR: 2.07e-04
Pre-train Epoch [341/4000], Train Loss: 0.030355, Val Loss: 0.070583, LR: 2.01e-04
Pre-train Epoch [342/4000], Train Loss: 0.028920, Val Loss: 0.071688, LR: 1.94e-04
Pre-train Epoch [343/4000], Train Loss: 0.029559, Val Loss: 0.072899, LR: 1.88e-04
Pre-train Epoch [344/4000], Train Loss: 0.029300, Val Loss: 0.071306, LR: 1.82e-04
Pre-train Epoch [345/4000], Train Loss: 0.030945, Val Loss: 0.071139, LR: 1.76e-04
Pre-train Epoch [346/4000], Train Loss: 0.029265, Val Loss: 0.069947, LR: 1.70e-04
Pre-train Epoch [347/4000], Train Loss: 0.028932, Val Loss: 0.071807, LR: 1.64e-04
Pre-train Epoch [348/4000], Train Loss: 0.028923, Val Loss: 0.069656, LR: 1.59e-04
Pre-train Epoch [349/4000], Train Loss: 0.028997, Val Loss: 0.069150, LR: 1.53e-04
Pre-train Epoch [350/4000], Train Loss: 0.029938, Val Loss: 0.069886, LR: 1.47e-04
Pre-train Epoch [351/4000], Train Loss: 0.028319, Val Loss: 0.069739, LR: 1.42e-04
Pre-train Epoch [352/4000], Train Loss: 0.029103, Val Loss: 0.069852, LR: 1.36e-04
Pre-train Epoch [353/4000], Train Loss: 0.029536, Val Loss: 0.070946, LR: 1.31e-04
Pre-train Epoch [354/4000], Train Loss: 0.029436, Val Loss: 0.070216, LR: 1.26e-04
Pre-train Epoch [355/4000], Train Loss: 0.030185, Val Loss: 0.069649, LR: 1.21e-04
Pre-train Epoch [356/4000], Train Loss: 0.029871, Val Loss: 0.069324, LR: 1.16e-04
Pre-train Epoch [357/4000], Train Loss: 0.027778, Val Loss: 0.069552, LR: 1.11e-04
Pre-train Epoch [358/4000], Train Loss: 0.028579, Val Loss: 0.069098, LR: 1.06e-04
Pre-train Epoch [359/4000], Train Loss: 0.029113, Val Loss: 0.068225, LR: 1.01e-04
Pre-train Epoch [360/4000], Train Loss: 0.029532, Val Loss: 0.068819, LR: 9.64e-05
Pre-train Epoch [361/4000], Train Loss: 0.029217, Val Loss: 0.070123, LR: 9.18e-05
Pre-train Epoch [362/4000], Train Loss: 0.029025, Val Loss: 0.069582, LR: 8.74e-05
Pre-train Epoch [363/4000], Train Loss: 0.027970, Val Loss: 0.069404, LR: 8.30e-05
Pre-train Epoch [364/4000], Train Loss: 0.028048, Val Loss: 0.069373, LR: 7.88e-05
Pre-train Epoch [365/4000], Train Loss: 0.027923, Val Loss: 0.069768, LR: 7.46e-05
Pre-train Epoch [366/4000], Train Loss: 0.027322, Val Loss: 0.068462, LR: 7.06e-05
Pre-train Epoch [367/4000], Train Loss: 0.028307, Val Loss: 0.068622, LR: 6.66e-05
Pre-train Epoch [368/4000], Train Loss: 0.028240, Val Loss: 0.068816, LR: 6.28e-05
Pre-train Epoch [369/4000], Train Loss: 0.027909, Val Loss: 0.068659, LR: 5.91e-05
Pre-train Epoch [370/4000], Train Loss: 0.027418, Val Loss: 0.068307, LR: 5.54e-05
Pre-train Epoch [371/4000], Train Loss: 0.027538, Val Loss: 0.068320, LR: 5.19e-05
Pre-train Epoch [372/4000], Train Loss: 0.027831, Val Loss: 0.068397, LR: 4.85e-05
Pre-train Epoch [373/4000], Train Loss: 0.027643, Val Loss: 0.068525, LR: 4.53e-05
Pre-train Epoch [374/4000], Train Loss: 0.028564, Val Loss: 0.069015, LR: 4.21e-05
Pre-train Epoch [375/4000], Train Loss: 0.028631, Val Loss: 0.069308, LR: 3.90e-05
Pre-train Epoch [376/4000], Train Loss: 0.027983, Val Loss: 0.069010, LR: 3.61e-05
Pre-train Epoch [377/4000], Train Loss: 0.027922, Val Loss: 0.069124, LR: 3.32e-05
Pre-train Epoch [378/4000], Train Loss: 0.028577, Val Loss: 0.069235, LR: 3.05e-05
Pre-train Epoch [379/4000], Train Loss: 0.027733, Val Loss: 0.069144, LR: 2.79e-05
Pre-train Epoch [380/4000], Train Loss: 0.027022, Val Loss: 0.068936, LR: 2.54e-05
Pre-train Epoch [381/4000], Train Loss: 0.028071, Val Loss: 0.068944, LR: 2.31e-05
Pre-train Epoch [382/4000], Train Loss: 0.028459, Val Loss: 0.069122, LR: 2.08e-05
Pre-train Epoch [383/4000], Train Loss: 0.027168, Val Loss: 0.069089, LR: 1.87e-05
Pre-train Epoch [384/4000], Train Loss: 0.028101, Val Loss: 0.068949, LR: 1.67e-05
Pre-train Epoch [385/4000], Train Loss: 0.028117, Val Loss: 0.069031, LR: 1.48e-05
Pre-train Epoch [386/4000], Train Loss: 0.028308, Val Loss: 0.068994, LR: 1.30e-05
Pre-train Epoch [387/4000], Train Loss: 0.027703, Val Loss: 0.068816, LR: 1.14e-05
Pre-train Epoch [388/4000], Train Loss: 0.028385, Val Loss: 0.068913, LR: 9.85e-06
Pre-train Epoch [389/4000], Train Loss: 0.027181, Val Loss: 0.068908, LR: 8.44e-06
Pre-train Epoch [390/4000], Train Loss: 0.027418, Val Loss: 0.068961, LR: 7.15e-06
Pre-train Epoch [391/4000], Train Loss: 0.027537, Val Loss: 0.068947, LR: 5.98e-06
Pre-train Epoch [392/4000], Train Loss: 0.029002, Val Loss: 0.069016, LR: 4.94e-06
Pre-train Epoch [393/4000], Train Loss: 0.027539, Val Loss: 0.069061, LR: 4.02e-06
Pre-train Epoch [394/4000], Train Loss: 0.027317, Val Loss: 0.069080, LR: 3.22e-06
Pre-train Epoch [395/4000], Train Loss: 0.027568, Val Loss: 0.069085, LR: 2.54e-06
Pre-train Epoch [396/4000], Train Loss: 0.027328, Val Loss: 0.069051, LR: 1.99e-06
Pre-train Epoch [397/4000], Train Loss: 0.027737, Val Loss: 0.069036, LR: 1.55e-06
Pre-train Epoch [398/4000], Train Loss: 0.027211, Val Loss: 0.069037, LR: 1.25e-06
Pre-train Epoch [399/4000], Train Loss: 0.026064, Val Loss: 0.069027, LR: 1.06e-06
Pre-train Epoch [400/4000], Train Loss: 0.027720, Val Loss: 0.069009, LR: 1.00e-03
--- Epoch 400 是一个重启点。重置 AdamW 优化器状态！ ---
Pre-train Epoch [401/4000], Train Loss: 0.034835, Val Loss: 0.068447, LR: 1.00e-03
Pre-train Epoch [402/4000], Train Loss: 0.037256, Val Loss: 0.071677, LR: 1.00e-03
Pre-train Epoch [403/4000], Train Loss: 0.036895, Val Loss: 0.073051, LR: 9.99e-04
Pre-train Epoch [404/4000], Train Loss: 0.039740, Val Loss: 0.071069, LR: 9.99e-04
Pre-train Epoch [405/4000], Train Loss: 0.039199, Val Loss: 0.078209, LR: 9.98e-04
Pre-train Epoch [406/4000], Train Loss: 0.039155, Val Loss: 0.073000, LR: 9.98e-04
Pre-train Epoch [407/4000], Train Loss: 0.038769, Val Loss: 0.073760, LR: 9.97e-04
Pre-train Epoch [408/4000], Train Loss: 0.037440, Val Loss: 0.073226, LR: 9.96e-04
Pre-train Epoch [409/4000], Train Loss: 0.036680, Val Loss: 0.072372, LR: 9.95e-04
Pre-train Epoch [410/4000], Train Loss: 0.038292, Val Loss: 0.074140, LR: 9.94e-04
Pre-train Epoch [411/4000], Train Loss: 0.037026, Val Loss: 0.072408, LR: 9.93e-04
Pre-train Epoch [412/4000], Train Loss: 0.038701, Val Loss: 0.074746, LR: 9.91e-04
Pre-train Epoch [413/4000], Train Loss: 0.037953, Val Loss: 0.073644, LR: 9.90e-04
Pre-train Epoch [414/4000], Train Loss: 0.038419, Val Loss: 0.074884, LR: 9.88e-04
Pre-train Epoch [415/4000], Train Loss: 0.037958, Val Loss: 0.072389, LR: 9.86e-04
Pre-train Epoch [416/4000], Train Loss: 0.038511, Val Loss: 0.074911, LR: 9.84e-04
Pre-train Epoch [417/4000], Train Loss: 0.037538, Val Loss: 0.076505, LR: 9.82e-04
Pre-train Epoch [418/4000], Train Loss: 0.038996, Val Loss: 0.075837, LR: 9.80e-04
Pre-train Epoch [419/4000], Train Loss: 0.040988, Val Loss: 0.078136, LR: 9.78e-04
Pre-train Epoch [420/4000], Train Loss: 0.037039, Val Loss: 0.074368, LR: 9.76e-04
Pre-train Epoch [421/4000], Train Loss: 0.038519, Val Loss: 0.074812, LR: 9.73e-04
Pre-train Epoch [422/4000], Train Loss: 0.036963, Val Loss: 0.074283, LR: 9.70e-04
Pre-train Epoch [423/4000], Train Loss: 0.038158, Val Loss: 0.073216, LR: 9.68e-04
Pre-train Epoch [424/4000], Train Loss: 0.037285, Val Loss: 0.074952, LR: 9.65e-04
Pre-train Epoch [425/4000], Train Loss: 0.035975, Val Loss: 0.073894, LR: 9.62e-04
Pre-train Epoch [426/4000], Train Loss: 0.036996, Val Loss: 0.077851, LR: 9.59e-04
Pre-train Epoch [427/4000], Train Loss: 0.039202, Val Loss: 0.072810, LR: 9.56e-04
Pre-train Epoch [428/4000], Train Loss: 0.037884, Val Loss: 0.073993, LR: 9.52e-04
Pre-train Epoch [429/4000], Train Loss: 0.036214, Val Loss: 0.075249, LR: 9.49e-04
Pre-train Epoch [430/4000], Train Loss: 0.037542, Val Loss: 0.073483, LR: 9.46e-04
Pre-train Epoch [431/4000], Train Loss: 0.038423, Val Loss: 0.074184, LR: 9.42e-04
Pre-train Epoch [432/4000], Train Loss: 0.037259, Val Loss: 0.076189, LR: 9.38e-04
Pre-train Epoch [433/4000], Train Loss: 0.037618, Val Loss: 0.075081, LR: 9.34e-04
Pre-train Epoch [434/4000], Train Loss: 0.037536, Val Loss: 0.070829, LR: 9.30e-04
Pre-train Epoch [435/4000], Train Loss: 0.034708, Val Loss: 0.073947, LR: 9.26e-04
Pre-train Epoch [436/4000], Train Loss: 0.035493, Val Loss: 0.073602, LR: 9.22e-04
Pre-train Epoch [437/4000], Train Loss: 0.035652, Val Loss: 0.073778, LR: 9.18e-04
Pre-train Epoch [438/4000], Train Loss: 0.036001, Val Loss: 0.072000, LR: 9.14e-04
Pre-train Epoch [439/4000], Train Loss: 0.035562, Val Loss: 0.071789, LR: 9.09e-04
Pre-train Epoch [440/4000], Train Loss: 0.036228, Val Loss: 0.073933, LR: 9.05e-04
Pre-train Epoch [441/4000], Train Loss: 0.034211, Val Loss: 0.076387, LR: 9.00e-04
Pre-train Epoch [442/4000], Train Loss: 0.036255, Val Loss: 0.074527, LR: 8.95e-04
Pre-train Epoch [443/4000], Train Loss: 0.035952, Val Loss: 0.071345, LR: 8.90e-04
Pre-train Epoch [444/4000], Train Loss: 0.035283, Val Loss: 0.072207, LR: 8.85e-04
Pre-train Epoch [445/4000], Train Loss: 0.035841, Val Loss: 0.075505, LR: 8.80e-04
Pre-train Epoch [446/4000], Train Loss: 0.035805, Val Loss: 0.074449, LR: 8.75e-04
Pre-train Epoch [447/4000], Train Loss: 0.034330, Val Loss: 0.073414, LR: 8.70e-04
验证损失连续 200 轮未改善，触发早停。
--- [阶段一] 本次运行完成，最佳损失: 0.067672 ---
--- [阶段一] 预训练完成，保存最佳模型 (损失: 0.067672) 至 results/two_stage_opamp_pretrained.pth ---
--- [阶段一] 加载最佳预训练模型 results/two_stage_opamp_pretrained.pth 以进行微调 ---

--- [阶段二] 开始整体模型微调 ---
Fine-tune Epoch [1/10000], Val NLL: 0.3099
- 微调验证损失改善 (inf -> 0.3099)。保存模型...
Fine-tune Epoch [2/10000], Val NLL: 0.0922
- 微调验证损失改善 (0.3099 -> 0.0922)。保存模型...
Fine-tune Epoch [3/10000], Val NLL: -0.0349
- 微调验证损失改善 (0.0922 -> -0.0349)。保存模型...
Fine-tune Epoch [4/10000], Val NLL: -0.1329
- 微调验证损失改善 (-0.0349 -> -0.1329)。保存模型...
Fine-tune Epoch [5/10000], Val NLL: -0.2227
- 微调验证损失改善 (-0.1329 -> -0.2227)。保存模型...
Fine-tune Epoch [6/10000], Val NLL: -0.3155
- 微调验证损失改善 (-0.2227 -> -0.3155)。保存模型...
Fine-tune Epoch [7/10000], Val NLL: -0.4005
- 微调验证损失改善 (-0.3155 -> -0.4005)。保存模型...
Fine-tune Epoch [8/10000], Val NLL: -0.4719
- 微调验证损失改善 (-0.4005 -> -0.4719)。保存模型...
Fine-tune Epoch [9/10000], Val NLL: -0.5223
- 微调验证损失改善 (-0.4719 -> -0.5223)。保存模型...
Fine-tune Epoch [10/10000], Val NLL: -0.5756
- 微调验证损失改善 (-0.5223 -> -0.5756)。保存模型...
Fine-tune Epoch [11/10000], Val NLL: -0.5980
- 微调验证损失改善 (-0.5756 -> -0.5980)。保存模型...
Fine-tune Epoch [12/10000], Val NLL: -0.6258
- 微调验证损失改善 (-0.5980 -> -0.6258)。保存模型...
Fine-tune Epoch [13/10000], Val NLL: -0.6563
- 微调验证损失改善 (-0.6258 -> -0.6563)。保存模型...
Fine-tune Epoch [14/10000], Val NLL: -0.6797
- 微调验证损失改善 (-0.6563 -> -0.6797)。保存模型...
Fine-tune Epoch [15/10000], Val NLL: -0.6912
- 微调验证损失改善 (-0.6797 -> -0.6912)。保存模型...
Fine-tune Epoch [16/10000], Val NLL: -0.7270
- 微调验证损失改善 (-0.6912 -> -0.7270)。保存模型...
Fine-tune Epoch [17/10000], Val NLL: -0.7374
- 微调验证损失改善 (-0.7270 -> -0.7374)。保存模型...
Fine-tune Epoch [18/10000], Val NLL: -0.7379
- 微调验证损失改善 (-0.7374 -> -0.7379)。保存模型...
Fine-tune Epoch [19/10000], Val NLL: -0.7662
- 微调验证损失改善 (-0.7379 -> -0.7662)。保存模型...
Fine-tune Epoch [20/10000], Val NLL: -0.7774
- 微调验证损失改善 (-0.7662 -> -0.7774)。保存模型...
Fine-tune Epoch [21/10000], Val NLL: -0.7668
Fine-tune Epoch [22/10000], Val NLL: -0.7883
- 微调验证损失改善 (-0.7774 -> -0.7883)。保存模型...
Fine-tune Epoch [23/10000], Val NLL: -0.7964
- 微调验证损失改善 (-0.7883 -> -0.7964)。保存模型...
Fine-tune Epoch [24/10000], Val NLL: -0.7999
- 微调验证损失改善 (-0.7964 -> -0.7999)。保存模型...
Fine-tune Epoch [25/10000], Val NLL: -0.8142
- 微调验证损失改善 (-0.7999 -> -0.8142)。保存模型...
Fine-tune Epoch [26/10000], Val NLL: -0.8131
Fine-tune Epoch [27/10000], Val NLL: -0.8381
- 微调验证损失改善 (-0.8142 -> -0.8381)。保存模型...
Fine-tune Epoch [28/10000], Val NLL: -0.8243
Fine-tune Epoch [29/10000], Val NLL: -0.8421
- 微调验证损失改善 (-0.8381 -> -0.8421)。保存模型...
Fine-tune Epoch [30/10000], Val NLL: -0.8524
- 微调验证损失改善 (-0.8421 -> -0.8524)。保存模型...
Fine-tune Epoch [31/10000], Val NLL: -0.8506
Fine-tune Epoch [32/10000], Val NLL: -0.8729
- 微调验证损失改善 (-0.8524 -> -0.8729)。保存模型...
Fine-tune Epoch [33/10000], Val NLL: -0.8616
Fine-tune Epoch [34/10000], Val NLL: -0.8741
- 微调验证损失改善 (-0.8729 -> -0.8741)。保存模型...
Fine-tune Epoch [35/10000], Val NLL: -0.8889
- 微调验证损失改善 (-0.8741 -> -0.8889)。保存模型...
Fine-tune Epoch [36/10000], Val NLL: -0.8951
- 微调验证损失改善 (-0.8889 -> -0.8951)。保存模型...
Fine-tune Epoch [37/10000], Val NLL: -0.9075
- 微调验证损失改善 (-0.8951 -> -0.9075)。保存模型...
Fine-tune Epoch [38/10000], Val NLL: -0.9062
Fine-tune Epoch [39/10000], Val NLL: -0.9073
Fine-tune Epoch [40/10000], Val NLL: -0.9181
- 微调验证损失改善 (-0.9075 -> -0.9181)。保存模型...
Fine-tune Epoch [41/10000], Val NLL: -0.9268
- 微调验证损失改善 (-0.9181 -> -0.9268)。保存模型...
Fine-tune Epoch [42/10000], Val NLL: -0.9273
- 微调验证损失改善 (-0.9268 -> -0.9273)。保存模型...
Fine-tune Epoch [43/10000], Val NLL: -0.9284
- 微调验证损失改善 (-0.9273 -> -0.9284)。保存模型...
Fine-tune Epoch [44/10000], Val NLL: -0.9256
Fine-tune Epoch [45/10000], Val NLL: -0.9480
- 微调验证损失改善 (-0.9284 -> -0.9480)。保存模型...
Fine-tune Epoch [46/10000], Val NLL: -0.9503
- 微调验证损失改善 (-0.9480 -> -0.9503)。保存模型...
Fine-tune Epoch [47/10000], Val NLL: -0.9603
- 微调验证损失改善 (-0.9503 -> -0.9603)。保存模型...
Fine-tune Epoch [48/10000], Val NLL: -0.9528
Fine-tune Epoch [49/10000], Val NLL: -0.9622
- 微调验证损失改善 (-0.9603 -> -0.9622)。保存模型...
Fine-tune Epoch [50/10000], Val NLL: -0.9545
Fine-tune Epoch [51/10000], Val NLL: -0.9634
- 微调验证损失改善 (-0.9622 -> -0.9634)。保存模型...
Fine-tune Epoch [52/10000], Val NLL: -0.9738
- 微调验证损失改善 (-0.9634 -> -0.9738)。保存模型...
Fine-tune Epoch [53/10000], Val NLL: -0.9603
Fine-tune Epoch [54/10000], Val NLL: -0.9717
Fine-tune Epoch [55/10000], Val NLL: -0.9763
- 微调验证损失改善 (-0.9738 -> -0.9763)。保存模型...
Fine-tune Epoch [56/10000], Val NLL: -0.9828
- 微调验证损失改善 (-0.9763 -> -0.9828)。保存模型...
Fine-tune Epoch [57/10000], Val NLL: -0.9818
Fine-tune Epoch [58/10000], Val NLL: -0.9860
- 微调验证损失改善 (-0.9828 -> -0.9860)。保存模型...
Fine-tune Epoch [59/10000], Val NLL: -1.0017
- 微调验证损失改善 (-0.9860 -> -1.0017)。保存模型...
Fine-tune Epoch [60/10000], Val NLL: -0.9974
Fine-tune Epoch [61/10000], Val NLL: -0.9925
Fine-tune Epoch [62/10000], Val NLL: -0.9765
Fine-tune Epoch [63/10000], Val NLL: -0.9962
Fine-tune Epoch [64/10000], Val NLL: -0.9890
Fine-tune Epoch [65/10000], Val NLL: -0.9827
Fine-tune Epoch [66/10000], Val NLL: -0.9901
Fine-tune Epoch [67/10000], Val NLL: -0.9996
Fine-tune Epoch [68/10000], Val NLL: -0.9993
Fine-tune Epoch [69/10000], Val NLL: -0.9902
Fine-tune Epoch [70/10000], Val NLL: -0.9716
Fine-tune Epoch [71/10000], Val NLL: -0.9978
Fine-tune Epoch [72/10000], Val NLL: -1.0083
- 微调验证损失改善 (-1.0017 -> -1.0083)。保存模型...
Fine-tune Epoch [73/10000], Val NLL: -0.9968
Fine-tune Epoch [74/10000], Val NLL: -0.9932
Fine-tune Epoch [75/10000], Val NLL: -0.9981
Fine-tune Epoch [76/10000], Val NLL: -0.9923
Fine-tune Epoch [77/10000], Val NLL: -0.9861
Fine-tune Epoch [78/10000], Val NLL: -0.9830
Fine-tune Epoch [79/10000], Val NLL: -0.9917
Fine-tune Epoch [80/10000], Val NLL: -0.9910
Fine-tune Epoch [81/10000], Val NLL: -0.9854
Fine-tune Epoch [82/10000], Val NLL: -1.0119
- 微调验证损失改善 (-1.0083 -> -1.0119)。保存模型...
Fine-tune Epoch [83/10000], Val NLL: -0.9972
Fine-tune Epoch [84/10000], Val NLL: -0.9880
Fine-tune Epoch [85/10000], Val NLL: -0.9711
Fine-tune Epoch [86/10000], Val NLL: -0.9988
Fine-tune Epoch [87/10000], Val NLL: -1.0031
Fine-tune Epoch [88/10000], Val NLL: -0.9930
Fine-tune Epoch [89/10000], Val NLL: -1.0033
Fine-tune Epoch [90/10000], Val NLL: -1.0031
Fine-tune Epoch [91/10000], Val NLL: -1.0198
- 微调验证损失改善 (-1.0119 -> -1.0198)。保存模型...
Fine-tune Epoch [92/10000], Val NLL: -1.0174
Fine-tune Epoch [93/10000], Val NLL: -1.0056
Fine-tune Epoch [94/10000], Val NLL: -1.0087
Fine-tune Epoch [95/10000], Val NLL: -1.0096
Fine-tune Epoch [96/10000], Val NLL: -1.0095
Fine-tune Epoch [97/10000], Val NLL: -1.0005
Fine-tune Epoch [98/10000], Val NLL: -1.0137
Fine-tune Epoch [99/10000], Val NLL: -1.0242
- 微调验证损失改善 (-1.0198 -> -1.0242)。保存模型...
Fine-tune Epoch [100/10000], Val NLL: -1.0149
Fine-tune Epoch [101/10000], Val NLL: -1.0111
Fine-tune Epoch [102/10000], Val NLL: -1.0177
Fine-tune Epoch [103/10000], Val NLL: -1.0074
Fine-tune Epoch [104/10000], Val NLL: -1.0185
Fine-tune Epoch [105/10000], Val NLL: -1.0100
Fine-tune Epoch [106/10000], Val NLL: -1.0138
Fine-tune Epoch [107/10000], Val NLL: -1.0114
Fine-tune Epoch [108/10000], Val NLL: -1.0051
Fine-tune Epoch [109/10000], Val NLL: -0.9994
Fine-tune Epoch [110/10000], Val NLL: -1.0056
Fine-tune Epoch [111/10000], Val NLL: -1.0232
Fine-tune Epoch [112/10000], Val NLL: -1.0215
Fine-tune Epoch [113/10000], Val NLL: -1.0184
Fine-tune Epoch [114/10000], Val NLL: -1.0052
Fine-tune Epoch [115/10000], Val NLL: -1.0027
Fine-tune Epoch [116/10000], Val NLL: -1.0183
Fine-tune Epoch [117/10000], Val NLL: -1.0107
Fine-tune Epoch [118/10000], Val NLL: -0.9913
Fine-tune Epoch [119/10000], Val NLL: -0.9979
Fine-tune Epoch [120/10000], Val NLL: -0.9832
Fine-tune Epoch [121/10000], Val NLL: -1.0098
Fine-tune Epoch [122/10000], Val NLL: -1.0015
Fine-tune Epoch [123/10000], Val NLL: -1.0031
Fine-tune Epoch [124/10000], Val NLL: -1.0270
- 微调验证损失改善 (-1.0242 -> -1.0270)。保存模型...
Fine-tune Epoch [125/10000], Val NLL: -1.0339
- 微调验证损失改善 (-1.0270 -> -1.0339)。保存模型...
Fine-tune Epoch [126/10000], Val NLL: -1.0062
Fine-tune Epoch [127/10000], Val NLL: -1.0062
Fine-tune Epoch [128/10000], Val NLL: -0.9918
Fine-tune Epoch [129/10000], Val NLL: -1.0025
Fine-tune Epoch [130/10000], Val NLL: -0.9883
Fine-tune Epoch [131/10000], Val NLL: -0.9823
Fine-tune Epoch [132/10000], Val NLL: -0.9956
Fine-tune Epoch [133/10000], Val NLL: -0.9833
Fine-tune Epoch [134/10000], Val NLL: -0.9907
Fine-tune Epoch [135/10000], Val NLL: -0.9909
Fine-tune Epoch [136/10000], Val NLL: -1.0015
Fine-tune Epoch [137/10000], Val NLL: -1.0041
Fine-tune Epoch [138/10000], Val NLL: -1.0169
Fine-tune Epoch [139/10000], Val NLL: -1.0276
Fine-tune Epoch [140/10000], Val NLL: -1.0007
Fine-tune Epoch [141/10000], Val NLL: -1.0071
Fine-tune Epoch [142/10000], Val NLL: -0.9884
Fine-tune Epoch [143/10000], Val NLL: -0.9895
Fine-tune Epoch [144/10000], Val NLL: -0.9950
Fine-tune Epoch [145/10000], Val NLL: -1.0179
Fine-tune Epoch [146/10000], Val NLL: -1.0021
Fine-tune Epoch [147/10000], Val NLL: -1.0076
Fine-tune Epoch [148/10000], Val NLL: -0.9810
Fine-tune Epoch [149/10000], Val NLL: -0.9894
Fine-tune Epoch [150/10000], Val NLL: -0.9747
Fine-tune Epoch [151/10000], Val NLL: -0.9952
Fine-tune Epoch [152/10000], Val NLL: -1.0185
Fine-tune Epoch [153/10000], Val NLL: -0.9968
Fine-tune Epoch [154/10000], Val NLL: -1.0203
Fine-tune Epoch [155/10000], Val NLL: -0.9933
Fine-tune Epoch [156/10000], Val NLL: -0.9971
Fine-tune Epoch [157/10000], Val NLL: -1.0062
Fine-tune Epoch [158/10000], Val NLL: -0.9937
Fine-tune Epoch [159/10000], Val NLL: -0.9988
Fine-tune Epoch [160/10000], Val NLL: -0.9831
Fine-tune Epoch [161/10000], Val NLL: -0.9789
Fine-tune Epoch [162/10000], Val NLL: -1.0169
Fine-tune Epoch [163/10000], Val NLL: -1.0050
Fine-tune Epoch [164/10000], Val NLL: -0.9971
Fine-tune Epoch [165/10000], Val NLL: -0.9971
Fine-tune Epoch [166/10000], Val NLL: -1.0001
Fine-tune Epoch [167/10000], Val NLL: -0.9984
Fine-tune Epoch [168/10000], Val NLL: -0.9763
Fine-tune Epoch [169/10000], Val NLL: -0.9952
Fine-tune Epoch [170/10000], Val NLL: -0.9846
Fine-tune Epoch [171/10000], Val NLL: -0.9882
Fine-tune Epoch [172/10000], Val NLL: -0.9881
Fine-tune Epoch [173/10000], Val NLL: -0.9783
Fine-tune Epoch [174/10000], Val NLL: -1.0029
Fine-tune Epoch [175/10000], Val NLL: -0.9896
Fine-tune Epoch [176/10000], Val NLL: -0.9946
Fine-tune Epoch [177/10000], Val NLL: -1.0068
Fine-tune Epoch [178/10000], Val NLL: -1.0011
Fine-tune Epoch [179/10000], Val NLL: -1.0130
Fine-tune Epoch [180/10000], Val NLL: -0.9779
Fine-tune Epoch [181/10000], Val NLL: -0.9676
Fine-tune Epoch [182/10000], Val NLL: -0.9682
Fine-tune Epoch [183/10000], Val NLL: -0.9775
Fine-tune Epoch [184/10000], Val NLL: -0.9841
Fine-tune Epoch [185/10000], Val NLL: -0.9773
Fine-tune Epoch [186/10000], Val NLL: -0.9826
Fine-tune Epoch [187/10000], Val NLL: -0.9924
Fine-tune Epoch [188/10000], Val NLL: -0.9736
Fine-tune Epoch [189/10000], Val NLL: -0.9559
Fine-tune Epoch [190/10000], Val NLL: -0.9993
Fine-tune Epoch [191/10000], Val NLL: -0.9592
Fine-tune Epoch [192/10000], Val NLL: -0.9550
Fine-tune Epoch [193/10000], Val NLL: -0.9882
Fine-tune Epoch [194/10000], Val NLL: -0.9829
Fine-tune Epoch [195/10000], Val NLL: -0.9625
Fine-tune Epoch [196/10000], Val NLL: -0.9718
Fine-tune Epoch [197/10000], Val NLL: -0.9877
Fine-tune Epoch [198/10000], Val NLL: -0.9983
Fine-tune Epoch [199/10000], Val NLL: -1.0101
Fine-tune Epoch [200/10000], Val NLL: -1.0071
Fine-tune Epoch [201/10000], Val NLL: -1.0000
Fine-tune Epoch [202/10000], Val NLL: -1.0073
Fine-tune Epoch [203/10000], Val NLL: -1.0138
Fine-tune Epoch [204/10000], Val NLL: -1.0268
Fine-tune Epoch [205/10000], Val NLL: -1.0090
Fine-tune Epoch [206/10000], Val NLL: -1.0361
- 微调验证损失改善 (-1.0339 -> -1.0361)。保存模型...
Fine-tune Epoch [207/10000], Val NLL: -1.0050
Fine-tune Epoch [208/10000], Val NLL: -1.0000
Fine-tune Epoch [209/10000], Val NLL: -0.9956
Fine-tune Epoch [210/10000], Val NLL: -0.9926
Fine-tune Epoch [211/10000], Val NLL: -0.9858
Fine-tune Epoch [212/10000], Val NLL: -0.9981
Fine-tune Epoch [213/10000], Val NLL: -1.0145
Fine-tune Epoch [214/10000], Val NLL: -1.0036
Fine-tune Epoch [215/10000], Val NLL: -0.9774
Fine-tune Epoch [216/10000], Val NLL: -0.9472
Fine-tune Epoch [217/10000], Val NLL: -0.9834
Fine-tune Epoch [218/10000], Val NLL: -0.9687
Fine-tune Epoch [219/10000], Val NLL: -0.9818
Fine-tune Epoch [220/10000], Val NLL: -0.9745
Fine-tune Epoch [221/10000], Val NLL: -0.9684
Fine-tune Epoch [222/10000], Val NLL: -0.9746
Fine-tune Epoch [223/10000], Val NLL: -0.9942
Fine-tune Epoch [224/10000], Val NLL: -0.9705
Fine-tune Epoch [225/10000], Val NLL: -0.9848
Fine-tune Epoch [226/10000], Val NLL: -0.9759
Fine-tune Epoch [227/10000], Val NLL: -0.9526
Fine-tune Epoch [228/10000], Val NLL: -0.9156
Fine-tune Epoch [229/10000], Val NLL: -0.9397
Fine-tune Epoch [230/10000], Val NLL: -0.9503
Fine-tune Epoch [231/10000], Val NLL: -0.9643
Fine-tune Epoch [232/10000], Val NLL: -0.9666
Fine-tune Epoch [233/10000], Val NLL: -0.9773
Fine-tune Epoch [234/10000], Val NLL: -0.9637
Fine-tune Epoch [235/10000], Val NLL: -0.9991
Fine-tune Epoch [236/10000], Val NLL: -0.9620
Fine-tune Epoch [237/10000], Val NLL: -0.9827
Fine-tune Epoch [238/10000], Val NLL: -0.9712
Fine-tune Epoch [239/10000], Val NLL: -0.9704
Fine-tune Epoch [240/10000], Val NLL: -0.9838
Fine-tune Epoch [241/10000], Val NLL: -0.9662
Fine-tune Epoch [242/10000], Val NLL: -0.9656
Fine-tune Epoch [243/10000], Val NLL: -0.9600
Fine-tune Epoch [244/10000], Val NLL: -0.9711
Fine-tune Epoch [245/10000], Val NLL: -0.9621
Fine-tune Epoch [246/10000], Val NLL: -0.9860
Fine-tune Epoch [247/10000], Val NLL: -0.9738
Fine-tune Epoch [248/10000], Val NLL: -0.9372
Fine-tune Epoch [249/10000], Val NLL: -0.9747
Fine-tune Epoch [250/10000], Val NLL: -0.9540
Fine-tune Epoch [251/10000], Val NLL: -0.9778
Fine-tune Epoch [252/10000], Val NLL: -0.9596
Fine-tune Epoch [253/10000], Val NLL: -0.9755
Fine-tune Epoch [254/10000], Val NLL: -0.9581
Fine-tune Epoch [255/10000], Val NLL: -0.9741
Fine-tune Epoch [256/10000], Val NLL: -0.9552
Fine-tune Epoch [257/10000], Val NLL: -0.9781
Fine-tune Epoch [258/10000], Val NLL: -0.9935
Fine-tune Epoch [259/10000], Val NLL: -0.9771
Fine-tune Epoch [260/10000], Val NLL: -0.9667
Fine-tune Epoch [261/10000], Val NLL: -1.0058
Fine-tune Epoch [262/10000], Val NLL: -0.9925
Fine-tune Epoch [263/10000], Val NLL: -0.9845
Fine-tune Epoch [264/10000], Val NLL: -0.9697
Fine-tune Epoch [265/10000], Val NLL: -0.9655
Fine-tune Epoch [266/10000], Val NLL: -0.9622
Fine-tune Epoch [267/10000], Val NLL: -0.9720
Fine-tune Epoch [268/10000], Val NLL: -1.0005
Fine-tune Epoch [269/10000], Val NLL: -0.9900
Fine-tune Epoch [270/10000], Val NLL: -0.9865
Fine-tune Epoch [271/10000], Val NLL: -1.0000
Fine-tune Epoch [272/10000], Val NLL: -0.9760
Fine-tune Epoch [273/10000], Val NLL: -0.9844
Fine-tune Epoch [274/10000], Val NLL: -0.9475
Fine-tune Epoch [275/10000], Val NLL: -0.9454
Fine-tune Epoch [276/10000], Val NLL: -0.9591
Fine-tune Epoch [277/10000], Val NLL: -0.9473
Fine-tune Epoch [278/10000], Val NLL: -0.9731
Fine-tune Epoch [279/10000], Val NLL: -0.9840
Fine-tune Epoch [280/10000], Val NLL: -0.9902
Fine-tune Epoch [281/10000], Val NLL: -0.9875
Fine-tune Epoch [282/10000], Val NLL: -0.9680
Fine-tune Epoch [283/10000], Val NLL: -0.9902
Fine-tune Epoch [284/10000], Val NLL: -0.9703
Fine-tune Epoch [285/10000], Val NLL: -0.9676
Fine-tune Epoch [286/10000], Val NLL: -0.9812
Fine-tune Epoch [287/10000], Val NLL: -0.9858
Fine-tune Epoch [288/10000], Val NLL: -0.9904
Fine-tune Epoch [289/10000], Val NLL: -0.9899
Fine-tune Epoch [290/10000], Val NLL: -0.9409
Fine-tune Epoch [291/10000], Val NLL: -0.9753
Fine-tune Epoch [292/10000], Val NLL: -0.9801
Fine-tune Epoch [293/10000], Val NLL: -0.9983
Fine-tune Epoch [294/10000], Val NLL: -0.9734
Fine-tune Epoch [295/10000], Val NLL: -0.9523
Fine-tune Epoch [296/10000], Val NLL: -0.9144
Fine-tune Epoch [297/10000], Val NLL: -0.9523
Fine-tune Epoch [298/10000], Val NLL: -0.9641
Fine-tune Epoch [299/10000], Val NLL: -0.9731
Fine-tune Epoch [300/10000], Val NLL: -0.9997
Fine-tune Epoch [301/10000], Val NLL: -0.9657
Fine-tune Epoch [302/10000], Val NLL: -0.9603
Fine-tune Epoch [303/10000], Val NLL: -0.9506
Fine-tune Epoch [304/10000], Val NLL: -0.9961
Fine-tune Epoch [305/10000], Val NLL: -0.9780
Fine-tune Epoch [306/10000], Val NLL: -0.9862
Fine-tune Epoch [307/10000], Val NLL: -0.9493
Fine-tune Epoch [308/10000], Val NLL: -0.9851
Fine-tune Epoch [309/10000], Val NLL: -0.9885
Fine-tune Epoch [310/10000], Val NLL: -0.9885
Fine-tune Epoch [311/10000], Val NLL: -0.9628
Fine-tune Epoch [312/10000], Val NLL: -0.9923
Fine-tune Epoch [313/10000], Val NLL: -0.9588
Fine-tune Epoch [314/10000], Val NLL: -0.9640
Fine-tune Epoch [315/10000], Val NLL: -0.9733
Fine-tune Epoch [316/10000], Val NLL: -0.9792
Fine-tune Epoch [317/10000], Val NLL: -1.0052
Fine-tune Epoch [318/10000], Val NLL: -1.0291
Fine-tune Epoch [319/10000], Val NLL: -0.9917
Fine-tune Epoch [320/10000], Val NLL: -0.9844
Fine-tune Epoch [321/10000], Val NLL: -0.9818
Fine-tune Epoch [322/10000], Val NLL: -0.9949
Fine-tune Epoch [323/10000], Val NLL: -0.9745
Fine-tune Epoch [324/10000], Val NLL: -0.9994
Fine-tune Epoch [325/10000], Val NLL: -0.9464
Fine-tune Epoch [326/10000], Val NLL: -0.9747
Fine-tune Epoch [327/10000], Val NLL: -0.9784
Fine-tune Epoch [328/10000], Val NLL: -0.9870
Fine-tune Epoch [329/10000], Val NLL: -0.9608
Fine-tune Epoch [330/10000], Val NLL: -0.9596
Fine-tune Epoch [331/10000], Val NLL: -0.9585
Fine-tune Epoch [332/10000], Val NLL: -0.9699
Fine-tune Epoch [333/10000], Val NLL: -0.9570
Fine-tune Epoch [334/10000], Val NLL: -0.9449
Fine-tune Epoch [335/10000], Val NLL: -0.9333
Fine-tune Epoch [336/10000], Val NLL: -0.9723
Fine-tune Epoch [337/10000], Val NLL: -0.9610
Fine-tune Epoch [338/10000], Val NLL: -0.9508
Fine-tune Epoch [339/10000], Val NLL: -0.9411
Fine-tune Epoch [340/10000], Val NLL: -0.9244
Fine-tune Epoch [341/10000], Val NLL: -0.9378
Fine-tune Epoch [342/10000], Val NLL: -0.9516
Fine-tune Epoch [343/10000], Val NLL: -0.9470
Fine-tune Epoch [344/10000], Val NLL: -0.9471
Fine-tune Epoch [345/10000], Val NLL: -0.9402
Fine-tune Epoch [346/10000], Val NLL: -0.9712
Fine-tune Epoch [347/10000], Val NLL: -0.9751
Fine-tune Epoch [348/10000], Val NLL: -0.9746
Fine-tune Epoch [349/10000], Val NLL: -0.9534
Fine-tune Epoch [350/10000], Val NLL: -0.9414
Fine-tune Epoch [351/10000], Val NLL: -0.9206
Fine-tune Epoch [352/10000], Val NLL: -0.9619
Fine-tune Epoch [353/10000], Val NLL: -0.9189
Fine-tune Epoch [354/10000], Val NLL: -0.9262
Fine-tune Epoch [355/10000], Val NLL: -0.8873
Fine-tune Epoch [356/10000], Val NLL: -0.9558
Fine-tune Epoch [357/10000], Val NLL: -0.9356
Fine-tune Epoch [358/10000], Val NLL: -0.9434
Fine-tune Epoch [359/10000], Val NLL: -0.9427
Fine-tune Epoch [360/10000], Val NLL: -0.9684
Fine-tune Epoch [361/10000], Val NLL: -0.9804
Fine-tune Epoch [362/10000], Val NLL: -0.9518
Fine-tune Epoch [363/10000], Val NLL: -0.9384
Fine-tune Epoch [364/10000], Val NLL: -0.9345
Fine-tune Epoch [365/10000], Val NLL: -0.9346
Fine-tune Epoch [366/10000], Val NLL: -0.9319
Fine-tune Epoch [367/10000], Val NLL: -0.8981
Fine-tune Epoch [368/10000], Val NLL: -0.9099
Fine-tune Epoch [369/10000], Val NLL: -0.9250
Fine-tune Epoch [370/10000], Val NLL: -0.9232
Fine-tune Epoch [371/10000], Val NLL: -0.9301
Fine-tune Epoch [372/10000], Val NLL: -0.9167
Fine-tune Epoch [373/10000], Val NLL: -0.9347
Fine-tune Epoch [374/10000], Val NLL: -0.9120
Fine-tune Epoch [375/10000], Val NLL: -0.9128
Fine-tune Epoch [376/10000], Val NLL: -0.8828
Fine-tune Epoch [377/10000], Val NLL: -0.8681
Fine-tune Epoch [378/10000], Val NLL: -0.8608
Fine-tune Epoch [379/10000], Val NLL: -0.8906
Fine-tune Epoch [380/10000], Val NLL: -0.9056
Fine-tune Epoch [381/10000], Val NLL: -0.8946
Fine-tune Epoch [382/10000], Val NLL: -0.9365
Fine-tune Epoch [383/10000], Val NLL: -0.8861
Fine-tune Epoch [384/10000], Val NLL: -0.9298
Fine-tune Epoch [385/10000], Val NLL: -0.9167
Fine-tune Epoch [386/10000], Val NLL: -0.9293
Fine-tune Epoch [387/10000], Val NLL: -0.9341
Fine-tune Epoch [388/10000], Val NLL: -0.9272
Fine-tune Epoch [389/10000], Val NLL: -0.9425
Fine-tune Epoch [390/10000], Val NLL: -0.9531
Fine-tune Epoch [391/10000], Val NLL: -0.9185
Fine-tune Epoch [392/10000], Val NLL: -0.9104
Fine-tune Epoch [393/10000], Val NLL: -0.8989
Fine-tune Epoch [394/10000], Val NLL: -0.8869
Fine-tune Epoch [395/10000], Val NLL: -0.9210
Fine-tune Epoch [396/10000], Val NLL: -0.9073
Fine-tune Epoch [397/10000], Val NLL: -0.9085
Fine-tune Epoch [398/10000], Val NLL: -0.9098
Fine-tune Epoch [399/10000], Val NLL: -0.9221
Fine-tune Epoch [400/10000], Val NLL: -0.9120
Fine-tune Epoch [401/10000], Val NLL: -0.8930
Fine-tune Epoch [402/10000], Val NLL: -0.8783
Fine-tune Epoch [403/10000], Val NLL: -0.9025
Fine-tune Epoch [404/10000], Val NLL: -0.9040
Fine-tune Epoch [405/10000], Val NLL: -0.8969
Fine-tune Epoch [406/10000], Val NLL: -0.8691
Fine-tune Epoch [407/10000], Val NLL: -0.8956
Fine-tune Epoch [408/10000], Val NLL: -0.8823
Fine-tune Epoch [409/10000], Val NLL: -0.8723
Fine-tune Epoch [410/10000], Val NLL: -0.8914
Fine-tune Epoch [411/10000], Val NLL: -0.8716
Fine-tune Epoch [412/10000], Val NLL: -0.8769
Fine-tune Epoch [413/10000], Val NLL: -0.8766
Fine-tune Epoch [414/10000], Val NLL: -0.8866
Fine-tune Epoch [415/10000], Val NLL: -0.8539
Fine-tune Epoch [416/10000], Val NLL: -0.8676
Fine-tune Epoch [417/10000], Val NLL: -0.8670
Fine-tune Epoch [418/10000], Val NLL: -0.8824
Fine-tune Epoch [419/10000], Val NLL: -0.8355
Fine-tune Epoch [420/10000], Val NLL: -0.8382
Fine-tune Epoch [421/10000], Val NLL: -0.8447
Fine-tune Epoch [422/10000], Val NLL: -0.8833
Fine-tune Epoch [423/10000], Val NLL: -0.9006
Fine-tune Epoch [424/10000], Val NLL: -0.9193
Fine-tune Epoch [425/10000], Val NLL: -0.8923
Fine-tune Epoch [426/10000], Val NLL: -0.8994
Fine-tune Epoch [427/10000], Val NLL: -0.8975
Fine-tune Epoch [428/10000], Val NLL: -0.9013
Fine-tune Epoch [429/10000], Val NLL: -0.9299
Fine-tune Epoch [430/10000], Val NLL: -0.9034
Fine-tune Epoch [431/10000], Val NLL: -0.8985
Fine-tune Epoch [432/10000], Val NLL: -0.8936
Fine-tune Epoch [433/10000], Val NLL: -0.9167
Fine-tune Epoch [434/10000], Val NLL: -0.8784
Fine-tune Epoch [435/10000], Val NLL: -0.8835
Fine-tune Epoch [436/10000], Val NLL: -0.8778
Fine-tune Epoch [437/10000], Val NLL: -0.9013
Fine-tune Epoch [438/10000], Val NLL: -0.8694
Fine-tune Epoch [439/10000], Val NLL: -0.8687
Fine-tune Epoch [440/10000], Val NLL: -0.8823
Fine-tune Epoch [441/10000], Val NLL: -0.8972
Fine-tune Epoch [442/10000], Val NLL: -0.8890
Fine-tune Epoch [443/10000], Val NLL: -0.8962
Fine-tune Epoch [444/10000], Val NLL: -0.8785
Fine-tune Epoch [445/10000], Val NLL: -0.8916
Fine-tune Epoch [446/10000], Val NLL: -0.9004
Fine-tune Epoch [447/10000], Val NLL: -0.8879
Fine-tune Epoch [448/10000], Val NLL: -0.8840
Fine-tune Epoch [449/10000], Val NLL: -0.9091
Fine-tune Epoch [450/10000], Val NLL: -0.9055
Fine-tune Epoch [451/10000], Val NLL: -0.8618
Fine-tune Epoch [452/10000], Val NLL: -0.9004
Fine-tune Epoch [453/10000], Val NLL: -0.8744
Fine-tune Epoch [454/10000], Val NLL: -0.8939
Fine-tune Epoch [455/10000], Val NLL: -0.8932
Fine-tune Epoch [456/10000], Val NLL: -0.8921
Fine-tune Epoch [457/10000], Val NLL: -0.8804
Fine-tune Epoch [458/10000], Val NLL: -0.8576
Fine-tune Epoch [459/10000], Val NLL: -0.8844
Fine-tune Epoch [460/10000], Val NLL: -0.8446
Fine-tune Epoch [461/10000], Val NLL: -0.8683
Fine-tune Epoch [462/10000], Val NLL: -0.8547
Fine-tune Epoch [463/10000], Val NLL: -0.8818
Fine-tune Epoch [464/10000], Val NLL: -0.8671
Fine-tune Epoch [465/10000], Val NLL: -0.8460
Fine-tune Epoch [466/10000], Val NLL: -0.8707
Fine-tune Epoch [467/10000], Val NLL: -0.8898
Fine-tune Epoch [468/10000], Val NLL: -0.8897
Fine-tune Epoch [469/10000], Val NLL: -0.8579
Fine-tune Epoch [470/10000], Val NLL: -0.8783
Fine-tune Epoch [471/10000], Val NLL: -0.8549
Fine-tune Epoch [472/10000], Val NLL: -0.8335
Fine-tune Epoch [473/10000], Val NLL: -0.8739
Fine-tune Epoch [474/10000], Val NLL: -0.8738
Fine-tune Epoch [475/10000], Val NLL: -0.8866
Fine-tune Epoch [476/10000], Val NLL: -0.8811
Fine-tune Epoch [477/10000], Val NLL: -0.8732
Fine-tune Epoch [478/10000], Val NLL: -0.8824
Fine-tune Epoch [479/10000], Val NLL: -0.8808
Fine-tune Epoch [480/10000], Val NLL: -0.8719
Fine-tune Epoch [481/10000], Val NLL: -0.8749
Fine-tune Epoch [482/10000], Val NLL: -0.8919
Fine-tune Epoch [483/10000], Val NLL: -0.8829
Fine-tune Epoch [484/10000], Val NLL: -0.8863
Fine-tune Epoch [485/10000], Val NLL: -0.8359
Fine-tune Epoch [486/10000], Val NLL: -0.8653
Fine-tune Epoch [487/10000], Val NLL: -0.8662
Fine-tune Epoch [488/10000], Val NLL: -0.8470
Fine-tune Epoch [489/10000], Val NLL: -0.8823
Fine-tune Epoch [490/10000], Val NLL: -0.8804
Fine-tune Epoch [491/10000], Val NLL: -0.8728
Fine-tune Epoch [492/10000], Val NLL: -0.9055
Fine-tune Epoch [493/10000], Val NLL: -0.8663
Fine-tune Epoch [494/10000], Val NLL: -0.8877
Fine-tune Epoch [495/10000], Val NLL: -0.9053
Fine-tune Epoch [496/10000], Val NLL: -0.8674
Fine-tune Epoch [497/10000], Val NLL: -0.8604
Fine-tune Epoch [498/10000], Val NLL: -0.8614
Fine-tune Epoch [499/10000], Val NLL: -0.8220
Fine-tune Epoch [500/10000], Val NLL: -0.8314
Fine-tune Epoch [501/10000], Val NLL: -0.8235
Fine-tune Epoch [502/10000], Val NLL: -0.8147
Fine-tune Epoch [503/10000], Val NLL: -0.8031
Fine-tune Epoch [504/10000], Val NLL: -0.8225
Fine-tune Epoch [505/10000], Val NLL: -0.8405
Fine-tune Epoch [506/10000], Val NLL: -0.8131
Fine-tune Epoch [507/10000], Val NLL: -0.8036
Fine-tune Epoch [508/10000], Val NLL: -0.7859
Fine-tune Epoch [509/10000], Val NLL: -0.8201
Fine-tune Epoch [510/10000], Val NLL: -0.8280
Fine-tune Epoch [511/10000], Val NLL: -0.8350
Fine-tune Epoch [512/10000], Val NLL: -0.7961
Fine-tune Epoch [513/10000], Val NLL: -0.7781
Fine-tune Epoch [514/10000], Val NLL: -0.8104
Fine-tune Epoch [515/10000], Val NLL: -0.8251
Fine-tune Epoch [516/10000], Val NLL: -0.8361
Fine-tune Epoch [517/10000], Val NLL: -0.8101
Fine-tune Epoch [518/10000], Val NLL: -0.7754
Fine-tune Epoch [519/10000], Val NLL: -0.7777
Fine-tune Epoch [520/10000], Val NLL: -0.8332
Fine-tune Epoch [521/10000], Val NLL: -0.8197
Fine-tune Epoch [522/10000], Val NLL: -0.8151
Fine-tune Epoch [523/10000], Val NLL: -0.8128
Fine-tune Epoch [524/10000], Val NLL: -0.8278
Fine-tune Epoch [525/10000], Val NLL: -0.8082
Fine-tune Epoch [526/10000], Val NLL: -0.8370
Fine-tune Epoch [527/10000], Val NLL: -0.8261
Fine-tune Epoch [528/10000], Val NLL: -0.8257
Fine-tune Epoch [529/10000], Val NLL: -0.8045
Fine-tune Epoch [530/10000], Val NLL: -0.8195
Fine-tune Epoch [531/10000], Val NLL: -0.8040
Fine-tune Epoch [532/10000], Val NLL: -0.8530
Fine-tune Epoch [533/10000], Val NLL: -0.8686
Fine-tune Epoch [534/10000], Val NLL: -0.8503
Fine-tune Epoch [535/10000], Val NLL: -0.8214
Fine-tune Epoch [536/10000], Val NLL: -0.7609
Fine-tune Epoch [537/10000], Val NLL: -0.7678
Fine-tune Epoch [538/10000], Val NLL: -0.7767
Fine-tune Epoch [539/10000], Val NLL: -0.8071
Fine-tune Epoch [540/10000], Val NLL: -0.8141
Fine-tune Epoch [541/10000], Val NLL: -0.8201
Fine-tune Epoch [542/10000], Val NLL: -0.8245
Fine-tune Epoch [543/10000], Val NLL: -0.7932
Fine-tune Epoch [544/10000], Val NLL: -0.7464
Fine-tune Epoch [545/10000], Val NLL: -0.7976
Fine-tune Epoch [546/10000], Val NLL: -0.7873
Fine-tune Epoch [547/10000], Val NLL: -0.8145
Fine-tune Epoch [548/10000], Val NLL: -0.8074
Fine-tune Epoch [549/10000], Val NLL: -0.8126
Fine-tune Epoch [550/10000], Val NLL: -0.7805
Fine-tune Epoch [551/10000], Val NLL: -0.7699
Fine-tune Epoch [552/10000], Val NLL: -0.7975
Fine-tune Epoch [553/10000], Val NLL: -0.7778
Fine-tune Epoch [554/10000], Val NLL: -0.7915
Fine-tune Epoch [555/10000], Val NLL: -0.7877
Fine-tune Epoch [556/10000], Val NLL: -0.7897
Fine-tune Epoch [557/10000], Val NLL: -0.8213
Fine-tune Epoch [558/10000], Val NLL: -0.7900
Fine-tune Epoch [559/10000], Val NLL: -0.7613
Fine-tune Epoch [560/10000], Val NLL: -0.8230
Fine-tune Epoch [561/10000], Val NLL: -0.7845
Fine-tune Epoch [562/10000], Val NLL: -0.7549
Fine-tune Epoch [563/10000], Val NLL: -0.7182
Fine-tune Epoch [564/10000], Val NLL: -0.7641
Fine-tune Epoch [565/10000], Val NLL: -0.7229
Fine-tune Epoch [566/10000], Val NLL: -0.7856
Fine-tune Epoch [567/10000], Val NLL: -0.7805
Fine-tune Epoch [568/10000], Val NLL: -0.7930
Fine-tune Epoch [569/10000], Val NLL: -0.7407
Fine-tune Epoch [570/10000], Val NLL: -0.7159
Fine-tune Epoch [571/10000], Val NLL: -0.7010
Fine-tune Epoch [572/10000], Val NLL: -0.7389
Fine-tune Epoch [573/10000], Val NLL: -0.7656
Fine-tune Epoch [574/10000], Val NLL: -0.7809
Fine-tune Epoch [575/10000], Val NLL: -0.7701
Fine-tune Epoch [576/10000], Val NLL: -0.7897
Fine-tune Epoch [577/10000], Val NLL: -0.7957
Fine-tune Epoch [578/10000], Val NLL: -0.7894
Fine-tune Epoch [579/10000], Val NLL: -0.7696
Fine-tune Epoch [580/10000], Val NLL: -0.7809
Fine-tune Epoch [581/10000], Val NLL: -0.7817
Fine-tune Epoch [582/10000], Val NLL: -0.7845
Fine-tune Epoch [583/10000], Val NLL: -0.8314
Fine-tune Epoch [584/10000], Val NLL: -0.7904
Fine-tune Epoch [585/10000], Val NLL: -0.7831
Fine-tune Epoch [586/10000], Val NLL: -0.7806
Fine-tune Epoch [587/10000], Val NLL: -0.8183
Fine-tune Epoch [588/10000], Val NLL: -0.8070
Fine-tune Epoch [589/10000], Val NLL: -0.7478
Fine-tune Epoch [590/10000], Val NLL: -0.7293
Fine-tune Epoch [591/10000], Val NLL: -0.7495
Fine-tune Epoch [592/10000], Val NLL: -0.7699
Fine-tune Epoch [593/10000], Val NLL: -0.7726
Fine-tune Epoch [594/10000], Val NLL: -0.7926
Fine-tune Epoch [595/10000], Val NLL: -0.7805
Fine-tune Epoch [596/10000], Val NLL: -0.7902
Fine-tune Epoch [597/10000], Val NLL: -0.7800
Fine-tune Epoch [598/10000], Val NLL: -0.7926
Fine-tune Epoch [599/10000], Val NLL: -0.7945
Fine-tune Epoch [600/10000], Val NLL: -0.7908
Fine-tune Epoch [601/10000], Val NLL: -0.7367
Fine-tune Epoch [602/10000], Val NLL: -0.7088
Fine-tune Epoch [603/10000], Val NLL: -0.7329
Fine-tune Epoch [604/10000], Val NLL: -0.6983
Fine-tune Epoch [605/10000], Val NLL: -0.7170
Fine-tune Epoch [606/10000], Val NLL: -0.7462
Fine-tune Epoch [607/10000], Val NLL: -0.7313
Fine-tune Epoch [608/10000], Val NLL: -0.7799
Fine-tune Epoch [609/10000], Val NLL: -0.7623
Fine-tune Epoch [610/10000], Val NLL: -0.7018
Fine-tune Epoch [611/10000], Val NLL: -0.7214
Fine-tune Epoch [612/10000], Val NLL: -0.7061
Fine-tune Epoch [613/10000], Val NLL: -0.7464
Fine-tune Epoch [614/10000], Val NLL: -0.7584
Fine-tune Epoch [615/10000], Val NLL: -0.7361
Fine-tune Epoch [616/10000], Val NLL: -0.6698
Fine-tune Epoch [617/10000], Val NLL: -0.6961
Fine-tune Epoch [618/10000], Val NLL: -0.6929
Fine-tune Epoch [619/10000], Val NLL: -0.7384
Fine-tune Epoch [620/10000], Val NLL: -0.7533
Fine-tune Epoch [621/10000], Val NLL: -0.7465
Fine-tune Epoch [622/10000], Val NLL: -0.7883
Fine-tune Epoch [623/10000], Val NLL: -0.7611
Fine-tune Epoch [624/10000], Val NLL: -0.7696
Fine-tune Epoch [625/10000], Val NLL: -0.7786
Fine-tune Epoch [626/10000], Val NLL: -0.8113
Fine-tune Epoch [627/10000], Val NLL: -0.7667
Fine-tune Epoch [628/10000], Val NLL: -0.7450
Fine-tune Epoch [629/10000], Val NLL: -0.7486
Fine-tune Epoch [630/10000], Val NLL: -0.7769
Fine-tune Epoch [631/10000], Val NLL: -0.7650
Fine-tune Epoch [632/10000], Val NLL: -0.7304
Fine-tune Epoch [633/10000], Val NLL: -0.7184
Fine-tune Epoch [634/10000], Val NLL: -0.7012
Fine-tune Epoch [635/10000], Val NLL: -0.6900
Fine-tune Epoch [636/10000], Val NLL: -0.6694
Fine-tune Epoch [637/10000], Val NLL: -0.6792
Fine-tune Epoch [638/10000], Val NLL: -0.6943
Fine-tune Epoch [639/10000], Val NLL: -0.6623
Fine-tune Epoch [640/10000], Val NLL: -0.6946
Fine-tune Epoch [641/10000], Val NLL: -0.7014
Fine-tune Epoch [642/10000], Val NLL: -0.6742
Fine-tune Epoch [643/10000], Val NLL: -0.7104
Fine-tune Epoch [644/10000], Val NLL: -0.6436
Fine-tune Epoch [645/10000], Val NLL: -0.6732
Fine-tune Epoch [646/10000], Val NLL: -0.6694
Fine-tune Epoch [647/10000], Val NLL: -0.6615
Fine-tune Epoch [648/10000], Val NLL: -0.7182
Fine-tune Epoch [649/10000], Val NLL: -0.6926
Fine-tune Epoch [650/10000], Val NLL: -0.6814
Fine-tune Epoch [651/10000], Val NLL: -0.6763
Fine-tune Epoch [652/10000], Val NLL: -0.7096
Fine-tune Epoch [653/10000], Val NLL: -0.6694
Fine-tune Epoch [654/10000], Val NLL: -0.6894
Fine-tune Epoch [655/10000], Val NLL: -0.6786
Fine-tune Epoch [656/10000], Val NLL: -0.6966
Fine-tune Epoch [657/10000], Val NLL: -0.6340
Fine-tune Epoch [658/10000], Val NLL: -0.6716
Fine-tune Epoch [659/10000], Val NLL: -0.6371
Fine-tune Epoch [660/10000], Val NLL: -0.6639
Fine-tune Epoch [661/10000], Val NLL: -0.7119
Fine-tune Epoch [662/10000], Val NLL: -0.7185
Fine-tune Epoch [663/10000], Val NLL: -0.6585
Fine-tune Epoch [664/10000], Val NLL: -0.6287
Fine-tune Epoch [665/10000], Val NLL: -0.6702
Fine-tune Epoch [666/10000], Val NLL: -0.6403
Fine-tune Epoch [667/10000], Val NLL: -0.6381
Fine-tune Epoch [668/10000], Val NLL: -0.6703
Fine-tune Epoch [669/10000], Val NLL: -0.6576
Fine-tune Epoch [670/10000], Val NLL: -0.6645
Fine-tune Epoch [671/10000], Val NLL: -0.6479
Fine-tune Epoch [672/10000], Val NLL: -0.6929
Fine-tune Epoch [673/10000], Val NLL: -0.7102
Fine-tune Epoch [674/10000], Val NLL: -0.6968
Fine-tune Epoch [675/10000], Val NLL: -0.6565
Fine-tune Epoch [676/10000], Val NLL: -0.6617
Fine-tune Epoch [677/10000], Val NLL: -0.6237
Fine-tune Epoch [678/10000], Val NLL: -0.6279
Fine-tune Epoch [679/10000], Val NLL: -0.6785
Fine-tune Epoch [680/10000], Val NLL: -0.6998
Fine-tune Epoch [681/10000], Val NLL: -0.7126
Fine-tune Epoch [682/10000], Val NLL: -0.6761
Fine-tune Epoch [683/10000], Val NLL: -0.6387
Fine-tune Epoch [684/10000], Val NLL: -0.6739
Fine-tune Epoch [685/10000], Val NLL: -0.6960
Fine-tune Epoch [686/10000], Val NLL: -0.6841
Fine-tune Epoch [687/10000], Val NLL: -0.6633
Fine-tune Epoch [688/10000], Val NLL: -0.6118
Fine-tune Epoch [689/10000], Val NLL: -0.6111
Fine-tune Epoch [690/10000], Val NLL: -0.6315
Fine-tune Epoch [691/10000], Val NLL: -0.6406
Fine-tune Epoch [692/10000], Val NLL: -0.6149
Fine-tune Epoch [693/10000], Val NLL: -0.6271
Fine-tune Epoch [694/10000], Val NLL: -0.6276
Fine-tune Epoch [695/10000], Val NLL: -0.6357
Fine-tune Epoch [696/10000], Val NLL: -0.6172
Fine-tune Epoch [697/10000], Val NLL: -0.6523
Fine-tune Epoch [698/10000], Val NLL: -0.6023
Fine-tune Epoch [699/10000], Val NLL: -0.5428
Fine-tune Epoch [700/10000], Val NLL: -0.6266
Fine-tune Epoch [701/10000], Val NLL: -0.6180
Fine-tune Epoch [702/10000], Val NLL: -0.6276
Fine-tune Epoch [703/10000], Val NLL: -0.6576
Fine-tune Epoch [704/10000], Val NLL: -0.6417
Fine-tune Epoch [705/10000], Val NLL: -0.6167
Fine-tune Epoch [706/10000], Val NLL: -0.5824
验证损失连续 500 轮未改善，触发早停。
--- [阶段二] 微调完成，最佳模型已保存至 results/two_stage_opamp_finetuned.pth ---

训练流程全部完成。

--- [评估流程启动] ---
为评估加载最佳模型权重: results/two_stage_opamp_finetuned.pth
在验证集上生成预测...

--- [评估阶段] 开始计算指标 ---

=== 目标域验证集指标（物理单位）===
slewrate_pos    MSE=1.165e+14  MAE=7.987e+06  R2=0.7283
dc_gain         MSE=2.423e+07  MAE=1463  R2=0.3781
ugf             MSE=8.287e+13  MAE=5.768e+06  R2=0.8127
phase_margin    MSE=141.7  MAE=8.671  R2=0.8765
cmrr            MSE=8.299e+11  MAE=8.874e+04  R2=0.0418

Avg  (all dims)   MSE=4.005e+13  MAE=2.769e+06  R2=0.5675

==================================================
=== TRAINING PIPELINE FINISHED ===
==================================================
