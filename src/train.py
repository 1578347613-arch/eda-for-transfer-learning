# unified_train.py
import os
import torch
import argparse
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import copy
import ast
import json

# --- ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥ ---
from data_loader import get_data_and_scalers
from models.align_hetero import AlignHeteroMLP
from loss_function import heteroscedastic_nll, batch_r2, coral_loss
from evaluate import calculate_and_print_metrics
import config

# ========== 1. å‚æ•°å®šä¹‰ä¸è§£æ (é‡æ„) ==========


def setup_args():
    """è®¾ç½®å’Œè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å°† config.py ä¸­çš„è®¾ç½®ä½œä¸ºé»˜è®¤å€¼"""
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€çš„é¢„è®­ç»ƒä¸å¾®è°ƒè„šæœ¬")

    # --- æ ¸å¿ƒå‚æ•° ---
    parser.add_argument("--opamp", type=str,
                        default=config.OPAMP_TYPE, help="è¿æ”¾ç±»å‹")
    parser.add_argument("--device", type=str,
                        default=config.DEVICE, help="è®¾å¤‡ 'cuda' or 'cpu'")
    parser.add_argument("--restart", action='store_true', help="å¼ºåˆ¶é‡æ–°æ‰§è¡Œé¢„è®­ç»ƒé˜¶æ®µ")
    parser.add_argument("--save_path", type=str,
                        default="../results", help="é¢„è®­ç»ƒæ¨¡å‹å­˜æ”¾åœ°å€")
    parser.add_argument("--evaluate", action='store_true',
                        help="è®­ç»ƒç»“æŸåï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹å¹¶è¿›è¡Œè¯„ä¼°")
    parser.add_argument("--results_file", type=str, default=None,
                        help="å¦‚æœæä¾›ï¼Œåˆ™å°†æœ€ç»ˆè¯„ä¼°ç»“æœä¿å­˜åˆ°æ­¤JSONæ–‡ä»¶")

    # --- æ¨¡å‹ç»“æ„å‚æ•° ---
    parser.add_argument("--hidden_dims", type=str, default=str(config.HIDDEN_DIMS),
                        help="MLPéšè—å±‚ç»´åº¦åˆ—è¡¨, e.g., '[256, 256, 256]'")
    parser.add_argument("--dropout_rate", type=float, default=config.DROPOUT_RATE,
                        help="Dropoutæ¯”ç‡")

    # --- è®­ç»ƒè¶…å‚æ•° ---
    # <<< å°†configä¸­çš„å‚æ•°å…¨éƒ¨ç§»åˆ°è¿™é‡Œï¼Œconfigçš„å€¼ä½œä¸ºé»˜è®¤å€¼
    parser.add_argument("--lr_pretrain", type=float,
                        default=config.LEARNING_RATE_PRETRAIN, help="å­¦ä¹ ç‡")
    parser.add_argument("--lr_finetune", type=float,
                        default=config.LEARNING_RATE_FINETUNE, help="å¾®è°ƒé˜¶æ®µ head çš„å­¦ä¹ ç‡")
    parser.add_argument("--epochs_finetune", type=int,
                        default=config.EPOCHS_FINETUNE, help="å¾®è°ƒé˜¶æ®µçš„æ€»è½®æ•°")
    parser.add_argument("--batch_a", type=int,
                        default=config.BATCH_A, help="æºåŸŸ Batch Size")
    parser.add_argument("--batch_b", type=int,
                        default=config.BATCH_B, help="ç›®æ ‡åŸŸ Batch Size")
    parser.add_argument("--patience_pretrain", type=int,
                        default=config.PATIENCE_PRETRAIN, help="é¢„è®­ç»ƒæ—©åœçš„è€å¿ƒè½®æ•°")
    parser.add_argument("--patience_finetune", type=int,
                        default=config.PATIENCE_FINETUNE, help="å¾®è°ƒæ—©åœçš„è€å¿ƒè½®æ•°")

    # --- æŸå¤±å‡½æ•°æƒé‡ ---
    parser.add_argument("--lambda_nll", type=float,
                        default=config.LAMBDA_NLL, help="NLL æŸå¤±çš„æƒé‡")
    parser.add_argument("--lambda_coral", type=float,
                        default=config.LAMBDA_CORAL, help="CORAL æŸå¤±çš„æƒé‡")
    parser.add_argument("--alpha_r2", type=float,
                        default=config.ALPHA_R2, help="R2 æŸå¤±çš„æƒé‡")

    args = parser.parse_args()
    return args


def make_loader(x, y, bs, shuffle=True, drop_last=False):
    """ä¾¿æ·å‡½æ•°ï¼Œåˆ›å»º DataLoader"""
    x = torch.tensor(x, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)
    return DataLoader(TensorDataset(x, y), batch_size=bs, shuffle=shuffle, drop_last=drop_last)

# ========== 2. é˜¶æ®µä¸€ï¼šBackbone é¢„è®­ç»ƒ ==========


def run_pretraining(model, train_loader, val_loader, device, args, scheduler_config):
    """åœ¨æºåŸŸæ•°æ®ä¸Šä»…é¢„è®­ç»ƒæ¨¡å‹çš„backbone"""
    print(
        f"\n--- [å­è¿è¡Œ] ä½¿ç”¨é…ç½®: T_0={scheduler_config['T_0']}, T_mult={scheduler_config['T_mult']} ---")

    optimizer = torch.optim.AdamW(
        model.backbone.parameters(), lr=args.lr_pretrain)
    scheduler = CosineAnnealingWarmRestarts(
        optimizer, T_0=scheduler_config['T_0'], T_mult=scheduler_config['T_mult'], eta_min=1e-6)
    criterion = torch.nn.HuberLoss(delta=1)
    best_val_loss_this_run = float('inf')

    patience = args.patience_pretrain
    patience_counter = patience  # ä½¿ç”¨ä¸€ä¸ªè®¡æ•°å™¨

    best_state_dict_this_run = None  # <<< åœ¨å†…å­˜ä¸­ä¿å­˜æœ€ä½³æƒé‡

    T_0 = scheduler_config['T_0']
    T_mult = scheduler_config['T_mult']
    current_T = T_0
    # ä½¿ç”¨åˆ—è¡¨å­˜å‚¨ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æœ€åä¸€ä¸ªå…ƒç´ æ¥è®¡ç®—ä¸‹ä¸€ä¸ªé‡å¯ç‚¹
    restart_epochs_list = [current_T]
    max_cycles = int(np.log(scheduler_config['epochs_pretrain'] / T_0) / np.log(T_mult)
                     ) + 2 if T_mult > 1 else scheduler_config['epochs_pretrain'] // T_0
    for _ in range(max_cycles):
        current_T *= T_mult
        next_restart = restart_epochs_list[-1] + current_T
        if next_restart <= scheduler_config['epochs_pretrain']:
            restart_epochs_list.append(next_restart)
        else:
            break

    # è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿åœ¨å¾ªç¯ä¸­å¿«é€ŸæŸ¥æ‰¾
    restart_epochs = set(restart_epochs_list)
    print(f"ä¼˜åŒ–å™¨å°†åœ¨ä»¥ä¸‹ epoch ç»“æŸåé‡ç½®: {sorted(restart_epochs_list)}")

    for epoch in range(scheduler_config['epochs_pretrain']):
        # ... (å¾ªç¯å†…éƒ¨çš„ä»£ç ä¿æŒä¸å˜)
        model.train()
        total_train_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            mu, _, _ = model(inputs)
            loss = criterion(mu, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                mu, _, _ = model(inputs)
                loss = criterion(mu, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        scheduler.step()

        current_lr = optimizer.param_groups[0]['lr']
        print(
            f"Pre-train Epoch [{epoch+1}/{scheduler_config['epochs_pretrain']}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {current_lr:.2e}")

        if avg_val_loss < best_val_loss_this_run:
            best_val_loss_this_run = avg_val_loss
            best_state_dict_this_run = copy.deepcopy(model.state_dict())
            print(f"    - æ–°çš„æœ¬æ¬¡è¿è¡Œæœ€ä½³æŸå¤±: {best_val_loss_this_run:.6f}")

        if (epoch + 1) in restart_epochs and (epoch + 1) < scheduler_config['epochs_pretrain']:
            print(f"--- Epoch {epoch+1} æ˜¯ä¸€ä¸ªé‡å¯ç‚¹ã€‚é‡ç½® AdamW ä¼˜åŒ–å™¨çŠ¶æ€ï¼ ---")
            optimizer = torch.optim.AdamW(
                model.backbone.parameters(), lr=args.lr_pretrain)
            scheduler.optimizer = optimizer

    return best_val_loss_this_run, best_state_dict_this_run


# ========== 3. é˜¶æ®µäºŒï¼šæ•´ä½“æ¨¡å‹å¾®è°ƒ (é‡æ„) ==========

def run_finetuning(model, data_loaders, device, final_save_path, args):
    """ä½¿ç”¨å¤åˆæŸå¤±å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒ"""
    print("\n--- [é˜¶æ®µäºŒ] å¼€å§‹æ•´ä½“æ¨¡å‹å¾®è°ƒ ---")

    # ä¸º backbone å’Œ head è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    optimizer_params = [
        {
            "params": model.backbone.parameters(),
            "lr": args.lr_finetune / 10  # ä¸º backbone è®¾ç½®ä¸€ä¸ªéå¸¸ä½çš„å­¦ä¹ ç‡
        },
        {
            "params": model.hetero_head.parameters(),
            "lr": args.lr_finetune  # ä¸ºæ–° head è®¾ç½®ä¸€ä¸ªç›¸å¯¹è¾ƒé«˜çš„å­¦ä¹ ç‡
        }
    ]

    opt = torch.optim.AdamW(optimizer_params, weight_decay=1e-4)

    dl_A, dl_B, dl_val = data_loaders['source_full'], data_loaders['target_train'], data_loaders['target_val']
    dl_A_iter = iter(dl_A)

    best_val = float('inf')

    patience = args.patience_finetune
    patience_counter = patience  # ä½¿ç”¨ä¸€ä¸ªè®¡æ•°å™¨

    temp_save_path = final_save_path + ".tmp"

    for epoch in range(args.epochs_finetune):
        model.train()
        total_train_loss = 0.0
        for xb_B, yb_B in dl_B:
            xb_B, yb_B = xb_B.to(device), yb_B.to(device)
            try:
                xa_A, _ = next(dl_A_iter)
            except StopIteration:
                dl_A_iter = iter(dl_A)
                xa_A, _ = next(dl_A_iter)
            if xa_A.size(0) != xb_B.size(0):
                xa_A = xa_A[:xb_B.size(0)]
            xa_A = xa_A.to(device)

            mu_B, logvar_B, feat_B = model(xb_B)
            with torch.no_grad():
                _, _, feat_A = model(xa_A)

            nll = heteroscedastic_nll(mu_B, logvar_B, yb_B)
            r2_loss = (
                1.0 - batch_r2(yb_B, mu_B).clamp(min=-1.0, max=1.0)).mean()
            coral = coral_loss(feat_A, feat_B)
            loss = args.lambda_nll * nll + args.alpha_r2 * \
                r2_loss + args.lambda_coral * coral
            total_train_loss += loss.item()

            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

        avg_train_loss = total_train_loss / len(dl_B)

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for xb, yb in dl_val:
                xb, yb = xb.to(device), yb.to(device)
                mu, logvar, _ = model(xb)
                val_loss += heteroscedastic_nll(mu, logvar, yb).item()
        val_loss /= len(dl_val)

        print(
            f"Fine-tune Epoch [{epoch+1}/{args.epochs_finetune}], Train Loss: {avg_train_loss:.4f}, Val NLL: {val_loss:.4f}")

        if val_loss < best_val:
            print(f"  - å¾®è°ƒéªŒè¯æŸå¤±æ”¹å–„ ({best_val:.4f} -> {val_loss:.4f})ã€‚ä¿å­˜æ¨¡å‹...")
            best_val = val_loss
            torch.save(model.state_dict(), temp_save_path)
            os.replace(temp_save_path, final_save_path)
            patience_counter = patience  # é‡ç½®è®¡æ•°å™¨
        else:
            patience_counter -= 1
            if patience_counter == 0:
                print(f"éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„ï¼Œè§¦å‘æ—©åœã€‚")
                break

    print(f"--- [é˜¶æ®µäºŒ] å¾®è°ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {final_save_path} ---")


# ========== 4.ï¼ˆå¯é€‰ï¼‰è·å–æœ€å¥½æ¨¡å‹çš„è¾“å‡ºç»“æœ ==========
def get_predictions(model, dataloader, device):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›Numpyæ•°ç»„ã€‚
    """
    model.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            # é€‚é… AlignHeteroMLP çš„è¾“å‡ºï¼Œåªå– mu
            preds, _, _ = model(inputs)

            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.numpy())  # labels æœ¬èº«å°±åœ¨ CPU ä¸Š

    return np.concatenate(all_preds), np.concatenate(all_labels)

# ========== 5. ä¸»å‡½æ•° (å·²é‡æ„ä¸ºâ€œå…ˆé€‰æ‹”ï¼Œåç²¾è°ƒâ€æ¨¡å¼) ==========


def main():
    args = setup_args()
    DEVICE = torch.device(args.device)
    print(f"ç¨‹åºåœ¨ï¼š{DEVICE}  ä¸Šè¿è¡Œ")
    os.makedirs(args.save_path, exist_ok=True)

    # --- è·¯å¾„å®šä¹‰ ---
    pretrained_path = os.path.join(
        args.save_path, f'{args.opamp}_pretrained.pth')
    finetuned_path = os.path.join(
        args.save_path, f'{args.opamp}_finetuned.pth')

    try:
        hidden_dims_list = ast.literal_eval(args.hidden_dims)
        if not isinstance(hidden_dims_list, list):
            raise ValueError
    except (ValueError, SyntaxError):
        print(f"é”™è¯¯: --hidden_dims å‚æ•°æ ¼å¼ä¸æ­£ç¡®: {args.hidden_dims}")
        return

    # --- æ•°æ®å‡†å¤‡ ---
    data = get_data_and_scalers(opamp_type=args.opamp)
    input_dim = data['source'][0].shape[1]
    output_dim = data['source'][1].shape[1]

    # --- æ£€æŸ¥æ˜¯å¦å¯ä»¥å®Œå…¨è·³è¿‡ ---
    if os.path.exists(finetuned_path) and not args.restart:
        print(f"æ£€æµ‹åˆ°æœ€ç»ˆæ¨¡å‹ {finetuned_path} ä¸”æœªæŒ‡å®š --restartã€‚è·³è¿‡æ‰€æœ‰è®­ç»ƒã€‚")
    else:
        # --- é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒå…ƒä¼˜åŒ– ---
        global_best_pretrain_val_loss = float('inf')

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰§è¡Œé¢„è®­ç»ƒ
        if args.restart or not os.path.exists(pretrained_path):
            print(f"\n{'='*30} é˜¶æ®µä¸€: é¢„è®­ç»ƒå…ƒä¼˜åŒ– {'='*30}")

            pretrain_loader_A = make_loader(
                data['source_train'][0], data['source_train'][1], args.batch_a, shuffle=True)
            pretrain_loader_val = make_loader(
                data['source_val'][0], data['source_val'][1], args.batch_a, shuffle=False)

            num_experiments = config.RESTART_PRETRAIN
            for i in range(num_experiments):
                print(f"\n--- [é¢„è®­ç»ƒå®éªŒ {i+1}/{num_experiments}] ---")

                model = AlignHeteroMLP(
                    input_dim=input_dim, output_dim=output_dim,
                    hidden_dims=hidden_dims_list, dropout_rate=args.dropout_rate
                ).to(DEVICE)

                scheduler_config = config.PRETRAIN_SCHEDULER_CONFIGS[i % len(
                    config.PRETRAIN_SCHEDULER_CONFIGS)]

                best_loss_this_run, best_state_dict_this_run = run_pretraining(
                    model, pretrain_loader_A, pretrain_loader_val, DEVICE, args, scheduler_config)

                if best_state_dict_this_run and best_loss_this_run < global_best_pretrain_val_loss:
                    global_best_pretrain_val_loss = best_loss_this_run
                    print(
                        f"  ğŸ† æ–°çš„å…¨å±€æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹ï¼æºåŸŸéªŒè¯æŸå¤±: {global_best_pretrain_val_loss:.6f}ã€‚ä¿å­˜ä¸­...")
                    torch.save(best_state_dict_this_run, pretrained_path)

                del model, best_state_dict_this_run
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

            print(
                f"\n--- [é˜¶æ®µä¸€å®Œæˆ] æ‰€æœ‰é¢„è®­ç»ƒå®éªŒç»“æŸã€‚æœ€ä½³æºåŸŸéªŒè¯æŸå¤±: {global_best_pretrain_val_loss:.6f} ---")
        else:
            print(f"--- [é˜¶æ®µä¸€] æ£€æµ‹åˆ°å·²å­˜åœ¨çš„é¢„è®­ç»ƒæ¨¡å‹ {pretrained_path}ï¼Œè·³è¿‡å…ƒä¼˜åŒ–ã€‚---")

        # --- é˜¶æ®µäºŒï¼šå¯¹å”¯ä¸€çš„æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ ---
        if not os.path.exists(pretrained_path):
            print("\n[é”™è¯¯] é¢„è®­ç»ƒé˜¶æ®µæœªèƒ½äº§ç”Ÿä»»ä½•æœ‰æ•ˆæ¨¡å‹ã€‚æ— æ³•è¿›è¡Œå¾®è°ƒã€‚")
        else:
            print(f"\n{'='*30} é˜¶æ®µäºŒ: å¯¹æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒ {'='*30}")
            model = AlignHeteroMLP(
                input_dim=input_dim, output_dim=output_dim,
                hidden_dims=hidden_dims_list, dropout_rate=args.dropout_rate
            ).to(DEVICE)

            print(f"åŠ è½½æœ€ä½³é¢„è®­ç»ƒæƒé‡ä»: {pretrained_path}")
            model.load_state_dict(torch.load(
                pretrained_path, map_location=DEVICE))

            finetune_loaders = {
                'source_full': make_loader(data['source'][0], data['source'][1], args.batch_a, shuffle=True, drop_last=True),
                'target_train': make_loader(data['target_train'][0], data['target_train'][1], args.batch_b, shuffle=True),
                'target_val': make_loader(data['target_val'][0], data['target_val'][1], args.batch_b, shuffle=False)
            }

            run_finetuning(model, finetune_loaders,
                           DEVICE, finetuned_path, args)

    # --- (å¯é€‰) æœ€ç»ˆè¯„ä¼° ---
    if args.evaluate:
        print("\n--- [æœ€ç»ˆè¯„ä¼°æµç¨‹å¯åŠ¨] ---")
        if not os.path.exists(finetuned_path):
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æœ€ç»ˆå¾®è°ƒæ¨¡å‹ {finetuned_path}ã€‚æ— æ³•è¯„ä¼°ã€‚")
            return

        model = AlignHeteroMLP(
            input_dim=input_dim, output_dim=output_dim,
            hidden_dims=hidden_dims_list, dropout_rate=args.dropout_rate
        ).to(DEVICE)

        print(f"åŠ è½½æœ€ç»ˆæ¨¡å‹ {finetuned_path} è¿›è¡Œè¯„ä¼°...")
        model.load_state_dict(torch.load(finetuned_path, map_location=DEVICE))

        # å‡†å¤‡è¯„ä¼°æ‰€éœ€çš„æ•°æ®åŠ è½½å™¨
        eval_loader = make_loader(
            data['target_val'][0], data['target_val'][1], args.batch_b, shuffle=False)

        pred_scaled, true_scaled = get_predictions(model, eval_loader, DEVICE)
        eval_metrics = calculate_and_print_metrics(
            pred_scaled, true_scaled, data['y_scaler'])

        final_results = {
            'opamp': args.opamp,
            'evaluation_metrics_on_target_validation': eval_metrics
        }

        if args.results_file:
            try:
                # ä½¿ç”¨ 'a' æ¨¡å¼è¿½åŠ ç»“æœ
                with open(args.results_file, 'a', encoding='utf-8') as f:
                    f.write(json.dumps(final_results, indent=4) + "\n")
                print(f"è¯„ä¼°ç»“æœå·²æˆåŠŸè¿½åŠ è‡³: {args.results_file}")
            except Exception as e:
                print(f"é”™è¯¯: ä¿å­˜ç»“æœæ–‡ä»¶å¤±è´¥ - {e}")


if __name__ == "__main__":
    main()
