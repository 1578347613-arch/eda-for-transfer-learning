# unified_train.py
import os
import torch
import argparse
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import copy

# --- ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥ ---
from data_loader import get_data_and_scalers
from models.align_hetero import AlignHeteroMLP
from loss_function import heteroscedastic_nll, batch_r2, coral_loss
from evaluate import calculate_and_print_metrics
# [ä¿®æ­£] å¯¼å…¥æ–°çš„ config ç»“æ„
from config import COMMON_CONFIG, TASK_CONFIGS

# ========== 1. å‚æ•°å®šä¹‰ä¸è§£æ (é‡æ„) ==========
# "é»„é‡‘æ ‡å‡†" setup_args å‡½æ•°
# è¯·åœ¨ unified_train.py å’Œ unified_inverse_train.py ä¸­ä½¿ç”¨å®ƒ


def setup_args():
    """
    ä¸€ä¸ªå¥å£®çš„å‚æ•°è§£æå™¨ï¼Œèƒ½åŠ¨æ€åœ°ä» config æ–‡ä»¶åŠ è½½é»˜è®¤å€¼ï¼Œå¹¶å…è®¸å‘½ä»¤è¡Œè¦†ç›–ã€‚
    """
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€çš„å¤šä»»åŠ¡è®­ç»ƒè„šæœ¬")
    parser.add_argument("--opamp", type=str, required=True,
                        choices=TASK_CONFIGS.keys(), help="å¿…é¡»æŒ‡å®šçš„ç”µè·¯ç±»å‹")

    # --- Step 1: å…ˆè§£æå‡º opamp ç±»å‹ï¼Œä»¥ä¾¿åŠ è½½æ­£ç¡®çš„é»˜è®¤å€¼ ---
    temp_args, _ = parser.parse_known_args()
    opamp_type = temp_args.opamp

    # --- Step 2: åˆå¹¶é€šç”¨é…ç½®å’Œç‰¹å®šä»»åŠ¡çš„é…ç½®ä½œä¸ºé»˜è®¤å€¼ ---
    defaults = {**COMMON_CONFIG, **TASK_CONFIGS.get(opamp_type, {})}

    # --- Step 3: åŠ¨æ€ä¸ºæ‰€æœ‰ç®€å•ç±»å‹çš„é»˜è®¤å‚æ•°åˆ›å»ºå‘½ä»¤è¡Œå¼€å…³ ---
    for key, value in defaults.items():
        # è·³è¿‡å¤æ‚ç±»å‹ï¼Œè¿™äº›ç±»å‹åº”åœ¨ config æ–‡ä»¶ä¸­å®šä¹‰ï¼Œä¸é€‚åˆå‘½ä»¤è¡Œä¿®æ”¹
        if isinstance(value, (list, dict)):
            continue

        if isinstance(value, bool):
            if value is False:  # e.g., restart = False
                parser.add_argument(
                    f"--{key}", action="store_true", help=f"å¯ç”¨ '{key}' (é»˜è®¤: å…³é—­)")
            else:  # e.g., evaluate = True
                parser.add_argument(
                    f"--no-{key}", action="store_false", dest=key, help=f"ç¦ç”¨ '{key}' (é»˜è®¤: å¼€å¯)")
        else:
            parser.add_argument(
                f"--{key}", type=type(value), help=f"è®¾ç½® '{key}' (é»˜è®¤: {value})")

    # --- Step 4: å°†åˆå¹¶åçš„é…ç½®è®¾ç½®ä¸ºè§£æå™¨çš„é»˜è®¤å€¼ ---
    parser.set_defaults(**defaults)

    # --- Step 5: è¿›è¡Œæœ€ç»ˆè§£æ ---
    # å‘½ä»¤è¡Œä¸­æä¾›çš„å€¼å°†è¦†ç›–æ‰€æœ‰æ¥è‡ª config æ–‡ä»¶çš„é»˜è®¤å€¼
    args = parser.parse_args()

    return args


def make_loader(x, y, bs, shuffle=True, drop_last=False):
    """ä¾¿æ·å‡½æ•°ï¼Œåˆ›å»º DataLoader"""
    x = torch.tensor(x, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)
    return DataLoader(TensorDataset(x, y), batch_size=bs, shuffle=shuffle, drop_last=drop_last)

# ========== 2. é˜¶æ®µä¸€ï¼šBackbone é¢„è®­ç»ƒ ==========
# (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)


def run_pretraining(model, train_loader, val_loader, device, args, scheduler_config):
    """åœ¨æºåŸŸæ•°æ®ä¸Šä»…é¢„è®­ç»ƒæ¨¡å‹çš„backbone"""
    print(
        f"\n--- [å­è¿è¡Œ] ä½¿ç”¨é…ç½®: T_0={scheduler_config['T_0']}, T_mult={scheduler_config['T_mult']}, Epochs={scheduler_config['epochs_pretrain']} ---")

    optimizer = torch.optim.AdamW(
        model.backbone.parameters(), lr=args.lr_pretrain)
    scheduler = CosineAnnealingWarmRestarts(
        optimizer, T_0=scheduler_config['T_0'], T_mult=scheduler_config['T_mult'], eta_min=1e-6)
    criterion = torch.nn.HuberLoss(delta=1)
    best_val_loss_this_run = float('inf')

    best_state_dict_this_run = None

    T_0 = scheduler_config['T_0']
    T_mult = scheduler_config['T_mult']
    current_T = T_0

    restart_epochs_list = [current_T]
    max_epochs_for_calc = scheduler_config['epochs_pretrain']

    if T_mult > 1:
        while restart_epochs_list[-1] < max_epochs_for_calc:
            current_T *= T_mult
            next_restart = restart_epochs_list[-1] + current_T
            if next_restart <= max_epochs_for_calc:
                restart_epochs_list.append(next_restart)
            else:
                break
    else:  # T_mult == 1
        restart_epochs_list = list(
            range(current_T, max_epochs_for_calc + 1, current_T))

    restart_epochs = set(restart_epochs_list)
    print(f"ä¼˜åŒ–å™¨å°†åœ¨ä»¥ä¸‹ epoch ç»“æŸåé‡ç½®: {sorted(list(restart_epochs))}")

    for epoch in range(scheduler_config['epochs_pretrain']):
        model.train()
        total_train_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            mu, _, _ = model(inputs)
            loss = criterion(mu, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                mu, _, _ = model(inputs)
                loss = criterion(mu, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        scheduler.step()

        current_lr = optimizer.param_groups[0]['lr']
        print(
            f"Pre-train Epoch [{epoch+1}/{scheduler_config['epochs_pretrain']}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {current_lr:.2e}")

        if avg_val_loss < best_val_loss_this_run:
            best_val_loss_this_run = avg_val_loss
            best_state_dict_this_run = copy.deepcopy(model.state_dict())
            print(f"    - æ–°çš„æœ¬æ¬¡è¿è¡Œæœ€ä½³æŸå¤±: {best_val_loss_this_run:.6f}")

        if (epoch + 1) in restart_epochs and (epoch + 1) < scheduler_config['epochs_pretrain']:
            print(f"--- Epoch {epoch+1} æ˜¯ä¸€ä¸ªé‡å¯ç‚¹ã€‚é‡ç½® AdamW ä¼˜åŒ–å™¨çŠ¶æ€ï¼ ---")
            optimizer = torch.optim.AdamW(
                model.backbone.parameters(), lr=args.lr_pretrain)
            scheduler.optimizer = optimizer

    return best_val_loss_this_run, best_state_dict_this_run


# ========== 3. é˜¶æ®µäºŒï¼šæ•´ä½“æ¨¡å‹å¾®è°ƒ (é‡æ„) ==========
# (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)
def run_finetuning(model, data_loaders, device, final_save_path, args):
    """ä½¿ç”¨å¤åˆæŸå¤±å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒ"""
    print("\n--- [é˜¶æ®µäºŒ] å¼€å§‹æ•´ä½“æ¨¡å‹å¾®è°ƒ ---")

    optimizer_params = [
        {"params": model.backbone.parameters(), "lr": args.lr_finetune / 10},
        {"params": model.hetero_head.parameters(), "lr": args.lr_finetune}
    ]

    opt = torch.optim.AdamW(optimizer_params, weight_decay=1e-4)

    dl_A, dl_B, dl_val = data_loaders['source'], data_loaders['target_train'], data_loaders['target_val']
    dl_A_iter = iter(dl_A)

    best_val = float('inf')
    patience_counter = args.patience_finetune

    for epoch in range(args.epochs_finetune):
        model.train()
        total_train_loss = 0.0
        for xb_B, yb_B in dl_B:
            xb_B, yb_B = xb_B.to(device), yb_B.to(device)
            try:
                xa_A, _ = next(dl_A_iter)
            except StopIteration:
                dl_A_iter = iter(dl_A)
                xa_A, _ = next(dl_A_iter)
            if xa_A.size(0) != xb_B.size(0):
                xa_A = xa_A[:xb_B.size(0)]
            xa_A = xa_A.to(device)

            mu_B, logvar_B, feat_B = model(xb_B)
            with torch.no_grad():
                _, _, feat_A = model(xa_A)

            nll = heteroscedastic_nll(mu_B, logvar_B, yb_B)
            r2_loss = (
                1.0 - batch_r2(yb_B, mu_B).clamp(min=-1.0, max=1.0)).mean()
            coral = coral_loss(feat_A, feat_B)
            loss = args.lambda_nll * nll + args.alpha_r2 * \
                r2_loss + args.lambda_coral * coral
            total_train_loss += loss.item()

            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

        avg_train_loss = total_train_loss / len(dl_B)

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for xb, yb in dl_val:
                xb, yb = xb.to(device), yb.to(device)
                mu, logvar, _ = model(xb)
                val_loss += heteroscedastic_nll(mu, logvar, yb).item()
        val_loss /= len(dl_val)

        print(
            f"Fine-tune Epoch [{epoch+1}/{args.epochs_finetune}], Train Loss: {avg_train_loss:.4f}, Val NLL: {val_loss:.4f}")

        if val_loss < best_val:
            print(f"  - å¾®è°ƒéªŒè¯æŸå¤±æ”¹å–„ ({best_val:.4f} -> {val_loss:.4f})ã€‚ä¿å­˜æ¨¡å‹...")
            best_val = val_loss
            torch.save(model.state_dict(), final_save_path)
            patience_counter = args.patience_finetune
        else:
            patience_counter -= 1
            if patience_counter == 0:
                print(f"éªŒè¯æŸå¤±è¿ç»­ {args.patience_finetune} è½®æœªæ”¹å–„ï¼Œè§¦å‘æ—©åœã€‚")
                break

    print(f"--- [é˜¶æ®µäºŒ] å¾®è°ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {final_save_path} ---")


# ========== 4.ï¼ˆå¯é€‰ï¼‰è·å–æœ€å¥½æ¨¡å‹çš„è¾“å‡ºç»“æœ ==========
# (æ­¤å‡½æ•°æ— éœ€ä¿®æ”¹)
def get_predictions(model, dataloader, device):
    model.eval()
    all_preds, all_labels = [], []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            preds, _, _ = model(inputs)
            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.numpy())
    return np.concatenate(all_preds), np.concatenate(all_labels)


# ========== 5. ä¸»å‡½æ•° ==========
def main():
    args = setup_args()
    DEVICE = torch.device(args.device)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    os.makedirs(args.save_path, exist_ok=True)

    print(f"--- ä»»åŠ¡å¯åŠ¨: {args.opamp} | è®¾å¤‡: {DEVICE} ---")

    pretrained_path = os.path.join(
        args.save_path, f'{args.opamp}_pretrained.pth')
    finetuned_path = os.path.join(
        args.save_path, f'{args.opamp}_finetuned.pth')

    data = get_data_and_scalers(opamp_type=args.opamp)
    input_dim, output_dim = data['source'][0].shape[1], data['source'][1].shape[1]

    global_best_val_loss = float('inf')

    # === é˜¶æ®µä¸€ï¼šé¢„è®­ç»ƒ ===
    if args.restart or not os.path.exists(pretrained_path):
        pretrain_loader_A = make_loader(
            data['source_train'][0], data['source_train'][1], args.batch_a, shuffle=True)
        pretrain_loader_val = make_loader(
            data['source_val'][0], data['source_val'][1], args.batch_a, shuffle=False)

        # <<< [æ ¸å¿ƒä¿®æ­£] æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤šé˜¶æ®µé…ç½® >>>
        if hasattr(args, 'pretrain_scheduler_configs') and args.pretrain_scheduler_configs:
            # --- é€»è¾‘ Aï¼šå¤„ç†å¤šé˜¶æ®µã€ç‹¬ç«‹é‡å¯çš„é¢„è®­ç»ƒ (for 5t_opamp) ---
            print(
                f"--- [é˜¶æ®µä¸€] å¯åŠ¨å¤šé˜¶æ®µé¢„è®­ç»ƒæµç¨‹ ({len(args.pretrain_scheduler_configs)} æ¬¡ç‹¬ç«‹å®éªŒ) ---")

            for i, scheduler_config in enumerate(args.pretrain_scheduler_configs):
                print(
                    f"\n{'='*30} ç‹¬ç«‹å®éªŒ {i+1}/{len(args.pretrain_scheduler_configs)} {'='*30}")

                model = AlignHeteroMLP(
                    input_dim=input_dim, output_dim=output_dim,
                    hidden_dim=args.hidden_dim, num_layers=args.num_layers,
                    dropout_rate=args.dropout_rate
                ).to(DEVICE)

                best_loss_this_run, best_state_dict_this_run = run_pretraining(
                    model, pretrain_loader_A, pretrain_loader_val, DEVICE, args, scheduler_config)
                print(
                    f"--- ç‹¬ç«‹å®éªŒ {i+1} å®Œæˆï¼Œæœ¬æ¬¡æœ€ä½³æŸå¤±: {best_loss_this_run:.6f} ---")

                if best_state_dict_this_run and best_loss_this_run < global_best_val_loss:
                    global_best_val_loss = best_loss_this_run
                    print(
                        f"  ğŸ†ğŸ†ğŸ† æ–°çš„å…¨å±€æœ€ä½³æŸå¤±ï¼ {global_best_val_loss:.6f}ã€‚æ­£åœ¨è¦†ç›– {pretrained_path}...")
                    torch.save(best_state_dict_this_run, pretrained_path)

            print(
                f"\næ‰€æœ‰ç‹¬ç«‹å®éªŒå®Œæˆï¼Œå…¨å±€æœ€ä½³æ¨¡å‹ (æŸå¤±: {global_best_val_loss:.6f}) å·²ä¿å­˜åœ¨ {pretrained_path}")

            del model
            del best_state_dict_this_run
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        else:
            # --- é€»è¾‘ Bï¼šå¤„ç†å•æ¬¡ã€æ ‡å‡†çš„é¢„è®­ç»ƒ (for two_stage_opamp) ---
            print(f"--- [é˜¶æ®µä¸€] å¯åŠ¨æ ‡å‡†å•æ¬¡é¢„è®­ç»ƒæµç¨‹ ---")

            model = AlignHeteroMLP(
                input_dim=input_dim, output_dim=output_dim,
                hidden_dim=args.hidden_dim, num_layers=args.num_layers,
                dropout_rate=args.dropout_rate
            ).to(DEVICE)

            # æ‰‹åŠ¨æ„å»ºä¸€ä¸ª scheduler_config å­—å…¸
            scheduler_config = {
                # å¦‚æœæ²¡æœ‰T_0ï¼Œç»™ä¸€ä¸ªé»˜è®¤å€¼
                'T_0': getattr(args, 'T_0', args.epochs_pretrain // 5),
                'T_mult': getattr(args, 'T_mult', 1),
                'epochs_pretrain': args.epochs_pretrain
            }

            _, best_state_dict_this_run = run_pretraining(
                model, pretrain_loader_A, pretrain_loader_val, DEVICE, args, scheduler_config)

            if best_state_dict_this_run:
                print(f"æ ‡å‡†é¢„è®­ç»ƒå®Œæˆï¼Œä¿å­˜æ¨¡å‹åˆ° {pretrained_path}")
                torch.save(best_state_dict_this_run, pretrained_path)
            else:
                print("é”™è¯¯ï¼šæ ‡å‡†é¢„è®­ç»ƒæœªèƒ½äº§å‡ºæœ‰æ•ˆæ¨¡å‹ã€‚")

    # === åŠ è½½æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹ ===
    print(f"--- [é˜¶æ®µä¸€å®Œæˆ] åŠ è½½æœ€ç»ˆçš„æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹: {pretrained_path} ---")
    model = AlignHeteroMLP(
        input_dim=input_dim, output_dim=output_dim,
        hidden_dim=args.hidden_dim, num_layers=args.num_layers,
        dropout_rate=args.dropout_rate
    ).to(DEVICE)
    # ç¡®ä¿æ–‡ä»¶å­˜åœ¨æ‰èƒ½åŠ è½½
    if os.path.exists(pretrained_path):
        model.load_state_dict(torch.load(pretrained_path, map_location=DEVICE))
    else:
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ {pretrained_path}ã€‚å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚")

    finetune_loaders = {
        'source': make_loader(data['source'][0], data['source'][1], args.batch_a, shuffle=True, drop_last=True),
        'target_train': make_loader(data['target_train'][0], data['target_train'][1], args.batch_b, shuffle=True),
        'target_val': make_loader(data['target_val'][0], data['target_val'][1], args.batch_b, shuffle=False)
    }

    if os.path.exists(finetuned_path) and not args.restart:
        print(f"--- [é˜¶æ®µäºŒ] æ£€æµ‹åˆ°å·²æœ‰å¾®è°ƒæ¨¡å‹: {finetuned_path}ï¼Œè·³è¿‡å¾®è°ƒå¹¶ç›´æ¥è½½å…¥è¯¥æƒé‡ ---")
        model.load_state_dict(torch.load(finetuned_path, map_location=DEVICE))
    else:
        run_finetuning(model, finetune_loaders, DEVICE, finetuned_path, args)

    print("\nè®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆã€‚")

    if args.evaluate:
        print("\n--- [è¯„ä¼°æµç¨‹å¯åŠ¨] ---")
        if not os.path.exists(finetuned_path):
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶ {finetuned_path}ã€‚è·³è¿‡è¯„ä¼°ã€‚")
            return

        print(f"ä¸ºè¯„ä¼°åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {finetuned_path}")
        # æ¨¡å‹ç»“æ„å·²æ­£ç¡®åŠ è½½ï¼Œåªéœ€åŠ è½½æƒé‡
        model.load_state_dict(torch.load(finetuned_path, map_location=DEVICE))

        print("åœ¨éªŒè¯é›†ä¸Šç”Ÿæˆé¢„æµ‹...")
        pred_scaled, true_scaled = get_predictions(
            model, finetune_loaders['target_val'], DEVICE)
        calculate_and_print_metrics(pred_scaled, true_scaled, data['y_scaler'])


if __name__ == "__main__":
    main()

                