# unified_train.py
import os
import torch
import argparse
import numpy as np
from torch.utils.data import DataLoader, TensorDataset
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

# --- ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥ ---
from data_loader import get_data_and_scalers
from models.align_hetero import AlignHeteroMLP
from loss_function import heteroscedastic_nll, batch_r2, coral_loss
from evaluate import calculate_and_print_metrics
import config
from config import COMMON_CONFIG, TASK_CONFIGS

# ========== 1. å‚æ•°å®šä¹‰ä¸è§£æ (é‡æ„) ==========
# "é»„é‡‘æ ‡å‡†" setup_args å‡½æ•°
# è¯·åœ¨ unified_train.py å’Œ unified_inverse_train.py ä¸­ä½¿ç”¨å®ƒ


def setup_args():
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€è®­ç»ƒè„šæœ¬")
    parser.add_argument("--opamp", type=str, required=True,
                        choices=TASK_CONFIGS.keys(), help="ç”µè·¯ç±»å‹")

    # --- 1. è‡ªåŠ¨æ·»åŠ æ‰€æœ‰å¯èƒ½çš„å‚æ•°å®šä¹‰ ---
    # a. ä» COMMON_CONFIG æ·»åŠ 
    for key, value in COMMON_CONFIG.items():
        if isinstance(value, bool):
            if value is False:
                parser.add_argument(
                    f"--{key}", action="store_true", help=f"å¯ç”¨ '{key}' (å¼€å…³)")
            else:
                parser.add_argument(
                    f"--no-{key}", action="store_false", dest=key, help=f"ç¦ç”¨ '{key}' (å¼€å…³)")
        else:
            parser.add_argument(
                f"--{key}", type=type(value), help=f"è®¾ç½® '{key}'")

    # b. ä» TASK_CONFIGS æ·»åŠ ä¸“å±å‚æ•°
    all_task_keys = set().union(*(d.keys() for d in TASK_CONFIGS.values()))
    task_only_keys = all_task_keys - set(COMMON_CONFIG.keys())

    for key in sorted(list(task_only_keys)):
        # ç®€å•çš„ç±»å‹æ¨æ–­
        sample_val = TASK_CONFIGS[next(iter(TASK_CONFIGS))][key]
        parser.add_argument(
            f"--{key}", type=type(sample_val), help=f"ä»»åŠ¡å‚æ•°: {key}")

    # --- 2. åº”ç”¨é»˜è®¤å€¼å¹¶æœ€ç»ˆè§£æ ---
    # a. å…ˆåº”ç”¨é€šç”¨é»˜è®¤å€¼
    parser.set_defaults(**COMMON_CONFIG)

    # b. è§£æä¸€æ¬¡ï¼Œæ‹¿åˆ° opamp ç±»å‹ï¼Œå†åº”ç”¨ä»»åŠ¡ä¸“å±é»˜è®¤å€¼
    # parse_known_args ä¸ä¼šå› ä¸ºä¸å®Œæ•´çš„å‘½ä»¤è¡Œè€ŒæŠ¥é”™
    temp_args, _ = parser.parse_known_args()
    if temp_args.opamp in TASK_CONFIGS:
        parser.set_defaults(**TASK_CONFIGS[temp_args.opamp])

    # c. æœ€åé‡æ–°è§£æï¼Œå‘½ä»¤è¡Œæä¾›çš„å€¼ä¼šè¦†ç›–æ‰€æœ‰é»˜è®¤å€¼
    args = parser.parse_args()

    return args


def make_loader(x, y, bs, shuffle=True, drop_last=False):
    """ä¾¿æ·å‡½æ•°ï¼Œåˆ›å»º DataLoader"""
    x = torch.tensor(x, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.float32)
    return DataLoader(TensorDataset(x, y), batch_size=bs, shuffle=shuffle, drop_last=drop_last)

# ========== 2. é˜¶æ®µä¸€ï¼šBackbone é¢„è®­ç»ƒ (å·²ä¿®æ”¹) ==========


def run_pretraining(model, train_loader, val_loader, device, args, scheduler_config):
    """
    åœ¨æºåŸŸæ•°æ®ä¸Šä»…é¢„è®­ç»ƒæ¨¡å‹çš„backboneã€‚
    æ­¤å‡½æ•°ç°åœ¨æ¥å— scheduler_config å¹¶è¿”å›æœ€ä½³æŸå¤±å’Œæ¨¡å‹çŠ¶æ€ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¿å­˜ã€‚
    """
    print(f"\n--- [é˜¶æ®µä¸€] å¼€å§‹ Backbone é¢„è®­ç»ƒ (é…ç½®: {scheduler_config}) ---")

    # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼
    epochs = scheduler_config.get("epochs_pretrain", args.epochs_pretrain)
    T_0 = scheduler_config.get("T_0", 200)  # åŸå§‹ç¡¬ç¼–ç å€¼ä¸º 200
    T_mult = scheduler_config.get("T_mult", 1)  # åŸå§‹ç¡¬ç¼–ç å€¼ä¸º 1

    optimizer = torch.optim.AdamW(
        model.backbone.parameters(), lr=args.lr_pretrain)
    scheduler = CosineAnnealingWarmRestarts(
        optimizer, T_0=T_0, T_mult=T_mult, eta_min=1e-6)

    criterion = torch.nn.HuberLoss(delta=1)
    best_val_loss = float('inf')
    best_model_state_dict = None  # åœ¨å†…å­˜ä¸­ä¿å­˜æœ€ä½³æ¨¡å‹

    patience = args.patience_pretrain
    patience_counter = patience

    # --- åŠ¨æ€è®¡ç®—é‡å¯ç‚¹ ---
    restart_epochs_list = []
    current_T = T_0
    current_restart_point = 0

    while True:
        current_restart_point += current_T
        if current_restart_point <= epochs:
            restart_epochs_list.append(current_restart_point)
            current_T *= T_mult
        else:
            break

    # è½¬æ¢ä¸ºé›†åˆä»¥ä¾¿åœ¨å¾ªç¯ä¸­å¿«é€ŸæŸ¥æ‰¾
    restart_epochs = set(restart_epochs_list)
    if restart_epochs:
        print(f"ä¼˜åŒ–å™¨å°†åœ¨ä»¥ä¸‹ epoch ç»“æŸåé‡ç½®: {sorted(restart_epochs_list)}")
    # --- é‡å¯ç‚¹è®¡ç®—ç»“æŸ ---

    for epoch in range(epochs):
        model.train()
        total_train_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            mu, _, _ = model(inputs)
            loss = criterion(mu, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()

        avg_train_loss = total_train_loss / len(train_loader)

        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                mu, _, _ = model(inputs)
                loss = criterion(mu, labels)
                total_val_loss += loss.item()

        avg_val_loss = total_val_loss / len(val_loader)
        scheduler.step()
        current_lr = optimizer.param_groups[0]['lr']
        print(
            f"Pre-train Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}, LR: {current_lr:.2e}")

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            best_model_state_dict = model.state_dict()  # ä¿å­˜åˆ°å†…å­˜
            patience_counter = patience  # é‡ç½®è®¡æ•°å™¨
            print(f"  - (å†…å­˜)é¢„è®­ç»ƒæ¨¡å‹å·²æ›´æ–°ï¼ŒéªŒè¯HuberLossæå‡è‡³: {best_val_loss:.6f}")
        else:
            patience_counter -= 1
            if patience_counter == 0:
                print(f"éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„ï¼Œè§¦å‘æ—©åœã€‚")
                break

        if (epoch + 1) in restart_epochs and (epoch + 1) < epochs:
            print(f"--- Epoch {epoch+1} æ˜¯ä¸€ä¸ªé‡å¯ç‚¹ã€‚é‡ç½® AdamW ä¼˜åŒ–å™¨çŠ¶æ€ï¼ ---")
            # é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€
            optimizer = torch.optim.AdamW(
                model.backbone.parameters(), lr=args.lr_pretrain)
            # å°†æ–°ä¼˜åŒ–å™¨äº¤ç»™è°ƒåº¦å™¨
            scheduler = CosineAnnealingWarmRestarts(
                optimizer, T_0=T_0, T_mult=T_mult, eta_min=1e-6)
            # æ‰‹åŠ¨å°† scheduler "å¿«è¿›" åˆ°å½“å‰ epoch
            for _ in range(epoch + 1):
                scheduler.step()

    print(f"--- [é˜¶æ®µä¸€] æœ¬æ¬¡è¿è¡Œå®Œæˆï¼Œæœ€ä½³æŸå¤±: {best_val_loss:.6f} ---")
    return best_val_loss, best_model_state_dict


# ========== 3. é˜¶æ®µäºŒï¼šæ•´ä½“æ¨¡å‹å¾®è°ƒ (é‡æ„) ==========

def run_finetuning(model, data_loaders, device, final_save_path, args):
    """ä½¿ç”¨å¤åˆæŸå¤±å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒ"""
    print("\n--- [é˜¶æ®µäºŒ] å¼€å§‹æ•´ä½“æ¨¡å‹å¾®è°ƒ ---")

    # ä¸º backbone å’Œ head è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
    optimizer_params = [
        {
            "params": model.backbone.parameters(),
            "lr": args.lr_finetune / 10  # ä¸º backbone è®¾ç½®ä¸€ä¸ªéå¸¸ä½çš„å­¦ä¹ ç‡
        },
        {
            "params": model.hetero_head.parameters(),
            "lr": args.lr_finetune  # ä¸ºæ–° head è®¾ç½®ä¸€ä¸ªç›¸å¯¹è¾ƒé«˜çš„å­¦ä¹ ç‡
        }
    ]

    opt = torch.optim.AdamW(optimizer_params, weight_decay=1e-4)

    dl_A, dl_B, dl_val = data_loaders['source'], data_loaders['target_train'], data_loaders['target_val']
    dl_A_iter = iter(dl_A)

    best_val = float('inf')

    patience = args.patience_finetune
    patience_counter = patience  # ä½¿ç”¨ä¸€ä¸ªè®¡æ•°å™¨

    for epoch in range(args.epochs_finetune):
        model.train()
        for xb_B, yb_B in dl_B:
            xb_B, yb_B = xb_B.to(device), yb_B.to(device)
            try:
                xa_A, _ = next(dl_A_iter)
            except StopIteration:
                dl_A_iter = iter(dl_A)
                xa_A, _ = next(dl_A_iter)
            if xa_A.size(0) != xb_B.size(0):
                xa_A = xa_A[:xb_B.size(0)]
            xa_A = xa_A.to(device)

            mu_B, logvar_B, feat_B = model(xb_B)
            with torch.no_grad():
                _, _, feat_A = model(xa_A)

            nll = heteroscedastic_nll(mu_B, logvar_B, yb_B)
            r2_loss = (
                1.0 - batch_r2(yb_B, mu_B).clamp(min=-1.0, max=1.0)).mean()
            coral = coral_loss(feat_A, feat_B)
            loss = nll + args.alpha_r2 * r2_loss + args.lambda_coral * coral

            opt.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for xb, yb in dl_val:
                xb, yb = xb.to(device), yb.to(device)
                mu, logvar, _ = model(xb)
                val_loss += heteroscedastic_nll(mu, logvar, yb).item()
        val_loss /= len(dl_val)

        print(
            f"Fine-tune Epoch [{epoch+1}/{args.epochs_finetune}], Val NLL: {val_loss:.4f}")

        if val_loss < best_val:
            print(f"  - å¾®è°ƒéªŒè¯æŸå¤±æ”¹å–„ ({best_val:.4f} -> {val_loss:.4f})ã€‚ä¿å­˜æ¨¡å‹...")
            best_val = val_loss
            torch.save(model.state_dict(), final_save_path)
            patience_counter = patience  # é‡ç½®è®¡æ•°å™¨
        else:
            patience_counter -= 1
            if patience_counter == 0:
                print(f"éªŒè¯æŸå¤±è¿ç»­ {patience} è½®æœªæ”¹å–„ï¼Œè§¦å‘æ—©åœã€‚")
                break

    print(f"--- [é˜¶æ®µäºŒ] å¾®è°ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²ä¿å­˜è‡³ {final_save_path} ---")


# ========== 4.ï¼ˆå¯é€‰ï¼‰è·å–æœ€å¥½æ¨¡å‹çš„è¾“å‡ºç»“æœ ==========
def get_predictions(model, dataloader, device):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œå¹¶è¿”å›Numpyæ•°ç»„ã€‚
    """
    model.eval()  # ç¡®ä¿æ¨¡å‹å¤„äºè¯„ä¼°æ¨¡å¼
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            # é€‚é… AlignHeteroMLP çš„è¾“å‡ºï¼Œåªå– mu
            preds, _, _ = model(inputs)

            all_preds.append(preds.cpu().numpy())
            all_labels.append(labels.numpy())  # labels æœ¬èº«å°±åœ¨ CPU ä¸Š

    return np.concatenate(all_preds), np.concatenate(all_labels)

# ========== 5. ä¸»å‡½æ•° (å·²ä¿®æ”¹) ==========


def main():
    # <<< ä¸€æ¬¡æ€§è§£ææ‰€æœ‰å‚æ•°
    args = setup_args()
    DEVICE = torch.device(args.device)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)
    os.makedirs(args.save_path, exist_ok=True)

    # æ¨¡å‹å­˜å‚¨åœ¨./resultsä¸‹
    pretrained_path = os.path.join(
        args.save_path, f'{args.opamp}_pretrained.pth')
    finetuned_path = os.path.join(
        args.save_path, f'{args.opamp}_finetuned.pth')

    data = get_data_and_scalers(opamp_type=args.opamp)
    X_src, y_src = data['source']
    input_dim = X_src.shape[1]
    output_dim = y_src.shape[1]
    print(
        f"--- åŠ¨æ€æ£€æµ‹åˆ° {args.opamp} çš„ç»´åº¦: Input={input_dim}, Output={output_dim} ---")
    X_src_train, y_src_train = data['source_train']
    X_src_val, y_src_val = data['source_val']
    X_trg_tr, y_trg_tr = data['target_train']
    X_trg_val, y_trg_val = data['target_val']

    # é¢„è®­ç»ƒçš„è®­ç»ƒé›†ä½¿ç”¨ source_train
    pretrain_loader_A = make_loader(
        X_src_train, y_src_train, args.batch_a, shuffle=True)
    # é¢„è®­ç»ƒçš„éªŒè¯é›†ä½¿ç”¨ source_val (ä¸å†æ˜¯ target_val)
    pretrain_loader_val = make_loader(
        X_src_val, y_src_val, args.batch_a, shuffle=False)

    # æ³¨æ„ï¼šinput_dim å’Œ output_dim ä¸åœ¨å‘½ä»¤è¡Œå‚æ•°é‡Œï¼Œæ‰€ä»¥æˆ‘ä»¬ä»å­—å…¸é‡Œå–
    # model_config = TASK_CONFIGS[args.opamp] # model_config æœªè¢«ä½¿ç”¨ï¼Œæ³¨é‡Šæ‰

    # ä»…åˆå§‹åŒ–ä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼Œç”¨äºåŠ è½½æƒé‡æˆ–ä½œä¸ºé5t opampçš„åŸºç¡€
    model = AlignHeteroMLP(
        input_dim=input_dim,
        output_dim=output_dim,
        hidden_dim=args.hidden_dim,
        num_layers=args.num_layers,
        dropout_rate=args.dropout_rate
    ).to(DEVICE)

    # --- [é˜¶æ®µä¸€] é¢„è®­ç»ƒé€»è¾‘ (å·²é‡æ„) ---
    if args.restart or not os.path.exists(pretrained_path):

        # --- START: 5t Opamp ç‰¹æ®Šå…ƒä¼˜åŒ–é€»è¾‘ ---
        if args.opamp == '5t':
            print("--- æ£€æµ‹åˆ° 5t opampï¼Œå¯ç”¨â€œå…ƒä¼˜åŒ–â€é¢„è®­ç»ƒç­–ç•¥ ---")

            # åœ¨è¿™é‡Œå®šä¹‰ 5t çš„ä¸“å±é…ç½®
            RESTART_PRETRAIN = 9
            PRETRAIN_SCHEDULER_CONFIGS = [  # é‡å¤æ‰§è¡Œä¸‰æ¬¡å…ƒä¼˜åŒ–
                # --- ç­–ç•¥ä¸€ï¼šå¹¿æ³›æ¢ç´¢ ---
                {"T_0": 50, "T_mult": 1, "epochs_pretrain": 100},  # ç¬¬1æ¬¡é‡å¯
                {"T_0": 55, "T_mult": 1, "epochs_pretrain": 110},  # ç¬¬2æ¬¡é‡å¯
                # --- ç­–ç•¥äºŒï¼šç²¾ç»†æ‰“ç£¨ ---
                {"T_0": 125, "T_mult": 1, "epochs_pretrain": 125},  # ç¬¬3æ¬¡é‡å¯
                # --- ç­–ç•¥ä¸€ï¼šå¹¿æ³›æ¢ç´¢ ---
                {"T_0": 50, "T_mult": 1, "epochs_pretrain": 100},  # ç¬¬4æ¬¡é‡å¯
                {"T_0": 55, "T_mult": 1, "epochs_pretrain": 110},  # ç¬¬5æ¬¡é‡å¯
                # --- ç­–ç•¥äºŒï¼šç²¾ç»†æ‰“ç£¨ ---
                {"T_0": 125, "T_mult": 1, "epochs_pretrain": 125},  # ç¬¬6æ¬¡é‡å¯
                # --- ç­–ç•¥ä¸€ï¼šå¹¿æ³›æ¢ç´¢ ---
                {"T_0": 50, "T_mult": 1, "epochs_pretrain": 100},  # ç¬¬7æ¬¡é‡å¯
                {"T_0": 55, "T_mult": 1, "epochs_pretrain": 110},  # ç¬¬8æ¬¡é‡å¯
                # --- ç­–ç•¥äºŒï¼šç²¾ç»†æ‰“ç£¨ ---
                {"T_0": 125, "T_mult": 1, "epochs_pretrain": 125},  # ç¬¬9æ¬¡é‡å¯
            ]

            global_best_val_loss = float('inf')

            print(f"--- [å…ƒä¼˜åŒ–æµç¨‹å¯åŠ¨] å°†æ‰§è¡Œ {RESTART_PRETRAIN} æ¬¡ç‹¬ç«‹é¢„è®­ç»ƒ ---")

            for i in range(RESTART_PRETRAIN):
                print(f"\n{'='*30} äººå·¥é‡å¯ {i+1}/{RESTART_PRETRAIN} {'='*30}")

                # æ¯æ¬¡éƒ½é‡æ–°åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹
                model_run = AlignHeteroMLP(
                    input_dim=input_dim,
                    output_dim=output_dim,
                    hidden_dim=args.hidden_dim,
                    num_layers=args.num_layers,
                    dropout_rate=args.dropout_rate
                ).to(DEVICE)

                if i < len(PRETRAIN_SCHEDULER_CONFIGS):
                    current_scheduler_config = PRETRAIN_SCHEDULER_CONFIGS[i]
                else:
                    # å¦‚æœé…ç½®åˆ—è¡¨ä¸å¤Ÿé•¿ï¼Œåˆ™å¤ç”¨æœ€åä¸€ä¸ª
                    current_scheduler_config = PRETRAIN_SCHEDULER_CONFIGS[-1]

                best_loss_this_run, best_state_dict_this_run = run_pretraining(
                    model_run, pretrain_loader_A, pretrain_loader_val, DEVICE, args, current_scheduler_config
                )
                print(
                    f"--- äººå·¥é‡å¯ {i+1} å®Œæˆï¼Œæœ¬æ¬¡æœ€ä½³æŸå¤±: {best_loss_this_run:.6f} ---")

                if best_state_dict_this_run and best_loss_this_run < global_best_val_loss:
                    global_best_val_loss = best_loss_this_run
                    print(
                        f"  ğŸ†ğŸ†ğŸ† æ–°çš„å…¨å±€æœ€ä½³æŸå¤±ï¼ {global_best_val_loss:.6f}ã€‚æ­£åœ¨è¦†ç›– {pretrained_path}...")
                    torch.save(best_state_dict_this_run, pretrained_path)
                elif not best_state_dict_this_run:
                    print(
                        f"  -- æœ¬æ¬¡è¿è¡Œæœªèƒ½äº§ç”Ÿæœ‰æ•ˆæ¨¡å‹ (æŸå¤±: {best_loss_this_run:.6f})ï¼Œä¸ä¿å­˜ã€‚")
                else:
                    print(
                        f"  -- æœ¬æ¬¡ç»“æœ ({best_loss_this_run:.6f}) æœªè¶…è¶Šå…¨å±€æœ€ä½³ ({global_best_val_loss:.6f})ï¼Œä¸ä¿å­˜ã€‚")

            print(
                f"\næ‰€æœ‰ {RESTART_PRETRAIN} æ¬¡é‡å¯å®Œæˆã€‚æœ€ç»ˆå…¨å±€æœ€ä½³æŸå¤±: {global_best_val_loss:.6f}")
            print(f"--- [é˜¶æ®µä¸€] 5t å…ƒä¼˜åŒ–é¢„è®­ç»ƒå®Œæˆ ---")

        # --- END: 5t Opamp é€»è¾‘ ---

        # --- START: å…¶ä»– Opamp çš„æ ‡å‡†é€»è¾‘ ---
        else:
            print(f"--- [é˜¶æ®µä¸€] {args.opamp} å¯ç”¨æ ‡å‡†é¢„è®­ç»ƒ ---")

            # ä¸ºæ ‡å‡†é€»è¾‘åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ scheduler_config
            # æ³¨æ„ï¼šT_0=200, T_mult=1 æ˜¯åŸæ–‡ä»¶ä¸­çš„ç¡¬ç¼–ç å€¼
            default_scheduler_config = {
                "epochs_pretrain": args.epochs_pretrain,
                "T_0": 200,
                "T_mult": 1
            }

            best_loss, best_state_dict = run_pretraining(
                model, pretrain_loader_A, pretrain_loader_val, DEVICE, args, default_scheduler_config
            )

            if best_state_dict:
                print(
                    f"--- [é˜¶æ®µä¸€] é¢„è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {best_loss:.6f}) è‡³ {pretrained_path} ---")
                torch.save(best_state_dict, pretrained_path)
            else:
                print(f"--- [é˜¶æ®µä¸€] é¢„è®­ç»ƒå®Œæˆï¼Œä½†æœªèƒ½æ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹ã€‚è·³è¿‡ä¿å­˜ã€‚ ---")

        # --- END: å…¶ä»– Opamp é€»è¾‘ ---

        # --- å…³é”®ï¼šæ— è®ºå“ªç§è®­ç»ƒæ–¹å¼ï¼Œè®­ç»ƒåéƒ½ä»ç£ç›˜åŠ è½½æœ€ä½³æ¨¡å‹ ---
        if os.path.exists(pretrained_path):
            print(f"--- [é˜¶æ®µä¸€] åŠ è½½æœ€ä½³é¢„è®­ç»ƒæ¨¡å‹ {pretrained_path} ä»¥è¿›è¡Œå¾®è°ƒ ---")
            model.load_state_dict(torch.load(
                pretrained_path, map_location=DEVICE))
        else:
            print(
                f"--- [é˜¶æ®µä¸€] è­¦å‘Šï¼šé¢„è®­ç»ƒæœªäº§ç”Ÿä»»ä½•æ¨¡å‹æ–‡ä»¶ ({pretrained_path})ã€‚å¾®è°ƒå°†ä»éšæœºæƒé‡å¼€å§‹ï¼ ---")

    else:
        # è¿™éƒ¨åˆ†é€»è¾‘ (åŠ è½½ç°æœ‰æ¨¡å‹) ä¿æŒä¸å˜
        print(f"--- [é˜¶æ®µä¸€] è·³è¿‡é¢„è®­ç»ƒï¼ŒåŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹: {pretrained_path} ---")
        model.load_state_dict(torch.load(pretrained_path, map_location=DEVICE))

    # --- [é˜¶æ®µäºŒ] å¾®è°ƒé€»è¾‘ (ä¿æŒä¸å˜) ---
    finetune_loaders = {
        'source': make_loader(X_src, y_src, args.batch_a, shuffle=True, drop_last=True),
        'target_train': make_loader(X_trg_tr, y_trg_tr, args.batch_b, shuffle=True),
        'target_val': make_loader(X_trg_val, y_trg_val, args.batch_b, shuffle=False)
    }
    if os.path.exists(finetuned_path) and not args.restart:
        print(f"--- [é˜¶æ®µäºŒ] æ£€æµ‹åˆ°å·²æœ‰å¾®è°ƒæ¨¡å‹: {finetuned_path}ï¼Œè·³è¿‡å¾®è°ƒå¹¶ç›´æ¥è½½å…¥è¯¥æƒé‡ ---")
        model.load_state_dict(torch.load(finetuned_path, map_location=DEVICE))
    else:
        # åªæœ‰åœ¨æ¨¡å‹æ–‡ä»¶ç¡®å®å­˜åœ¨æ—¶æ‰å¼€å§‹å¾®è°ƒï¼ˆé˜²æ­¢é¢„è®­ç»ƒå½»åº•å¤±è´¥ï¼‰
        if os.path.exists(pretrained_path) or (args.restart and os.path.exists(pretrained_path)):
            run_finetuning(model, finetune_loaders,
                           DEVICE, finetuned_path, args)
        elif not os.path.exists(pretrained_path) and not args.restart:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†æˆ‘ä»¬æ²¡æœ‰é‡å¯ï¼Œè¯´æ˜æ˜¯åŠ è½½æ¨¡å¼ï¼Œ
            # è¿™ç§æƒ…å†µä¸‹ finetuned_path åº”è¯¥å­˜åœ¨ (å¦‚ä¸Šä¸€ä¸ª if æ‰€ç¤º)ï¼Œ
            # å¦‚æœ finetuned_path ä¹Ÿä¸å­˜åœ¨ï¼Œæˆ‘ä»¬å°±ä¸åº”è¯¥è¿è¡Œ finetuning
            print(
                f"é”™è¯¯ï¼šæœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ {pretrained_path} ä¸”æœªæ‰¾åˆ°å¾®è°ƒæ¨¡å‹ {finetuned_path}ã€‚æ— æ³•å¼€å§‹å¾®è°ƒã€‚")
        else:
            print(f"è­¦å‘Šï¼šé¢„è®­ç»ƒå¤±è´¥ï¼Œæœªç”Ÿæˆ {pretrained_path}ã€‚è·³è¿‡å¾®è°ƒé˜¶æ®µã€‚")

    print("\nè®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆã€‚")

    # (å¯é€‰)æµ‹è¯•
    if args.evaluate:
        print("\n--- [è¯„ä¼°æµç¨‹å¯åŠ¨] ---")

        # 1. æ£€æŸ¥æœ€ä½³æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(finetuned_path):
            print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°å·²è®­ç»ƒçš„æ¨¡å‹æ–‡ä»¶ {finetuned_path}ã€‚è·³è¿‡è¯„ä¼°ã€‚")
            return

        # 2. åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡
        print(f"ä¸ºè¯„ä¼°åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {finetuned_path}")
        model.load_state_dict(torch.load(finetuned_path, map_location=DEVICE))

        # 3. è·å–é¢„æµ‹å€¼å’ŒçœŸå®å€¼ (æ ‡å‡†åŒ–ç©ºé—´)
        # æ³¨æ„ï¼šfinetune_loaders['target_val'] æ˜¯éªŒè¯é›†çš„æ•°æ®åŠ è½½å™¨
        print("åœ¨éªŒè¯é›†ä¸Šç”Ÿæˆé¢„æµ‹...")
        pred_scaled, true_scaled = get_predictions(
            model, finetune_loaders['target_val'], DEVICE)

        # 4. è°ƒç”¨å¤–éƒ¨è¯„ä¼°å‡½æ•°
        calculate_and_print_metrics(pred_scaled, true_scaled, data['y_scaler'])


if __name__ == "__main__":
    main()
