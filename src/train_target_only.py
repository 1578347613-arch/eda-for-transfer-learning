from pathlib import Path
from typing import Tuple
import argparse
import ast # <-- (C3 æ‰‹æœ¯) å¯¼å…¥ ast æ¥è§£æåˆ—è¡¨

import numpy as np
import torch
from torch.utils.data import DataLoader, TensorDataset

from data_loader import get_data_and_scalers
from loss_function import heteroscedastic_nll, batch_r2
# â¬‡ï¸ (C3 æ‰‹æœ¯) æˆ‘ä»¬å‡è®¾ C3/models/ é‡Œæ˜¯ C1 çš„æ¨¡å‹ï¼
from models.align_hetero import AlignHeteroMLP 
# â¬‡ï¸ (C3 æ‰‹æœ¯) æˆ‘ä»¬ç°åœ¨å¯¼å…¥ C3 çš„â€œæ··åˆåœ£ç»â€
import config 

# --- è·¯å¾„å®šä¹‰ (æ¥è‡ª C2) ---
SRC_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SRC_DIR.parent
RESULTS_DIR = PROJECT_ROOT / "src" / "results"
# RESULTS_DIR.mkdir(parents=True, exist_ok=True) # (C2 çš„é€»è¾‘)

# --- è¾…åŠ©å‡½æ•° (æ¥è‡ª C2) ---
def set_seed(seed: int):
    # (ä»£ç  100% æ¥è‡ª C2ï¼Œæ­¤å¤„çœç•¥)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

def make_loader(x: np.ndarray, y: np.ndarray, bs: int, shuffle: bool, drop_last: bool) -> DataLoader:
    # (ä»£ç  100% æ¥è‡ª C2ï¼Œæ­¤å¤„çœç•¥)
    ds = TensorDataset(torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))
    return DataLoader(ds, batch_size=bs, shuffle=shuffle, drop_last=drop_last)

def run_epoch(model, loader, optimizer, alpha_r2, device, phase="train"):
    # (ä»£ç  100% æ¥è‡ª C2ï¼Œæ­¤å¤„çœç•¥)
    is_train = (optimizer is not None) and (phase == "train")
    model.train(is_train)
    total_nll, total_r2l, n_batches = 0.0, 0.0, max(1, len(loader))
    for xb, yb in loader:
        xb, yb = xb.to(device), yb.to(device)
        with torch.set_grad_enabled(is_train):
            mu, logv, _ = model(xb) # <-- (C1 çš„ align_hetero.py å®Œç¾å…¼å®¹)
            nll = heteroscedastic_nll(mu, logv, yb, reduction="mean")
            r2l = (1.0 - batch_r2(yb, mu)).mean()
            loss = nll + alpha_r2 * r2l
            if is_train:
                optimizer.zero_grad(set_to_none=True)
                loss.backward()
                optimizer.step()
        total_nll += nll.item()
        total_r2l += r2l.item()
    return total_nll / n_batches, total_r2l / n_batches


# ==========================================================
# ========== ğŸ‘¨â€âš•ï¸ "C3 èåˆæ‰‹æœ¯" æ ¸å¿ƒ ğŸ‘¨â€âš•ï¸ ==========
# ==========================================================

def setup_args():
    """
    [C3 èåˆç‰ˆ] æ™ºèƒ½ setup_args (é€»è¾‘æ¥è‡ª C1 train_align_hetero.py)ï¼š
    å®ƒç°åœ¨ä¼šè¯»å– C3 "æ··åˆåœ£ç»" config.py é‡Œçš„ TASK_CONFIGS å­—å…¸ï¼
    """
    parser = argparse.ArgumentParser(description="Target-Only è®­ç»ƒè„šæœ¬ (C3 èåˆç‰ˆ)")
    parser.add_argument("--opamp", type=str, required=True,
                        choices=config.TASK_CONFIGS.keys(), help="å¿…é¡»æŒ‡å®šçš„ç”µè·¯ç±»å‹")

    # Step 1: å…ˆè§£æå‡º opamp ç±»å‹
    temp_args, other_args = parser.parse_known_args()
    opamp_type = temp_args.opamp

    # Step 2: åˆå¹¶ C3 "æ··åˆåœ£ç»" çš„é…ç½®ä½œä¸ºé»˜è®¤å€¼
    # (COMMON_CONFIG + TASK_CONFIGS[opamp])
    defaults = {**config.COMMON_CONFIG, **config.TASK_CONFIGS.get(opamp_type, {})}

    # Step 3: åŠ¨æ€ä¸ºæ‰€æœ‰ç®€å•ç±»å‹çš„é»˜è®¤å‚æ•°åˆ›å»ºå‘½ä»¤è¡Œå¼€å…³
    for key, value in defaults.items():
        if isinstance(value, (list, dict)):
            continue  # è·³è¿‡ C1 çš„ hidden_dims å’Œ PRETRAIN_SCHEDULER_CONFIGS
        if isinstance(value, bool):
            parser.add_argument(
                f"--{key}", action=argparse.BooleanOptionalAction, help=f"å¼€å…³ '{key}'")
        else:
            # (ç¡®ä¿ key å­˜åœ¨ï¼Œå› ä¸º C1/C2 çš„ config key å¯èƒ½ä¸å®Œå…¨ä¸€æ ·)
            if key in config.COMMON_CONFIG or key in config.TASK_CONFIGS[opamp_type]:
                 parser.add_argument(
                    f"--{key}", type=type(value), help=f"è®¾ç½® '{key}'")

    # Step 4: å°†åˆå¹¶åçš„é…ç½®è®¾ç½®ä¸ºè§£æå™¨çš„é»˜è®¤å€¼å¹¶è¿›è¡Œæœ€ç»ˆè§£æ
    parser.set_defaults(**defaults)

    # --- è¦†ç›–ï¼šæ‰‹åŠ¨æ·»åŠ åœ¨configä¸­æ˜¯åˆ—è¡¨ï¼Œä½†å¸Œæœ›åœ¨å‘½ä»¤è¡Œä¸­è¦†ç›–çš„å‚æ•° ---
    # (è¿™æ˜¯ä¸ºäº† 100% å…¼å®¹ C1 çš„æ¶æ„)
    parser.add_argument("--hidden_dims", type=str,
                        help="MLPéšè—å±‚ç»´åº¦åˆ—è¡¨, e.g., '[256, 256]'")
    
    args = parser.parse_args()

    # --- (C3 æ‰‹æœ¯) C1 "é»„é‡‘æ¶æ„" çš„åå¤„ç† ---
    # (é€»è¾‘ 100% æ¥è‡ª C1 çš„ train_align_hetero.py)
    if isinstance(args.hidden_dims, str):
        try:
            # å¦‚æœä»å‘½ä»¤è¡Œä¼ å…¥ '[...]' å­—ç¬¦ä¸²ï¼Œåˆ™è§£æå®ƒ
            args.hidden_dims = ast.literal_eval(args.hidden_dims)
        except (ValueError, SyntaxError):
            print(f"é”™è¯¯: --hidden_dims å‚æ•°æ ¼å¼ä¸æ­£ç¡®: {args.hidden_dims}")
            sys.exit(1)
    # (å¦‚æœå‘½ä»¤è¡Œæ²¡ä¼ ï¼Œargs.hidden_dims å·²ç»æ˜¯æ¥è‡ª config.py çš„ C1 é»„é‡‘åˆ—è¡¨ï¼)
    
    return args


# --- ä¸»è®­ç»ƒå‡½æ•° (C3 èåˆç‰ˆ) ---
def main():
    args = setup_args() # <-- (C3 æ‰‹æœ¯) è°ƒç”¨æˆ‘ä»¬æ–°çš„ C3 setup_args()

    device = torch.device(args.device)
    set_seed(args.seed)

    # è¶…å‚æ˜ å°„ï¼ˆç°åœ¨ 100% å…¼å®¹ C3 "æ··åˆåœ£ç»"ï¼‰
    # (C1 çš„ config é‡Œå« 'epochs_finetune', 'patience_finetune'...)
    epochs = args.epochs_finetune 
    patience = args.patience_finetune
    lr = args.lr_finetune # <-- (C1 config é‡Œå« lr_finetune)
    batch_size = args.batch_b
    alpha_r2 = args.alpha_r2

    data = get_data_and_scalers(opamp_type=args.opamp)
    Xtr, Ytr = data["target_train"]
    Xva, Yva = data["target_val"]
    
    # (æˆ‘ä»¬å‡è®¾ C3 ä¼šä½¿ç”¨ C1 çš„ data_loader.py)
    try:
        input_dim = data['x_dim']
        output_dim = data['y_dim']
    except KeyError:
        # å…œåº• C2 data_loader
        input_dim, output_dim = Xtr.shape[1], Ytr.shape[1]

    # ==========================================================
    # ========== ğŸ‘¨â€âš•ï¸ "C3 èåˆæ‰‹æœ¯" æ ¸å¿ƒ ğŸ‘¨â€âš•ï¸ ==========
    # ==========================================================
    print(f"âœ… [C3 èåˆ] æ­£åœ¨ä¸º {args.opamp} æ„å»º C1 é»„é‡‘æ¶æ„...")
    
    # â¬‡ï¸ (C3 æ‰‹æœ¯) è¿™ 6 è¡Œä»£ç æ˜¯â€œæ‰‹æœ¯â€çš„æ ¸å¿ƒï¼
    model = AlignHeteroMLP(
        input_dim=input_dim, 
        output_dim=output_dim,
        # â¬‡ï¸ å…³é”®ï¼ä½¿ç”¨ C1 çš„å¤æ‚åˆ—è¡¨ï¼ â¬‡ï¸
        hidden_dims=args.hidden_dims,
        dropout_rate=args.dropout_rate
        # (æˆ‘ä»¬å‡è®¾ C3/models/align_hetero.py æ˜¯ C1 çš„ç‰ˆæœ¬)
    ).to(device)

    # ==========================================================
    # ========== "C3 èåˆæ‰‹æœ¯" ç»“æŸ ==========
    # ==========================================================

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    train_loader = make_loader(
        Xtr, Ytr, batch_size, shuffle=True,  drop_last=True)
    val_loader = make_loader(Xva, Yva, batch_size,
                             shuffle=False, drop_last=False)

    ckpt_path = RESULTS_DIR / f"{args.opamp}_target_only.pth"
    print(
        f"[Target-Only (C3 èåˆç‰ˆ)] opamp: {args.opamp}, device: {device}, saving to: {ckpt_path.name}")

    # ========== (C2 çš„è·³è¿‡é€»è¾‘ï¼Œ100% ä¿ç•™) ==========
    if ckpt_path.exists() and not args.restart:
        try:
            state = torch.load(ckpt_path, map_location=device)
            state_dict = state.get("state_dict", state)
            # â¬‡ï¸ å®Œç¾ï¼ç°åœ¨åŠ è½½çš„æƒé‡ 100% å…¼å®¹ C1 é»„é‡‘æ¶æ„ï¼
            model.load_state_dict(state_dict) 
            va_nll0, _ = run_epoch(
                model, val_loader, None, alpha_r2, device, "val")
            print(f"[Target-Only] æ£€æµ‹åˆ°å·²æœ‰ ckptï¼ˆ{ckpt_path.name}ï¼‰ã€‚æŒ‰é»˜è®¤ç­–ç•¥è·³è¿‡è®­ç»ƒå¹¶é€€å‡ºã€‚"
                  f"å½“å‰ Val NLL={va_nll0:.4f}")
        except Exception as e:
            print(f"[Target-Only] å‘ç° ckpt ä½†è½½å…¥å¤±è´¥ï¼ˆ{e}ï¼‰ã€‚å°†ä»å¤´è®­ç»ƒã€‚")
        else:
            return

    # ========== (C2 çš„è®­ç»ƒå¾ªç¯ï¼Œ100% ä¿ç•™) ==========
    if args.restart and ckpt_path.exists():
        # (ä»£ç  100% æ¥è‡ª C2ï¼Œæ­¤å¤„çœç•¥)
        try:
            ckpt_path.unlink()
            print("`--restart` æŒ‡å®šï¼šå·²åˆ é™¤æ—§ checkpointï¼Œå°†ä»å¤´è®­ç»ƒã€‚")
        except Exception as e:
            print(f"åˆ é™¤æ—§ checkpoint å¤±è´¥ï¼ˆå¿½ç•¥ç»§ç»­ï¼‰ï¼š{e}")

    best_val_nll = float("inf")
    patience_counter = patience

    for ep in range(1, epochs + 1):
        tr_nll, tr_r2l = run_epoch(
            model, train_loader, optimizer, alpha_r2, device, "train")
        va_nll, va_r2l = run_epoch(
            model, val_loader, None,       alpha_r2, device, "val")
        print(
            f"[Target-Only][{ep:03d}/{epochs}] Train NLL={tr_nll:.4f} | Val NLL={va_nll:.4f}")

        if va_nll < best_val_nll:
            best_val_nll = va_nll
            # â¬‡ï¸ å®Œç¾ï¼ç°åœ¨ä¿å­˜çš„æƒé‡ 100% å…¼å®¹ C1 é»„é‡‘æ¶æ„ï¼
            torch.save({"state_dict": model.state_dict()}, ckpt_path) 
            patience_counter = patience
            print(" Â -> New best model saved.")
        else:
            patience_counter -= 1
            if patience_counter <= 0:
                print(f"Early stopping at epoch {ep}.")
                break

    print(f"\n[Target-Only (C3 èåˆç‰ˆ)] Finished. Best model at: {ckpt_path}")


if __name__ == "__main__":
    main()