# fused_project/src/unified_inverse_train.py (C3 èåˆç‰ˆ v2 - ç»ˆæç‰ˆï¼)
#
# åŸºç¡€ï¼šæ¥è‡ª C2 (æ‚¨å‘çš„ [2025-11-10_07:01:21] ç‰ˆæœ¬)
# æ‰‹æœ¯ï¼š
#   1. å½»åº•æ›¿æ¢ setup_args()ï¼Œä½¿å…¶ 100% å…¼å®¹ C3 "æ··åˆåœ£ç»"ï¼
#   2. æ›¿æ¢ prepare_inverse_dataset() ä¸º C1 çš„â€œæ›´ä¼˜â€é€»è¾‘ (è®­ç»ƒæ‰€æœ‰æ•°æ®)ï¼
#   3. ä¿®æ”¹ main() æ¥è°ƒç”¨æ–°å‡½æ•°ï¼
#
import os
import argparse
import json
import joblib
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from pathlib import Path

# --- ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥ ---
from data_loader import get_data_and_scalers
# â¬‡ï¸ (C3 æ‰‹æœ¯) æˆ‘ä»¬ç°åœ¨å¯¼å…¥ C3 çš„â€œæ··åˆåœ£ç»â€
import config 

# ==============================================================================
#  æ ¸å¿ƒé€»è¾‘ (æ¥è‡ª C2ï¼Œå› ä¸º C2 çš„ MDN æ¨¡å‹æ›´å¼ºï¼)
# ==============================================================================

class InverseMDN(nn.Module):
    """(100% ä¿æŒ C2 çš„ 5 å±‚æ¨¡å‹å®šä¹‰)"""
    def __init__(self, input_dim, output_dim, n_components, hidden_dim, num_layers):
        super().__init__()
        layers = [nn.Linear(input_dim, hidden_dim), nn.GELU()]
        for _ in range(num_layers - 1):
            layers += [nn.Linear(hidden_dim, hidden_dim), nn.GELU()]
        self.backbone = nn.Sequential(*layers)
        self.pi = nn.Linear(hidden_dim, n_components)
        self.mu = nn.Linear(hidden_dim, n_components * output_dim)
        self.sigma_raw = nn.Linear(hidden_dim, n_components * output_dim)
        self.n_components = n_components
        self.output_dim = output_dim
        self.softplus = nn.Softplus()

    def forward(self, y):
        h = self.backbone(y)
        pi = torch.softmax(self.pi(h), dim=-1)
        mu = self.mu(h).view(-1, self.n_components, self.output_dim)
        sigma = self.softplus(self.sigma_raw(h)).view(-1, self.n_components, self.output_dim) + 1e-6
        return pi, mu, sigma

def mdn_nll_loss(pi, mu, sigma, target_x):
    """(100% ä¿æŒ C2 çš„ NLL æŸå¤±å®šä¹‰)"""
    B, K, D = mu.shape
    target = target_x.unsqueeze(1).expand(B, K, D)
    log_prob = -0.5 * torch.sum(((target - mu) / sigma) ** 2 + 2 * torch.log(sigma) + np.log(2 * np.pi), dim=2)
    log_mix = torch.logsumexp(torch.log(pi + 1e-9) + log_prob, dim=1)
    return -torch.mean(log_mix)

# ==========================================================
# ========== ğŸ‘¨â€âš•ï¸ "C3 èåˆæ‰‹æœ¯" æ ¸å¿ƒ 1 ğŸ‘¨â€âš•ï¸ ==========
# ==========================================================
def prepare_inverse_dataset(opamp_type, device):
    """
    [C3 èåˆç‰ˆ] (é€»è¾‘æ¥è‡ª C1ï¼Œå› ä¸ºå®ƒæ›´ä¼˜ï¼)
    å®ƒä¼šåŠ è½½ A åŸŸå…¨é›† + B åŸŸ Tain/Valï¼Œè®© MDN åœ¨æ‰€æœ‰å¯ç”¨æ•°æ®ä¸Šè®­ç»ƒï¼
    """
    data = get_data_and_scalers(opamp_type=opamp_type)
    
    # (æˆ‘ä»¬å‡è®¾ C3 ä¼šä½¿ç”¨ C1 çš„ data_loader.py)
    x_a, y_a = data["source"]
    x_b_tr, y_b_tr = data["target_train"]
    x_b_val, y_b_val = data["target_val"]
    
    # å †å æ‰€æœ‰æ•°æ®ï¼(A-full + B-train + B-val)
    x_all = np.vstack([x_a, x_b_tr, x_b_val]).astype(np.float32)
    y_all = np.vstack([y_a, y_b_tr, y_b_val]).astype(np.float32)

    print(f"âœ… [C3 MDN æ•°æ®] æˆåŠŸå †å æ‰€æœ‰æ•°æ® (Total: {len(x_all)} points)")

    return (
        torch.from_numpy(y_all).to(device),
        torch.from_numpy(x_all).to(device),
        data["x_scaler"],
        data["y_scaler"]
    )

def train_mdn(model, dataloader, optimizer, epochs, device):
    """(100% ä¿æŒ C2 çš„è®­ç»ƒå¾ªç¯)"""
    print(f"--- [åå‘æ¨¡å‹] å¼€å§‹è®­ç»ƒ ---")
    model.train()
    for ep in range(1, epochs + 1):
        total_loss = 0.0
        for y_batch, x_batch in dataloader:
            optimizer.zero_grad(set_to_none=True)
            pi, mu, sigma = model(y_batch)
            loss = mdn_nll_loss(pi, mu, sigma, x_batch)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)
            optimizer.step()
            total_loss += loss.item() * y_batch.size(0)
        
        avg_loss = total_loss / len(dataloader.dataset)
        if ep % 50 == 0 or ep == epochs:
            print(f"[MDN][Epoch {ep:04d}/{epochs}] NLL: {avg_loss:.4f}")

# ==========================================================
# ========== ğŸ‘¨â€âš•ï¸ "C3 èåˆæ‰‹æœ¯" æ ¸å¿ƒ 2 ğŸ‘¨â€âš•ï¸ ==========
# ==========================================================

def setup_args():
    """
    [C3 èåˆç‰ˆ] æ™ºèƒ½ setup_args (é€»è¾‘æ¥è‡ª C1 train_align_hetero.py)ï¼š
    å®ƒç°åœ¨ä¼šè¯»å– C3 "æ··åˆåœ£ç»" config.py é‡Œçš„ TASK_CONFIGS å­—å…¸ï¼Œ
    å¹¶ä¸”åªè¯»å–â€œé»„é‡‘åå‘å‚æ•°â€ ('mdn_...')ï¼
    """
    parser = argparse.ArgumentParser(description="ç»Ÿä¸€çš„åå‘ MDN è®­ç»ƒè„šæœ¬ (C3 èåˆç‰ˆ)")
    parser.add_argument("--opamp", type=str, required=True,
                        choices=config.TASK_CONFIGS.keys(), help="å¿…é¡»æŒ‡å®šçš„ç”µè·¯ç±»å‹")

    # Step 1: å…ˆè§£æå‡º opamp ç±»å‹
    temp_args, other_args = parser.parse_known_args()
    opamp_type = temp_args.opamp

    # Step 2: åˆå¹¶ C3 "æ··åˆåœ£ç»" çš„é…ç½®ä½œä¸ºé»˜è®¤å€¼
    # (COMMON_CONFIG + TASK_CONFIGS[opamp])
    defaults = {**config.COMMON_CONFIG, **config.TASK_CONFIGS.get(opamp_type, {})}

    # Step 3: åŠ¨æ€ä¸ºæ‰€æœ‰ç®€å•ç±»å‹çš„é»˜è®¤å‚æ•°åˆ›å»ºå‘½ä»¤è¡Œå¼€å…³
    for key, value in defaults.items():
        # (æˆ‘ä»¬åªå…³å¿ƒ MDN å’Œ é€šç”¨å‚æ•°)
        if not key.startswith('mdn_') and key not in config.COMMON_CONFIG:
            continue # è·³è¿‡ C1 çš„æ­£å‘å‚æ•° (hidden_dims, lr_pretrain...)
            
        if isinstance(value, (list, dict)):
            continue  # è·³è¿‡ C1 çš„ PRETRAIN_SCHEDULER_CONFIGS
        
        if isinstance(value, bool):
            parser.add_argument(
                f"--{key}", action=argparse.BooleanOptionalAction, help=f"å¼€å…³ '{key}'")
        else:
            parser.add_argument(
                f"--{key}", type=type(value), help=f"è®¾ç½® '{key}'")

    # Step 4: å°†åˆå¹¶åçš„é…ç½®è®¾ç½®ä¸ºè§£æå™¨çš„é»˜è®¤å€¼å¹¶è¿›è¡Œæœ€ç»ˆè§£æ
    parser.set_defaults(**defaults)
    args = parser.parse_args()
    
    return args

# ==========================================================
# ========== ğŸ‘¨â€âš•ï¸ "C3 èåˆæ‰‹æœ¯" æ ¸å¿ƒ 3 ğŸ‘¨â€âš•ï¸ ==========
# ==========================================================

def main():
    args = setup_args() # <-- (C3 æ‰‹æœ¯) è°ƒç”¨æˆ‘ä»¬æ–°çš„ C3 setup_args()
    DEVICE = torch.device(args.device)
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.seed)

    # --- 1. å‡†å¤‡æ•°æ®å’Œè·¯å¾„ ---
    # â¬‡ï¸ (C3 æ‰‹æœ¯) è°ƒç”¨æˆ‘ä»¬ C1 é€»è¾‘çš„ C3 prepare_inverse_dataset()
    y_tensor, x_tensor, x_scaler, y_scaler = prepare_inverse_dataset(args.opamp, DEVICE)
    input_dim = y_tensor.shape[1]
    output_dim = x_tensor.shape[1]
    
    # â¬‡ï¸ (C3 æ‰‹æœ¯) è·¯å¾„æ¥è‡ª C3 "æ··åˆåœ£ç»" çš„ COMMON_CONFIG
    save_dir = Path(args.save_path) 
    save_dir.mkdir(exist_ok=True)
    model_path = save_dir / f"mdn_{args.opamp}.pth"
    
    print(f"--- [C3 é»„é‡‘åå‘è®­ç»ƒ] ä»»åŠ¡: {args.opamp} | è®¾å¤‡: {DEVICE} ---")
    print(f"--- åŠ¨æ€æ£€æµ‹åˆ°ç»´åº¦: Input(y)={input_dim}, Output(x)={output_dim} ---")

    # --- 2. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨ ---
    # â¬‡ï¸ (C3 æ‰‹æœ¯) 
    #    è¿™é‡Œ 100% ä½¿ç”¨äº† C3 "æ··åˆåœ£ç»" é‡Œçš„â€œé»„é‡‘åå‘å‚æ•°â€ï¼
    #    (args.mdn_... å…¨éƒ¨æ¥è‡ª C3 config.py é‡Œçš„ TASK_CONFIGSï¼)
    print(f"âœ… [C3] æ­£åœ¨æ„å»º C2 é»„é‡‘åå‘ MDN (L={args.mdn_num_layers}, H={args.mdn_hidden_dim}, K={args.mdn_components})...")
    model = InverseMDN(
        input_dim=input_dim,
        output_dim=output_dim,
        n_components=args.mdn_components,
        hidden_dim=args.mdn_hidden_dim,
        num_layers=args.mdn_num_layers
    ).to(DEVICE)
    
    optimizer = torch.optim.AdamW(
        model.parameters(), 
        lr=args.mdn_lr, 
        weight_decay=args.mdn_weight_decay
    )
    
    dataset = TensorDataset(y_tensor, x_tensor)
    dataloader = DataLoader(dataset, batch_size=args.mdn_batch_size, shuffle=True)

    # --- 3. è®­ç»ƒ ---
    if args.restart or not model_path.exists():
        # â¬‡ï¸ (C3 æ‰‹æœ¯) è°ƒç”¨ C2 çš„è®­ç»ƒå¾ªç¯
        train_mdn(model, dataloader, optimizer, args.mdn_epochs, DEVICE)
        
        # --- 4. ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ® ---
        print(f"[MDN] æ­£åœ¨ä¿å­˜æ¨¡å‹å’Œå…ƒæ•°æ®...")
        torch.save({
            "state_dict": model.state_dict(),
            # â¬‡ï¸ (C3 æ‰‹æœ¯) 
            #    æˆ‘ä»¬å°† C3 "æ··åˆåœ£ç»" é‡Œçš„é»„é‡‘å‚æ•°ä¿å­˜åˆ° .pth é‡Œï¼
            "config": {
                "opamp_type": args.opamp,
                "input_dim": input_dim,
                "output_dim": output_dim,
                "n_components": args.mdn_components,
                "hidden_dim": args.mdn_hidden_dim,
                "num_layers": args.mdn_num_layers,
            }
        }, model_path)
        print(f"[MDN] æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

        # (C2 çš„ scaler ä¿å­˜é€»è¾‘ï¼Œ100% ä¿ç•™)
        x_scaler_path = save_dir / f"{args.opamp}_x_scaler.gz"
        y_scaler_path = save_dir / f"{args.opamp}_y_scaler.gz"
        joblib.dump(x_scaler, x_scaler_path)
        joblib.dump(y_scaler, y_scaler_path)
        
        meta = {
            "opamp": args.opamp,
            "model_path": str(model_path.resolve()),
            "x_scaler": str(x_scaler_path.resolve()),
            "y_scaler": str(y_scaler_path.resolve()),
        }
        meta_path = model_path.with_suffix(".json")
        meta_path.write_text(json.dumps(meta, indent=2))
        print(f"[MDN] Scalerså’Œå…ƒä¿¡æ¯å·²ä¿å­˜ã€‚")
        
    else:
        print(f"--- [åå‘æ¨¡å‹] è·³è¿‡è®­ç»ƒï¼Œæ¨¡å‹å·²å­˜åœ¨: {model_path} ---")

if __name__ == "__main__":
    main()