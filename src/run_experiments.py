# src/run_experiments.py (å·²æ›´æ–°ï¼šæ”¯æŒé™é»˜è®­ç»ƒ)
import subprocess
import os
import pandas as pd
import time
import json
import shutil
from pathlib import Path
from find_lr_utils import find_pretrain_lr
from models.align_hetero import AlignHeteroMLP
from data_loader import get_data_and_scalers

# ==============================================================================
# --- 0. è·¯å¾„å’Œå®éªŒæ§åˆ¶ ---
# ==============================================================================
SRC_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SRC_DIR.parent

CLEANUP_AFTER_RUN = True
SILENT_TRAINING = True  # <-- æ–°å¢å¼€å…³: è®¾ç½®ä¸º True æ¥ç¦ç”¨è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—

# ==============================================================================
# --- 1. å®šä¹‰ä½ çš„å®éªŒæœç´¢ç©ºé—´ ---
# ==============================================================================
BASE_EXPERIMENT_GRID = [
    {"name": "128, 256, 512]", "hidden_dims": [
        128, 256, 512], "dropout_rate": 0.2},
    {"name": "128, 256, 512, 256]0.2", "hidden_dims": [
        128, 256, 512, 256], "dropout_rate": 0.2},
    {"name": "128, 256, 512, 256]0.3", "hidden_dims": [
        128, 256, 512, 256], "dropout_rate": 0.3},
    {"name": "128, 256, 512, 512]0.2", "hidden_dims": [
        128, 256, 512, 512], "dropout_rate": 0.2},
    {"name": "128, 256, 512, 512]0.3", "hidden_dims": [
        128, 256, 512, 512], "dropout_rate": 0.3},
    {"name": "128, 256, 512, 768]", "hidden_dims": [
        128, 256, 512, 768], "dropout_rate": 0.3},
    {"name": "128, 256, 512, 256, 128]", "hidden_dims": [
        128, 256, 512, 256, 128], "dropout_rate": 0.35},
]

# --- å®éªŒæ§åˆ¶è®¾ç½® ---
NUM_REPETITIONS = 1
OPAMP_TYPE = '5t_opamp'
BASE_RESULTS_DIR = PROJECT_ROOT / "results_experiments_fixed_lr"
FIXED_LR_FINETUNE = 1e-4

# ==============================================================================
# --- 2. åŠ¨æ€ç”Ÿæˆå®Œæ•´çš„å®éªŒåˆ—è¡¨ ---
# ==============================================================================
# ... (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜) ...
EXPERIMENT_GRID = []
for exp_params in BASE_EXPERIMENT_GRID:
    for run_num in range(1, NUM_REPETITIONS + 1):
        new_params = exp_params.copy()
        new_params['name'] = f"{exp_params['name']}_run{run_num}"
        new_params['base_name'] = exp_params['name']
        EXPERIMENT_GRID.append(new_params)

# ==============================================================================
# --- 3. å®éªŒæ‰§è¡Œä¸ç»“æœæ•è· ---
# ==============================================================================
RESULTS = []
start_time = time.time()
BASE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)

# --- é¢„å…ˆåŠ è½½ä¸€æ¬¡æ•°æ® ---
print("æ­£åœ¨é¢„åŠ è½½æ•°æ®...")
data = get_data_and_scalers(opamp_type=OPAMP_TYPE)
input_dim = data['source'][0].shape[1]
output_dim = data['source'][1].shape[1]
print("æ•°æ®åŠ è½½å®Œæˆã€‚")

for i, params in enumerate(EXPERIMENT_GRID):
    exp_name = f"{i+1:02d}_{params['name']}"
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹å®éªŒ {i+1}/{len(EXPERIMENT_GRID)}: {exp_name}")

    exp_results_path = BASE_RESULTS_DIR / exp_name
    exp_results_path.mkdir(parents=True, exist_ok=True)

    print("\n--- æ­¥éª¤ A: æ­£åœ¨ä¸ºå½“å‰ç»“æ„è‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜é¢„è®­ç»ƒå­¦ä¹ ç‡... ---")
    model_params = {
        'input_dim': input_dim, 'output_dim': output_dim,
        'hidden_dims': params['hidden_dims'], 'dropout_rate': params['dropout_rate']
    }
    optimal_lr_pretrain = find_pretrain_lr(AlignHeteroMLP, model_params, data)
    print(f"   - æ‰¾åˆ°çš„æœ€ä¼˜é¢„è®­ç»ƒå­¦ä¹ ç‡ (lr_pretrain): {optimal_lr_pretrain:.2e}")

    final_results_file = exp_results_path / "final_metrics.json"

    command = [
        "python", "train.py", "--opamp", OPAMP_TYPE,
        "--hidden_dims", str(params['hidden_dims']),
        "--dropout_rate", str(params['dropout_rate']),
        "--lr_pretrain", str(optimal_lr_pretrain),
        "--lr_finetune", str(FIXED_LR_FINETUNE),
        "--save_path", str(exp_results_path),
        "--restart", "--evaluate",
        "--results_file", str(final_results_file)
    ]

    # <<< --- æ ¸å¿ƒä¿®æ”¹ï¼šæ§åˆ¶è®­ç»ƒæ—¥å¿—è¾“å‡º --- >>>
    if SILENT_TRAINING:
        print(f"æ­£åœ¨é™é»˜æ‰§è¡Œè®­ç»ƒ... (è¯¦ç»†æ—¥å¿—å·²ç¦ç”¨)")
        process = subprocess.Popen(
            command, cwd=SRC_DIR, stdout=subprocess.PIPE,
            stderr=subprocess.PIPE, text=True, encoding='utf-8'  # æ•è· stdout å’Œ stderr
        )
        # ç­‰å¾…è¿›ç¨‹ç»“æŸå¹¶æ•è·æ‰€æœ‰è¾“å‡ºï¼Œä½†ä¸æ‰“å°
        stdout_output, stderr_output = process.communicate()

        # ä»…åœ¨å‘ç”Ÿé”™è¯¯æ—¶æ‰“å°é”™è¯¯ä¿¡æ¯
        if process.returncode != 0:
            print(f"âš ï¸ å®éªŒ {exp_name} è®­ç»ƒå¤±è´¥ã€‚ä»¥ä¸‹æ˜¯é”™è¯¯æ—¥å¿—ï¼š")
            print(stderr_output)
    else:
        # ä¿æŒåŸæ¥çš„è¯¦ç»†æ—¥å¿—è¡Œä¸º
        print(f"æ­£åœ¨æ‰§è¡Œè®­ç»ƒ... (è¯¦ç»†æ—¥å¿—å·²å¯ç”¨)")
        process = subprocess.Popen(
            command, cwd=SRC_DIR, stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT, text=True, encoding='utf-8'
        )
        for line in iter(process.stdout.readline, ''):
            print(line.strip())
        process.wait()
    # --- ä¿®æ”¹ç»“æŸ ---

    # --- è¯»å–ç»“æœæ–‡ä»¶ ---
    if final_results_file.exists():
        with open(final_results_file, 'r', encoding='utf-8') as f:
            # è¯»å–æ–‡ä»¶å†…å®¹ï¼Œä½†æ³¨æ„æˆ‘ä»¬æ˜¯è¿½åŠ æ¨¡å¼ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªJSONå¯¹è±¡
            # æˆ‘ä»¬åªå–æœ€åä¸€ä¸ª
            all_results = []
            file_content = f.read().strip()
            if file_content:
                json_objects = file_content.split('\n')
                for i, obj_str in enumerate(json_objects):
                    if not obj_str.strip():
                        continue
                    try:
                        all_results.append(json.loads(obj_str))
                    except json.JSONDecodeError as e:
                        print(
                            f"è­¦å‘Š: è§£æ final_metrics.json çš„ç¬¬ {i+1} è¡Œå¤±è´¥ (å†…å®¹: '{obj_str[:50]}...'): {e}")

            if not all_results:
                print(
                    f"âš ï¸ å®éªŒ {exp_name} å®Œæˆï¼Œä½† {final_results_file.name} ä¸ºç©ºæˆ–æ— æ•ˆã€‚")
                final_nll = float('NaN')
                avg_mse = float('NaN')
            else:
                final_metrics = all_results[-1]  # åªå–æœ€åä¸€ä¸ªJSONå¯¹è±¡
                final_nll = final_metrics.get('best_finetune_val_nll')
                avg_mse = final_metrics.get(
                    'evaluation_metrics', {}).get('avg_mse')

        print(
            f"âœ… å®éªŒ {exp_name} å®Œæˆã€‚ æœ€ç»ˆ Val NLL: {final_nll:.6f}, Avg MSE: {avg_mse:.4g}")
        RESULTS.append({
            'å®Œæ•´å®éªŒåç§°': exp_name, 'åŸºç¡€æ¨¡å‹': params['base_name'],
            'hidden_dims': str(params['hidden_dims']), 'dropout_rate': params['dropout_rate'],
            'final_val_nll': final_nll, 'avg_mse': avg_mse
        })

        if CLEANUP_AFTER_RUN:
            try:
                shutil.rmtree(exp_results_path)
                print(f"æ¸…ç†å®Œæ¯•: å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹ {exp_results_path}")
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†å¤±è´¥: åˆ é™¤æ–‡ä»¶å¤¹ {exp_results_path} æ—¶å‡ºé”™ - {e}")
    else:
        print(f"âš ï¸ å®éªŒ {exp_name} å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {final_results_file}")
        RESULTS.append({
            'å®Œæ•´å®éªŒåç§°': exp_name, 'åŸºç¡€æ¨¡å‹': params['base_name'],
            'hidden_dims': str(params['hidden_dims']), 'dropout_rate': params['dropout_rate'],
            'final_val_nll': float('NaN'), 'avg_mse': float('NaN')
        })

# ==============================================================================
# --- 4. æ±‡æ€»å¹¶å±•ç¤ºæœ€ç»ˆç»“æœ ---
# ==============================================================================
# ... (è¿™éƒ¨åˆ†ä»£ç æ— éœ€ä¿®æ”¹) ...
end_time = time.time()
total_duration = end_time - start_time
print(f"\n\n{'='*80}\nğŸ‰ æ‰€æœ‰å®éªŒå·²å®Œæˆï¼æ€»è€—æ—¶: {total_duration / 60:.2f} åˆ†é’Ÿ\n{'='*80}")

if RESULTS:
    results_df = pd.DataFrame(RESULTS)
    print("\nğŸ“Š æ‰€æœ‰è¿è¡Œçš„è¯¦ç»†ç»“æœ (ä»ä¼˜åˆ°åŠ£æ’åº):")
    detailed_results = results_df.sort_values(
        by='final_val_nll', ascending=True)
    print(detailed_results.to_string(index=False))
    summary_path = BASE_RESULTS_DIR / "experiment_summary_detailed.csv"
    detailed_results.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {summary_path}")

    print("\n\n" + "="*80)
    print("ğŸ“ˆ æŒ‰åŸºç¡€æ¨¡å‹èšåˆçš„ç»Ÿè®¡ç»“æœ:")
    aggregated_df = results_df.groupby('åŸºç¡€æ¨¡å‹')['final_val_nll'].agg(
        ['mean', 'std', 'min', 'max', 'count']).sort_values(by='mean', ascending=True)
    aggregated_df.rename(columns={'mean': 'å¹³å‡NLL', 'std': 'NLLæ ‡å‡†å·®',
                         'min': 'æœ€ä½³NLL', 'max': 'æœ€å·®NLL', 'count': 'è¿è¡Œæ¬¡æ•°'}, inplace=True)
    print(aggregated_df)
    agg_summary_path = BASE_RESULTS_DIR / "experiment_summary_aggregated.csv"
    aggregated_df.to_csv(agg_summary_path, encoding='utf-8-sig')
    print(f"\nğŸ“„ èšåˆç»Ÿè®¡ç»“æœå·²ä¿å­˜è‡³: {agg_summary_path}")

    best_model_name = aggregated_df.index[0]
    best_model_stats = aggregated_df.iloc[0]
    print("\n\nğŸ† ç»¼åˆè¡¨ç°æœ€ä½³çš„æ¨¡å‹ç»“æ„ (åŸºäºå¹³å‡NLL):")
    print(f"   - åç§°: {best_model_name}")
    print(f"   - å¹³å‡éªŒè¯é›†NLL: {best_model_stats['å¹³å‡NLL']:.6f}")
    print(f"   - ç¨³å®šæ€§ (æ ‡å‡†å·®): {best_model_stats['NLLæ ‡å‡†å·®']:.6f}")
