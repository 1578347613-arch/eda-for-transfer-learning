# src/run_experiments.py (å·²ç®€åŒ–ï¼šè·³è¿‡é¢„è®­ç»ƒå’ŒLRæŸ¥æ‰¾)
import subprocess
import os
import time
import json
import shutil
from pathlib import Path
import logging
import sys
import re

# --- ä»é¡¹ç›®æ¨¡å—ä¸­å¯¼å…¥ ---
# (ä¸å†éœ€è¦ find_lr_utils)
from data_loader import get_data_and_scalers
import config  # <-- å¯¼å…¥ config ä»¥è·å–é»˜è®¤å€¼å’Œè·¯å¾„

# ==============================================================================
# --- 0. è·¯å¾„å’Œå®éªŒæ§åˆ¶ ---
# ==============================================================================
SRC_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SRC_DIR.parent

# <<< --- æ ¸å¿ƒä¿®æ”¹ï¼šæŒ‡å‘æ‚¨å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ --- >>>
# (ç¡®ä¿ config.OPAMP_TYPE æ­£ç¡®)
EXISTING_PRETRAIN_FILE = PROJECT_ROOT / "results" / \
    f"{config.OPAMP_TYPE}_pretrained.pth"

CLEANUP_AFTER_RUN = False
SILENT_TRAINING = False

# ==============================================================================
# --- 1. å®šä¹‰ä½ çš„å®éªŒæœç´¢ç©ºé—´ ---
# ==============================================================================
# (æ­¤ç½‘æ ¼ç°åœ¨æ˜¯æ‚¨å”¯ä¸€è¦è°ƒæ•´çš„)
BASE_EXPERIMENT_GRID = [
    {"name": "HBase_Scale_0.5", "backbone_lr_scale": 0.2},
    {"name": "HBase_Scale_0.1", "backbone_lr_scale": 0.1},
    {"name": "HBase_Scale_0.05", "backbone_lr_scale": 0.05},
    {"name": "HBase_Scale_0.01", "backbone_lr_scale": 0.01},
]

# --- å®éªŒæ§åˆ¶è®¾ç½® ---
NUM_REPETITIONS = 3
OPAMP_TYPE = config.OPAMP_TYPE  # (ä» config åŠ è½½)
BASE_RESULTS_DIR = PROJECT_ROOT / "results_experiments_finetune_only"

# --- æäº¤æ–‡ä»¶è®¾ç½® ---
TEST_FILE_PATH = PROJECT_ROOT / "data/02_public_test_set/features/features_A.csv"
SUBMISSION_FILE_PREFIX = "predA"

# ==============================================================================
# --- 2. è®¾ç½®æ—¥å¿—ç³»ç»Ÿ (ä¿æŒä¸å˜) ---
# ==============================================================================
BASE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)
EVALUATION_LOG_FILE = BASE_RESULTS_DIR / "experiment_evaluation_log.txt"
file_logger = logging.getLogger('ExperimentLogger')
file_logger.setLevel(logging.INFO)
file_logger.propagate = False
if file_logger.hasHandlers():
    file_logger.handlers.clear()
file_handler = logging.FileHandler(
    EVALUATION_LOG_FILE, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter('%(message)s'))
file_logger.addHandler(file_handler)

# ==============================================================================
# --- 3. åŠ¨æ€ç”Ÿæˆå®Œæ•´çš„å®éªŒåˆ—è¡¨ (ä¿æŒä¸å˜) ---
# ==============================================================================
EXPERIMENT_GRID = []
for exp_params in BASE_EXPERIMENT_GRID:
    for run_num in range(1, NUM_REPETITIONS + 1):
        new_params = exp_params.copy()
        new_params['name'] = f"{exp_params['name']}_run{run_num}"
        new_params['base_name'] = exp_params['name']
        EXPERIMENT_GRID.append(new_params)

# ==============================================================================
# --- 4. è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
# ==============================================================================


def run_command(command, log_prefix=""):
    """
    è¾…åŠ©å‡½æ•°ï¼šæ‰§è¡Œå­è¿›ç¨‹å¹¶æ•è·æ‰€æœ‰è¾“å‡ºã€‚
    """
    print(f"--- [CMD] {log_prefix} æ­£åœ¨æ‰§è¡Œ: {' '.join(command)} ---")
    process = subprocess.Popen(
        command, cwd=SRC_DIR, stdout=subprocess.PIPE,
        stderr=subprocess.PIPE, text=True, encoding='utf-8'
    )
    stdout_lines = []
    if not SILENT_TRAINING:
        for line in iter(process.stdout.readline, ''):
            line = line.strip()
            if line:
                print(f"[{log_prefix}] {line}")
            stdout_lines.append(line)
        process.wait()
        stderr_output = process.stderr.read()
    else:
        stdout_data, stderr_output = process.communicate()
        stdout_lines = stdout_data.splitlines()
    if process.returncode != 0:
        print(f"âš ï¸ {log_prefix} æ‰§è¡Œå¤±è´¥ã€‚")
        if SILENT_TRAINING:
            print("--- é”™è¯¯æ—¥å¿—å¼€å§‹ ---")
            print(stderr_output)
            print("--- é”™è¯¯æ—¥å¿—ç»“æŸ ---")
        return False, stdout_lines, stderr_output
    print(f"--- [CMD] {log_prefix} æ‰§è¡Œå®Œæ¯• ---")
    return True, stdout_lines, stderr_output


def parse_evaluation_log(stdout_lines, exp_name, exp_num):
    """
    ä» train.py çš„ stdout ä¸­æå–è¯„ä¼°æ—¥å¿—å—ã€‚
    """
    eval_block = []
    capturing = False
    start_marker = re.compile(r"===\s*ç›®æ ‡åŸŸéªŒè¯é›†æŒ‡æ ‡ï¼ˆç‰©ç†å•ä½ï¼‰\s*===")
    for line in stdout_lines:
        if start_marker.search(line):
            capturing = True
            eval_block.append(
                f"=== å®éªŒ {exp_num} ({exp_name})ï¼šç›®æ ‡åŸŸéªŒè¯é›†æŒ‡æ ‡ï¼ˆç‰©ç†å•ä½ï¼‰===")
            continue
        if capturing and line.strip():
            eval_block.append(line)
        if capturing and not line.strip():
            capturing = False
            break
    return "\n".join(eval_block)


# ==============================================================================
# --- 5. å®éªŒæ‰§è¡Œä¸ç»“æœæ•è· (å·²ç®€åŒ–) ---
# ==============================================================================
start_time = time.time()
print(f"--- å®éªŒå¼€å§‹ï¼šå…± {len(EXPERIMENT_GRID)} æ¬¡è¿è¡Œ ---")
print(f"--- è¯„ä¼°æ—¥å¿—å°†ä¿å­˜åˆ°: {EVALUATION_LOG_FILE} ---")
print(f"--- å°†ä½¿ç”¨å›ºå®šçš„é¢„è®­ç»ƒæ¨¡å‹: {EXISTING_PRETRAIN_FILE.name} ---")
print(f"--- å°†ä½¿ç”¨å›ºå®šçš„åŸºç¡€å­¦ä¹ ç‡: {config.LEARNING_RATE_HETERO:.2e} ---")

for i, params in enumerate(EXPERIMENT_GRID):
    exp_name = f"{i+1:02d}_{params['name']}"
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹å®éªŒ {i+1}/{len(EXPERIMENT_GRID)}: {exp_name}")

    exp_results_path = BASE_RESULTS_DIR / exp_name
    exp_results_path.mkdir(parents=True, exist_ok=True)

    # å®šä¹‰æ¨¡å‹è·¯å¾„
    pretrained_model_path = exp_results_path / f"{OPAMP_TYPE}_pretrained.pth"
    final_model_path = exp_results_path / f"{OPAMP_TYPE}_finetuned.pth"
    final_results_file = exp_results_path / "final_metrics.json"

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ A & B: (å·²ç¦ç”¨) å¤åˆ¶ç°æœ‰çš„ .pth æ–‡ä»¶ ---
    # --------------------------------------------------------------------------
    print(f"\n--- æ­¥éª¤ AB: æ­£åœ¨å¤åˆ¶é¢„è®­ç»ƒæ¨¡å‹... ---")
    if not EXISTING_PRETRAIN_FILE.exists():
        print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°æ‚¨æŒ‡å®šçš„é¢„è®­ç»ƒæ–‡ä»¶: {EXISTING_PRETRAIN_FILE}")
        continue
    try:
        shutil.copy(EXISTING_PRETRAIN_FILE, pretrained_model_path)
        print(
            f"   - æˆåŠŸå¤åˆ¶ {EXISTING_PRETRAIN_FILE.name} åˆ° {exp_results_path.name}")
    except Exception as e:
        print(f"âŒ å¤åˆ¶æ–‡ä»¶å¤±è´¥: {e}")
        continue

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ C: (å·²ç¦ç”¨) å¯»æ‰¾æœ€ä¼˜å¾®è°ƒå­¦ä¹ ç‡ ---
    # --------------------------------------------------------------------------
    # (å·²è·³è¿‡)

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ D: è¿è¡Œ Finetune + Evaluate (ä½¿ç”¨å›ºå®š LR) ---
    # --------------------------------------------------------------------------

    # <<< --- æ ¸å¿ƒä¿®æ”¹ï¼šä» config å’Œ grid è¯»å– --- >>>
    current_lr_hetero = config.LEARNING_RATE_HETERO
    current_hidden_dims = config.HIDDEN_DIMS
    current_dropout_rate = config.DROPOUT_RATE
    current_backbone_scale = params['backbone_lr_scale']

    print(
        f"\n--- æ­¥éª¤ D: æ­£åœ¨æ‰§è¡Œ Finetune + Evaluate (LR={current_lr_hetero:.2e}, Scale={current_backbone_scale})... ---")

    finetune_command = [
        "python", "train.py", "--opamp", OPAMP_TYPE,
        "--hidden_dims", str(current_hidden_dims),
        "--dropout_rate", str(current_dropout_rate),

        # <<< --- ä½¿ç”¨å›ºå®šçš„ LR å’Œ grid-searched scale --- >>>
        "--lr_hetero", str(current_lr_hetero),
        "--backbone_lr_scale", str(current_backbone_scale),

        "--save_path", str(exp_results_path),
        "--results_file", str(final_results_file),

        "--finetune",  # å¼ºåˆ¶é‡æ–°å¾®è°ƒ
        "--evaluate"
    ]

    success, stdout_lines, _ = run_command(
        finetune_command, f"{exp_name}_FinetuneEval")

    if not success:
        print(f"âŒ å®éªŒ {exp_name} åœ¨å¾®è°ƒ/è¯„ä¼°é˜¶æ®µå¤±è´¥ã€‚è·³è¿‡æ­¤å®éªŒã€‚")
        continue

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ E: æå–æ—¥å¿—å¹¶ä¿å­˜ (ä¿æŒä¸å˜) ---
    # --------------------------------------------------------------------------
    print(f"\n--- æ­¥éª¤ E: æå–æ—¥å¿—å¹¶ä¿å­˜... ---")
    evaluation_text = parse_evaluation_log(stdout_lines, exp_name, i+1)
    if evaluation_text:
        file_logger.info(evaluation_text + "\n")
        print(f"âœ… è¯„ä¼°æ—¥å¿—å·²ä¿å­˜åˆ° {EVALUATION_LOG_FILE.name}")
    else:
        print(f"âš ï¸ è­¦å‘Š: æœªèƒ½ä» {exp_name} çš„è®­ç»ƒè¾“å‡ºä¸­æ•è·åˆ°è¯„ä¼°æ—¥å¿—ã€‚")

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ F: ç”Ÿæˆæäº¤æ–‡ä»¶ (ä¿æŒä¸å˜) ---
    # --------------------------------------------------------------------------
    print(f"\n--- æ­¥éª¤ F: æ­£åœ¨ä¸ºå®éªŒ {i+1} ç”Ÿæˆæäº¤æ–‡ä»¶... ---")
    submission_path = BASE_RESULTS_DIR / f"{SUBMISSION_FILE_PREFIX}_{i+1}"

    if not final_model_path.exists():
        print(f"âŒ å®éªŒ {exp_name} æœªèƒ½ç”Ÿæˆ {final_model_path.name}ã€‚æ— æ³•æäº¤ã€‚")
        continue

    submit_cmd = [
        "python", "submit.py",
        "--opamp", OPAMP_TYPE,
        "--model-path", str(final_model_path),
        "--output-file", str(submission_path),
        "--test-file", str(TEST_FILE_PATH),
        "--hidden-dims", str(params['hidden_dims']),
        "--dropout-rate", str(params['dropout_rate']),
        "--device", config.DEVICE
    ]
    success, _, _ = run_command(submit_cmd, f"{exp_name}_Submit")
    if success:
        print(f"âœ… æˆåŠŸç”Ÿæˆæäº¤æ–‡ä»¶: {submission_path.name}")
    else:
        print(f"âŒ ç”Ÿæˆæäº¤æ–‡ä»¶å¤±è´¥: {submission_path.name}")

    # --------------------------------------------------------------------------
    # --- æ­¥éª¤ G: æ¸…ç† (ä¿æŒä¸å˜) ---
    # --------------------------------------------------------------------------
    if CLEANUP_AFTER_RUN:
        try:
            shutil.rmtree(exp_results_path)
            print(f"æ¸…ç†å®Œæ¯•: å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤¹ {exp_results_path}")
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†å¤±è´¥: åˆ é™¤æ–‡ä»¶å¤¹ {exp_results_path} æ—¶å‡ºé”™ - {e}")

# ==============================================================================
# --- 5. æ±‡æ€»å¹¶å±•ç¤ºæœ€ç»ˆç»“æœ (ä¿æŒä¸å˜) ---
# ==============================================================================
end_time = time.time()
total_duration = end_time - start_time
final_message = f"\n\n{'='*80}\nğŸ‰ æ‰€æœ‰å®éªŒå·²å®Œæˆï¼æ€»è€—æ—¶: {total_duration / 60:.2f} åˆ†é’Ÿ\n{'='*80}\n"
final_message += f"è¯„ä¼°æ—¥å¿—å·²å…¨éƒ¨ä¿å­˜åˆ°: {EVALUATION_LOG_FILE}\n"
final_message += f"æäº¤æ–‡ä»¶å·²ç”Ÿæˆåœ¨: {BASE_RESULTS_DIR}\n"
print(final_message)
file_logger.info(final_message)
